<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="kafka,unix pipeline," />





  <link rel="alternate" href="/atom.xml" title="Bowen's blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="Here’s another blog from Confluent’s website. What’s interesting about this article is that it compares the design philosophy between Unix and monolith database, and refers to Kafka as a distributed v">
<meta property="og:type" content="article">
<meta property="og:title" content="Apache Kafka, Samza, and the Unix Philosopy of Distributed Data">
<meta property="og:url" content="https://phoenixjiangnan.github.io/2016/10/06/distributed system/kafka/Apache-Kafka-Samza-and-the-Unix-Philosopy-of-Distributed-Data/index.html">
<meta property="og:site_name" content="Bowen's blog">
<meta property="og:description" content="Here’s another blog from Confluent’s website. What’s interesting about this article is that it compares the design philosophy between Unix and monolith database, and refers to Kafka as a distributed v">
<meta property="og:image" content="http://www.confluent.io/wp-content/uploads/2016/08/unixphil-01.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-03.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-04.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-05.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-06.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-08.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-10.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-12.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-13.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-14.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-15.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-16.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-17.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-18.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-19.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-20.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-21.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-22.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-23.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-24.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-25.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-26.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-27.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-28.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-29.png">
<meta property="og:image" content="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-30.png">
<meta property="og:updated_time" content="2016-10-06T12:09:05.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Apache Kafka, Samza, and the Unix Philosopy of Distributed Data">
<meta name="twitter:description" content="Here’s another blog from Confluent’s website. What’s interesting about this article is that it compares the design philosophy between Unix and monolith database, and refers to Kafka as a distributed v">
<meta name="twitter:image" content="http://www.confluent.io/wp-content/uploads/2016/08/unixphil-01.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="https://phoenixjiangnan.github.io/2016/10/06/distributed system/kafka/Apache-Kafka-Samza-and-the-Unix-Philosopy-of-Distributed-Data/"/>

  <title> Apache Kafka, Samza, and the Unix Philosopy of Distributed Data | Bowen's blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Bowen's blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">keep learning - learning notes and blogs</p>
</div>



<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Apache Kafka, Samza, and the Unix Philosopy of Distributed Data
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-10-06T19:55:32+08:00" content="2016-10-06">
              2016-10-06
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/distributed-system/" itemprop="url" rel="index">
                    <span itemprop="name">distributed system</span>
                  </a>
                </span>

                
                
                  , 
                

              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/distributed-system/kafka/" itemprop="url" rel="index">
                    <span itemprop="name">kafka</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/10/06/distributed system/kafka/Apache-Kafka-Samza-and-the-Unix-Philosopy-of-Distributed-Data/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/10/06/distributed system/kafka/Apache-Kafka-Samza-and-the-Unix-Philosopy-of-Distributed-Data/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Here’s another blog from Confluent’s website. What’s interesting about this article is that it compares the design philosophy between Unix and monolith database, and refers to Kafka as a distributed version of Unix pipeline. The latter point is exceptionally interesting and brings a whole new view of Kafka to me.</p>
<p>After working fulltime in the IT industry for almost two years, I’ve seen repeatedly that design of modern system borrows ideas from the design of Linux/Unix. Besides Kafka and Unix pipeline, another example is the <a href="http://docs.oracle.com/cd/E13924_01/coh.340/e13819/readthrough.htm" target="_blank" rel="external">Write Behind Caching Pattern</a> in modern system actually refers to <a href="https://www.thomas-krenn.com/en/wiki/Linux_Page_Cache_Basics" target="_blank" rel="external">Linux’s Page Cache</a></p>
<hr>
<p><a href="http://www.confluent.io/blog/apache-kafka-samza-and-the-unix-philosophy-of-distributed-data/" target="_blank" rel="external">Original Post</a> by Martin Kleppmann. August 1, 2015.</p>
<hr>
<p><img src="http://www.confluent.io/wp-content/uploads/2016/08/unixphil-01.png" alt="unixphil-01"></p>
<p>One of the things I realised while doing research for my book is that contemporary software engineering still has a lot to learn from the 1970s. As we’re in such a fast-moving field, we often have a tendency of dismissing older ideas as irrelevant – and consequently, we end up having to learn the same lessons over and over again, the hard way. Although computers have got faster, data has got bigger and requirements have become more complex, many old ideas are actually still highly relevant today. In this blog post I’d like to highlight one particular set of old ideas that I think deserves more attention today: <code>the Unix philosophy</code>. I’ll show how this philosophy is very different from the design approach of mainstream databases, and explore what it would look like if modern distributed data systems learnt a thing or two from Unix.</p>
<p>In particular, I’m going to argue that there are a lot of similarities between <code>Unix pipes</code> and <code>Apache Kafka</code>, and that this similarity enables good architectural styles for large-scale applications. But before we get into that, let me remind you of the foundations of the <code>Unix philosophy</code>. You’ve probably seen the power of Unix tools before – but to get started, let me give you a concrete example that we can talk about. Say you have a web server that writes an entry to a log file every time it serves a request. For example, using the nginx default access log format, one line of the log might look like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] &quot;GET /css/typography.css HTTP/1.1&quot; 200 3377 &quot;http://martin.kleppmann.com/&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36&quot;</div></pre></td></tr></table></figure>
<p>(That is actually one line, it’s only broken up into multiple lines here for readability.) This line of the log indicates that on 27 February 2015 at 17:55:11 UTC, the server received a request for the file /css/typography.css from the client IP address 216.58.210.78. It then goes on to note various other details, including the browser’s user-agent string. Various tools can take these log files and produce pretty reports about your website traffic, but for the sake of the exercise, let’s build our own, using basic Unix tools. Let’s determine the 5 most popular URLs on our website. To start with, we need to extract the path of the URL that was requested, for which we can use <code>awk</code>. <code>awk</code> doesn’t know about the format of nginx logs – it just treats the log file as text. By default, <code>awk</code> takes one line of input at a time, splits it by whitespace, and makes the whitespace-separated components available as variables <code>$1</code>, <code>$2</code>, etc. In the nginx log example, the requested URL path is the seventh whitespace-separated component: </p>
<a id="more"></a>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-03.png" alt=""></p>
<p>Now that you’ve extracted the path, you can determine the 5 most popular pages on your website as follows:</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">awk '&#123;<span class="built_in">print</span> $<span class="number">7</span>&#125;' access.log | # Split by whitespace, <span class="number">7</span>th field is request <span class="built_in">path</span></div><div class="line">    sort                    | # Make occurrences of the same URL appear consecutively <span class="keyword">in</span> file</div><div class="line">    uniq -c                 | # <span class="built_in">Replace</span> consecutive occurrences of the same URL with a count</div><div class="line">    sort -rn                | # Sort by number of occurrences, descending</div><div class="line">    head -n <span class="number">5</span>                 # Output top <span class="number">5</span> URLs</div></pre></td></tr></table></figure>
<p>The output of that series of commands looks something like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">4189 /favicon.ico</div><div class="line">3631 /2013/05/24/improving-security-of-ssh-private-keys.html</div><div class="line">2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html</div><div class="line">1369 /</div><div class="line">915 /css/typography.css</div></pre></td></tr></table></figure>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-04.png" alt=""></p>
<p>Although the above command line looks a bit obscure if you’re unfamiliar with Unix tools, it is incredibly powerful. It will process gigabytes of log files in a matter of seconds, and you can easily modify the analysis to suit your needs. For example, if you want to count top client IP addresses instead of top pages, change the awk argument to <code>{print $1}</code>. Many data analyses can be done in a few minutes using some combination of <code>awk</code>, <code>sed</code>, <code>grep</code>, <code>sort</code>, <code>uniq</code> and <code>xargs</code>, and they perform surprisingly well. This is no coincidence: it is a direct result of the design philosophy of Unix. </p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-05.png" alt=""></p>
<p>The Unix philosophy is a set of principles that emerged gradually during the design and implementation of Unix systems during the late 1960s and ‘70s. There are various interpretations of the Unix philosophy, but two points that particularly stand out were described by Doug McIlroy, Elliot Pinson and Berk Tague as follows in 1978:</p>
<ul>
<li>Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new <code>features.</code></li>
<li>Expect the output of every program to become the input to another, as yet unknown, program.</li>
</ul>
<p>These principles are the foundation for chaining together programs into pipelines that can accomplish complex processing tasks. The key idea here is that a program does not know or care where its input is coming from, or where its output is going to: it may be a file, or another program that’s part of the operating system, or another program written by someone else entirely.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-06.png" alt=""></p>
<p>The tools that come with the operating system are generic, but they are designed such that they can be <strong>composed</strong> together into larger programs that can perform application-specific tasks. The benefits that the designers of Unix derived from this design approach sound quite like the ideas of the Agile and DevOps movements that appeared decades later: scripting and automation, rapid prototyping, incremental iteration, being friendly to experimentation, and breaking down large projects into manageable chunks. Plus ça change.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-08.png" alt=""></p>
<p>When you join two commands with the pipe character in your shell, the shell starts both programs at the same time, and attaches the output of the first process to the second process’ input. This attachment mechanism uses the <code>pipe</code> syscall provided by the operating system. Note that this wiring is not done by the programs themselves, but by the shell – this allows them to be <a href="https://en.wikipedia.org/wiki/Loose_coupling" target="_blank" rel="external">loosely coupled</a>, and not worry about where their input is coming from, or where their output is going.</p>
<p>The pipe had been invented in 1964 by Doug McIlroy, who first described it like this in an internal Bell Labs memo: “We should have some ways of connecting programs like [a] garden hose – screw in another segment when it becomes necessary to massage data in another way.” Dennis Richie later wrote up his perspective on the memo.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-10.png" alt=""></p>
<p>They also realised early that the inter-process communication mechanism (pipes) can look very similar to the mechanism for reading and writing files. We now call this input redirection (using the contents of a file as input to a process) and output redirection (writing the output of a process to a file). The reason that Unix programs can be composed so flexibly is that they all conform to the same interface: most programs have one stream for input data (stdin) and two output streams (stdout for regular output data, and stderr for errors and diagnostic messages).</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-12.png" alt=""></p>
<p>Programs may also do other things besides reading <code>stdin</code> and writing <code>stdout</code>, such as reading and writing files, communicating over the network, or drawing a graphical user interface. However, the <code>stdin</code>/<code>stdout</code> communication is considered to be the main way how data flows from one Unix tool to another. And the great thing about the <code>stdin</code>/<code>stdout</code> interface is that anyone can implement it easily, in any programming language. You can develop your own tool that conforms to this interface, and it will play nicely with all the standard tools that ship as part of the operating system.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-13.png" alt=""></p>
<p>For example, when analysing a web server log file, perhaps you want to find out how many visitors you have from each country. The log doesn’t tell you the country, but it does tell you the IP address, which you can translate into a country using an IP geolocation database. Such a database isn’t included with your operating system by default, but you can write your own tool that takes IP addresses on <code>stdin</code> and outputs country codes on <code>stdout</code>. Once you’ve written that tool, you can include it in the data processing pipeline we discussed previously, and it will work just fine. This may seem painfully obvious if you’ve been working with Unix for a while, but I’d like to emphasise how remarkable this is: your own code runs on equal terms with the tools provided by the operating system. Apps with graphical user interfaces or web apps cannot simply be extended and wired together like this. You can’t just pipe Gmail into a separate search engine app, and post results to a wiki. Today it’s an exception, not the norm, to have programs that work together as smoothly as Unix tools do.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-14.png" alt=""></p>
<p>Change of scene. Around the same time as Unix was being developed, the <code>relational data model</code> was proposed, which in time became SQL, and was implemented in many popular databases. Many databases actually run on Unix systems. Does that mean they also follow the Unix philosophy?</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-15.png" alt=""></p>
<p>The dataflow in most database systems is very different from Unix tools. Rather than using <code>stdin</code> and <code>stdout</code> as communication channels, there is a <strong>database server</strong>, and several <strong>clients</strong>. The clients send queries to read or write data on the server, the server handles the queries and sends responses to the clients. This relationship is fundamentally asymmetric: <code>clients and servers are distinct roles</code>.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-16.png" alt=""></p>
<p>What about the <code>composability</code> and <code>extensibility</code> that we find in Unix systems? Clients can do anything they like (since they are application code), but database servers are mostly in the business of storing and retrieving your data. Letting you run arbitrary code is not their top priority. That said, many databases do provide some ways of extending the database server with your own code. For example, many relational databases let you write stored procedures in their own, rudimentary procedural language such as <code>PL/SQL</code> (and some let you run code in a general-purpose programming language such as JavaScript). However, the things you can do in stored procedures are limited. Other extension points in some databases are support for custom data types (this was one of the early design goals of Postgres), or pluggable storage engines. Essentially, these are plugin APIs: you can run your code in the database server, provided that your module adheres to a plugin API exposed by the database server for a particular purpose. This kind of extensibility is not the same as the arbitrary composability we saw with Unix tools. The plugin API is totally controlled by the database server, and subordinate to it. Your extension code is a guest in the database server’s home, not an equal partner.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-17.png" alt=""></p>
<p>A consequence of this design is that you can’t just pipe one database into another, even if they have the same data model. Nor can you insert your own code into the database’s internal processing pipelines (unless the server has specifically provided an extension point for you, such as triggers). I feel the design of databases is very self-centered. A database seems to assume that it’s the centre of your universe: the only place where you might want to store and query your data, the source of truth, and the destination for all queries. The closest you can get to piping data in and out of it is through bulk-loading and bulk-dumping (backup) operations, but those operations don’t really use any of the database’s features, such as query planning and indexes. If a database was designed according to the Unix philosophy, it would be based on a small number of core primitives that you could easily combine, extend and replace at will. Instead, databases are tremendously complicated, monolithic beasts. While Unix acknowledges that the operating system will never do everything you might want, and thus encourages you to extend it, databases try to implement all the features you may need in a single program.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-18.png" alt=""></p>
<p>Perhaps that design is fine in simple applications where a single database is indeed sufficient. However, many complex applications find that they have to use their data in various different ways: they need fast random access for OLTP, big sequential scans for analytics, inverted indexes for full-text search, graph indexes for connected data, machine learning systems for recommendation engines, a push mechanism for notifications, various different cached representations of the data for fast reads, and so on. A general-purpose database may try to do all of those things in one product (“one size fits all”), but in all likelihood it will not perform as well as a tool that is specialized for one particular purpose. In practice, you can often get the best results by combining various different data storage and indexing systems: for example, you may take the same data and store it in a relational database for random access, in Elasticsearch for full-text search, in a columnar format in Hadoop for analytics, and cached in a denormalized form in memcached. When you need to integrate different databases, the lack of Unix-style composability is a severe limitation. (I’ve done some work on piping data out of Postgres into other applications, but there’s still a long way to go before we can simply pipe any database into any other database.)</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-19.png" alt=""></p>
<p>We said that Unix tools are <code>composable</code> because they all implement the same interface of <code>stdin</code>, <code>stdout</code> and <code>stderr</code> – and each of these is a file descriptor, i.e. a stream of bytes that you can read or write like a file. This interface is simple enough that anyone can easily implement it, but it is also powerful enough that you can use it for anything. Because all Unix tools implement the same interface, we call it a uniform interface. That’s why you can pipe the output of <code>gunzip</code> to <code>wc</code> without a second thought, even though the authors of those two tools probably never spoke to each other. It’s like lego bricks, which all implement the same pattern of knobbly bits and grooves, allowing you to stack any lego brick on any other, regardless of their shape, size or colour.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-20.png" alt=""></p>
<p>The uniform interface of file descriptors in Unix doesn’t just apply to the input and output of processes, but it’s a very broadly applied pattern. If you open a file on the filesystem, you get a file descriptor. Pipes and unix sockets provide file descriptors that are a communication channel to another process on the same machine. On Linux, the virtual files in <code>/dev</code> are the interfaces of device drivers, so you might be talking to a USB port or even a GPU. The virtual files in <code>/proc</code> are an API for the kernel, but since they’re exposed as files, you can access them with the same tools as regular files. Even a TCP connection to a process on another machine is a file descriptor, although the BSD sockets API (which is most commonly used to establish TCP connections) is arguably not as Unixy as it could be. Plan 9 shows that even the network could have been cleanly integrated into the same uniform interface. To a first approximation, everything on Unix is a file. This uniformity means the logic of Unix tools is separated from the wiring, making it more composable. sed doesn’t need to care whether it’s talking to a pipe to another process, or a socket, or a device driver, or a real file on the filesystem. It’s all the same.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-21.png" alt=""></p>
<p>A file is <strong>a stream of bytes</strong>, perhaps with an end-of-file (EOF) marker at some point, indicating that the stream has ended (a stream can be of arbitrary length, and a process may not know in advance how long its input is going to be). A few tools (e.g. <code>gzip</code>) operate purely on byte streams, and don’t care about the structure of the data. But most tools need to parse their input in order to do anything useful with it. For this, most Unix tools use ASCII, with each record on one line, and fields separated by tabs or spaces, or maybe commas. Files are totally obvious to us today, which shows that a byte stream turned out to be a good uniform interface. However, the implementors of Unix could have decided to do it very differently. For example, it could have been a function callback interface, using a schema to pass records from process to process. Or it could have been shared memory (like System V IPC or mmap, which came along later). Or it could have been a bit stream rather than a byte stream. In a sense, a byte stream is a lowest common denominator – the simplest possible interface. Everything can be expressed in terms of a stream of bytes, and it’s fairly agnostic to the transport medium (pipe from another process, file on disk, TCP connection, tape, etc). But this is also a disadvantage, as we shall discuss later.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-22.png" alt=""></p>
<p>We’ve seen that Unix developed some very good design principles for software development, and that databases have taken a very different route. I would love to see a future in which we can learn from both paths of development, and combine the best ideas from each. How can we make 21st-century data systems better by learning from the Unix philosophy? In the rest of this post I’d like to explore what it might look like if we bring the Unix philosophy to the world of databases.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-23.png" alt=""></p>
<p>First, let’s acknowledge that Unix is not perfect. Although I think the simple, uniform interface of byte streams was very successful at enabling an ecosystem of flexible, composable, powerful tools, Unix has some limitations:</p>
<ul>
<li><p>It’s designed for use on a single machine. As our applications get every more data and traffic, and have higher uptime requirements, moving to distributed systems is becoming increasingly inevitable. Although a TCP connection can be made to look somewhat like a file, I don’t think that’s the right answer: it only works if both sides of the connection are up, and it has somewhat messy edge case semantics. TCP is good, but by itself it’s too low-level to serve as a distributed pipe implementation.</p>
</li>
<li><p>A Unix pipe is designed to have a single sender process, and a single recipient. You can’t use pipes to send output to several processes, or to collect input from several processes. (You can branch a pipeline with <code>tee</code>, but a pipe itself is always one-to-one.)</p>
</li>
<li><p>ASCII text (or rather, UTF-8) is great for making data easily explorable, but it quickly gets messy. Every process needs to be set up with its own input parsing: first breaking the byte stream into records (usually separated by newline, though some advocate <code>0x1e</code>, the ASCII record separator). Then a record needs to be broken up into fields, like the <code>$7</code> in the <code>awk</code> example at the beginning. Separator characters that appear in the data need to be escaped somehow. Even a fairly simple tool like xargs has about half a dozen command-line options to specify how its input should be parsed. Text-based interfaces work tolerably well, but in retrospect, I am pretty sure that a richer data model with explicit schemas would have worked better.</p>
</li>
<li><p>Unix processes are generally assumed to be fairly short-running. For example, if a process in the middle of a pipeline crashes, there is no way for it to resume processing from its input pipe – the entire pipeline fails and must be re-run from scratch. That’s no problem if the commands run only for a few seconds, but if an application is expected to run continuously for years, better fault tolerance is needed.</p>
</li>
</ul>
<p>I think we can find a solution that overcomes these downsides, while retaining the Unix philosophy’s benefits. </p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-24.png" alt=""></p>
<p>The cool thing is that this solution already exists, and is implemented in Kafka and Samza, two open source projects that work together to provide distributed stream processing. As you probably already know from other posts on this blog, Kafka is a scalable distributed message broker, and Samza is a framework that lets you write code to consume and produce data streams.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-25.png" alt="streams"></p>
<p>In fact, when you look at it through the Unix lens, Kafka looks quite like the pipe that connects the output of one process to the input of another. And Samza looks quite like a standard library that helps you read <code>stdin</code> and write <code>stdout</code> (and a few helpful additions, such as a deployment mechanism, state management, metrics, and monitoring). The style of stream processing jobs that you can write with Kafka and Samza closely follows the Unix tradition of small, composable tools:</p>
<ul>
<li>In Unix, the operating system kernel provides the pipe, a transport mechanism for getting a stream of bytes from one process to another.</li>
<li>In stream processing, Kafka provides publish-subscribe streams, a transport mechanism for getting messages from one stream processing job to another.</li>
</ul>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-26.png" alt=""></p>
<p>Kafka addresses the downsides of Unix pipes that we discussed previously:</p>
<ul>
<li><p>The single-machine limitation is lifted: Kafka itself is distributed by default, and any stream processors that use it can also be distributed across multiple machines.</p>
</li>
<li><p>A Unix pipe connects exactly one process output with exactly one process input, whereas a stream in Kafka can have many producers and many consumers. Many inputs is important for services that are distributed across multiple machines, and many outputs makes Kafka more like a broadcast channel. This is very useful, since it allows the same data stream to be consumed independently for several different purposes (including monitoring and audit purposes, which are often outside of the application itself). Kafka consumers can come and go without affecting other consumers.</p>
</li>
<li><p>Kafka also provides good fault tolerance: data is replicated across multiple Kafka nodes, so if one node fails, another node can automatically take over. If a stream processor node fails and is restarted, it can resume processing at its last checkpoint.</p>
</li>
<li><p>Rather than a stream of bytes, Kafka provides a stream of messages, which saves the first step of input parsing (breaking the stream of bytes into a sequence of records). Each message is just an array of bytes, so you can use your favourite serialisation format for individual messages: JSON, XML, Avro, Thrift or Protocol Buffers are all reasonable choices. It’s well worth standardising on one encoding, and Confluent provides particularly good schema management support for Avro. This allows applications to work with objects that have meaningful field names, and not have to worry about input parsing or output escaping. It also provides good support for schema evolution without breaking compatibility.</p>
</li>
</ul>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-27.png" alt=""></p>
<p>There are a few more things that Kafka does differently from Unix pipes, which are worth calling out briefly:</p>
<ul>
<li><p>As mentioned, Unix pipes provide a byte stream, whereas Kafka provides a stream of messages. This is especially important if several processes are concurrently writing to the same stream: in a byte stream, the bytes from different writers can be interleaved, leading to an unparseable mess. Since messages are coarser-grained and self-contained, they can be safely interleaved, making it safe for multiple processes to concurrently write to the same stream.</p>
</li>
<li><p>Unix pipes are just a small in-memory buffer, whereas Kafka durably writes all messages to disk. In this regard, Kafka is less like a pipe, and more like one process writing to a temporary file, while several other processes continuously read that file using tail -f (each consumer tails the file independently). Kafka’s approach provides better fault tolerance, since it allows a consumer to fail and restart without skipping messages. Kafka automatically splits those ‘temporary’ files into segments and garbage-collects old segments on a configurable schedule.</p>
</li>
<li><p>In Unix, if the consuming process of a pipe is slow to read the data, the buffer fills up and the sending process is blocked from writing to the pipe. This is a kind of backpressure. In Kafka, the producer and consumer are more decoupled: a slow consumer has its input buffered, so it doesn’t slow down the producer or other consumers. As long as the buffer fits within Kafka’s available disk space, the slow consumer can catch up later. This makes the system less sensitive to individual slow components, and more robust overall.</p>
</li>
<li><p>A data stream in Kafka is called a <code>topic</code>, and you can refer to it by name (which makes it more like a Unix named pipe. A pipeline of Unix programs is usually started all at once, so the pipes normally don’t need explicit names. On the other hand, a long-running application usually has bits added, removed or replaced gradually over time, so you need names in order to tell the system what you want to connect to. Naming also helps with discovery and management.</p>
</li>
</ul>
<p>Despite those differences, I still think it makes sense to think of Kafka as Unix pipes for distributed data. For example, one thing they have in common is that Kafka keeps messages in a fixed order (like Unix pipes, which keep the byte stream in a fixed order). This is a very useful property for event log data: the order in which things happened is often meaningful and needs to be preserved. Other types of message broker, like AMQP and JMS, do not have this ordering property.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-28.png" alt=""></p>
<p>So we’ve got Unix tools and stream processors that look quite similar. Both read some input stream, modify or transform it in some way, and produce an output stream that is somehow derived from the input. Importantly, the processing does not modify the input itself: it remains immutable. If you run <code>awk</code> on some input file, the file remains unmodified (unless you explicitly choose to overwrite it). Also, most Unix tools are deterministic, i.e. if you give them the same input, they always produce the same output. This means you can re-run the same command as many times as you want, and gradually iterate your way towards a working program. It’s great for experimentation, because you can always go back to your original data if you mess up the processing. This deterministic and side-effect-free processing looks a lot like functional programming. That doesn’t mean you have to use a functional programming language like Haskell (although you’re welcome to do so if you want), but you still get many of the benefits of functional code.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-29.png" alt=""></p>
<p>The Unix-like design principles of Kafka enable building composable systems at a large scale. In a large organisation, different teams can each publish their data to Kafka. Each team can independently develop and maintain stream processing jobs that consume streams and produce new streams. Since a stream can have any number of independent consumers, no coordination is required to set up a new consumer. We’ve been calling this idea a stream data platform. In this kind of architecture, the data streams in Kafka act as the communication channel between different teams’ systems. Each team focusses on making their particular part of the system do one thing well. While Unix tools can be composed to accomplish a data processing task, distributed streaming systems can be composed to comprise the entire operation of a large organisation. A Unixy approach manages the complexity of a large system by encouraging loose coupling: thanks to the uniform interface of streams, different components can be developed and deployed independently. Thanks to the fault tolerance and buffering of the pipe (Kafka), when a problem occurs in one part of the system, it remains localised. And schema management allows changes to data structures to be made safely, so that each team can move fast without breaking things for other teams.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-30.png" alt=""></p>
<p>To wrap up this post, let’s consider a real-life example of how this works at LinkedIn. As you may know, companies can post their job openings on LinkedIn, and jobseekers can browse and apply for those jobs. What happens if a LinkedIn member (user) views one of those job postings? It’s very useful to know who has looked at which jobs, so the service that handles job views publishes an event to Kafka, saying something like “member 123 viewed job 456 at time 789”. Now that this information is in Kafka, it can be used for many good purposes:</p>
<ul>
<li><p>Monitoring systems: Companies pay LinkedIn to post their job openings, so it’s important that the site is working correctly. If the rate of job views drops unexpectedly, alarms should go off, because it indicates a problem that needs to be investigated.</p>
</li>
<li><p>Relevance and recommendations: It’s annoying for users to see the same thing over and over again, so it’s good to track how many times the users has seen a job posting, and feed that into the scoring process. Keeping track of who viewed what also allows for collaborative filtering recommendations (people who viewed X also viewed Y).</p>
</li>
<li><p>Preventing abuse: LinkedIn doesn’t want people to be able to scrape all the jobs, submit spam, or otherwise violate the terms of service. Knowing who is doing what is the first step towards detecting and blocking abuse.<br>Job poster analytics: The companies who post their job openings want to see stats (in the style of Google Analytics) about who is viewing their postings, for example so that they can test which wording attracts the best candidates.</p>
</li>
<li><p>Import into Hadoop and Data Warehouse: For LinkedIn’s internal business analytics, for senior management’s dashboards, for crunching numbers that are reported to Wall Street, for evaluating A/B tests, and so on.</p>
</li>
</ul>
<p>All of those systems are complex in their own right, and are maintained by different teams. Kafka provides a fault-tolerant, scalable implementation of a pipe. A stream data platform based on Kafka allows all of these various systems to be developed independently, and to be connected and composed in a robust way. If you enjoyed this post, you’ll love my book <a href="http://dataintensive.net/" target="_blank" rel="external">Designing Data-Intensive Applications</a>, published by O’Reilly. Thank you to Jay Kreps, Gwen Shapira, Michael Noll, Ewen Cheslack-Postava, Jason Gustafson, and Jeff Hartley for feedback on a draft of this post, and thanks also to Jay for providing the LinkedIn job view example.</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/kafka/" rel="tag">#kafka</a>
          
            <a href="/tags/unix-pipeline/" rel="tag">#unix pipeline</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/10/05/system design/Throttling-and-Traffic-Shaping/" rel="next" title="Throttling and Traffic Shaping">
                <i class="fa fa-chevron-left"></i> Throttling and Traffic Shaping
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/avatar/avatar.jpg"
               alt="Bowen" />
          <p class="site-author-name" itemprop="name">Bowen</p>
          <p class="site-description motion-element" itemprop="description">keep learning - learning notes and blogs</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">107</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">48</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">255</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <p class="post-toc-empty">This post does not have a Table of Contents</p>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bowen</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'bowensgithubblog';
      var disqus_identifier = '2016/10/06/distributed system/kafka/Apache-Kafka-Samza-and-the-Unix-Philosopy-of-Distributed-Data/';
      var disqus_title = "Apache Kafka, Samza, and the Unix Philosopy of Distributed Data";
      var disqus_url = 'https://phoenixjiangnan.github.io/2016/10/06/distributed system/kafka/Apache-Kafka-Samza-and-the-Unix-Philosopy-of-Distributed-Data/';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
        run_disqus_script('embed.js');
      
    </script>
  




  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = true;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '' && data_content != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title < 0 && index_content < 0 ){
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                            }
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });

                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  

  

  

</body>
</html>
