<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Bowen&#39;s blog</title>
  <subtitle>keep learning - learning notes and blogs</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://phoenixjiangnan.github.io/"/>
  <updated>2016-10-19T23:22:12.000Z</updated>
  <id>https://phoenixjiangnan.github.io/</id>
  
  <author>
    <name>Bowen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hexo SEO | Hexo SEO Tutorial</title>
    <link href="https://phoenixjiangnan.github.io/2016/10/19/hexo/Hexo-SEO-Hexo-SEO-Tutorial/"/>
    <id>https://phoenixjiangnan.github.io/2016/10/19/hexo/Hexo-SEO-Hexo-SEO-Tutorial/</id>
    <published>2016-10-19T23:09:54.000Z</published>
    <updated>2016-10-19T23:22:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://hunao.info/2016/06/01/Hexo-Seo%E4%BC%98%E5%8C%96%E8%AE%A9%E4%BD%A0%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%9C%A8google%E6%90%9C%E7%B4%A2%E6%8E%92%E5%90%8D%E7%AC%AC%E4%B8%80/" target="_blank" rel="external">This blog</a> is very helpful and practical tutorial of how to do SEO on Hexo and Next theme, as well as how to use Google Search Console as a general tool for SEO.</p>
<p>I’ve followed the tutorial to tune up my site configs. Try to search my blogs on google!</p>
<hr>
<h2 id="Hexo-SEO-优化让你的博客在google搜索排名第一"><a href="#Hexo-SEO-优化让你的博客在google搜索排名第一" class="headerlink" title="Hexo SEO 优化让你的博客在google搜索排名第一"></a>Hexo SEO 优化让你的博客在google搜索排名第一</h2><p>刚刚建买了域名建了博客，发现在google，百度毛都搜不到，真是悲伤，后来才知道原来是要seo的，所以看了一些文章，然后自己也摸索了一下，终于在让自己的博客在google搜索排名第一了!!!上图！</p>
<p><img src="http://i.imgur.com/eA6Jesb.png" alt="google更新了"></p>
<p>哈哈，顿时觉得自己好厉害耶!<br>下面给大家分享一些我的经验。</p>
<h3 id="本教程在NexT主题上操作，其他主题请自行测试"><a href="#本教程在NexT主题上操作，其他主题请自行测试" class="headerlink" title="本教程在NexT主题上操作，其他主题请自行测试"></a>本教程在NexT主题上操作，其他主题请自行测试</h3><h3 id="首页title优化"><a href="#首页title优化" class="headerlink" title="首页title优化"></a>首页title优化</h3><p>更改<strong>index.swig</strong>文件<strong>(your-hexo-site\themes\next\layout)</strong>;</p>
<p>将下面这段代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;% block title %&#125; &#123;&#123; config.title &#125;&#125; &#123;% endblock %&#125;</div></pre></td></tr></table></figure>
<p>改成</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;% block title %&#125; &#123;&#123; config.title &#125;&#125; - &#123;&#123; theme.description &#125;&#125; &#123;% endblock %&#125;</div></pre></td></tr></table></figure>
<p>这时候你的首页会更符合<strong>网站名称 - 网站描述</strong>这习惯。<br>进阶，做了 SEO 优化，把关键词也显示在title标题里，可改成</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;% block title %&#125; &#123;&#123; theme.keywords &#125;&#125; - &#123;&#123; config.title &#125;&#125;&#123;&#123; theme.description &#125;&#125; &#123;% endblock %&#125;</div></pre></td></tr></table></figure>
<p>注意：别堆砌关键字，整个标题一般不超过80个字符，可以通过chinaz的seo综合查询检查。</p>
<h3 id="给你的博客添加sitemap站点地图"><a href="#给你的博客添加sitemap站点地图" class="headerlink" title="给你的博客添加sitemap站点地图"></a>给你的博客添加sitemap站点地图</h3><p>安装sitemap站点地图自动生成插件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">npm install hexo-generator-sitemap --save</div><div class="line">npm install hexo-generator-baidu-sitemap --save</div></pre></td></tr></table></figure>
<p>在主题配置文件中添加一下配置,这里有的文章说的是在站点配置文章中添加，这个应该问题不大。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sitemap: </div><div class="line">  path: sitemap.xml</div><div class="line">baidusitemap:</div><div class="line">  path: baidusitemap.xml</div></pre></td></tr></table></figure>
<p>注意：上面的格式一定要正确，一定要有缩进，不然会出错，我想信很多小伙伴因为没有缩进而不能编译的。</p>
<p>然后在<strong>主题配置文件</strong>中修改<strong>url</strong>为你的域名，例如我的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">url: http://hunao.info</div></pre></td></tr></table></figure>
<p>NexT主题默认的是<code>http://yoursite.com</code></p>
<p>配置好后，<code>hexo g</code> 就能在<strong>your-hexo-site\public</strong> 中生成<strong>sitemap.xml</strong> 和 <strong>baidusitemap.xml</strong>了;其中第一个是一会要提交给google的，后面那个看名字当然就是提交给Baidu的了；</p>
<p>在<code>your-hexo-site\source</code>中新建文件<strong>robots.txt</strong>,内容可以参照我的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">User-agent: *</div><div class="line">Allow: /</div><div class="line">Allow: /archives/</div><div class="line">Allow: /categories/</div><div class="line">Allow: /tags/ </div><div class="line">Disallow: /vendors/</div><div class="line">Disallow: /js/</div><div class="line">Disallow: /css/</div><div class="line">Disallow: /fonts/</div><div class="line">Disallow: /vendors/</div><div class="line">Disallow: /fancybox/</div></pre></td></tr></table></figure>
<p><strong>其中Allow后面的就是你的menu；</strong></p>
<p>在<strong>robots.txt</strong>中添加下面的代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Sitemap: http://www.hunao.info/sitemap.xml</div><div class="line">Sitemap: http://www.hunao.info/baidusitemap.xml</div></pre></td></tr></table></figure>
<p>请自行将<strong>www.hunao.info</strong>改成自己的域名, 然后<code>hexo d -g</code>提交一下</p>
<a id="more"></a>
<h3 id="给非友情链接的出站链接添加-“nofollow”-标签"><a href="#给非友情链接的出站链接添加-“nofollow”-标签" class="headerlink" title="给非友情链接的出站链接添加 “nofollow” 标签"></a>给非友情链接的出站链接添加 “nofollow” 标签</h3><p>这个可以参考文章末尾提供的参考链接，写的很详细，我就不贴出来了。</p>
<h3 id="注册Google-Search-Console"><a href="#注册Google-Search-Console" class="headerlink" title="注册Google Search Console"></a>注册Google Search Console</h3><p>链接：<a href="https://www.google.com/webmasters/" target="_blank" rel="external">https://www.google.com/webmasters/</a></p>
<p>根据提示注册好之后，添加你的博客域名。</p>
<p><img src="http://i.imgur.com/LkWvicZ.png" alt="google console"></p>
<p>如图，我添加了两个，你可以视情况而定。然后点击你的域名进入:</p>
<p><img src="http://i.imgur.com/7ehsLCS.png" alt="管理界面"></p>
<h3 id="测试robots-txt"><a href="#测试robots-txt" class="headerlink" title="测试robots.txt"></a>测试robots.txt</h3><p>点击左侧的<strong>robots.txt</strong>测试工具，根据提示提交你的<strong>robots.txt</strong>，其实刚才我们已经提交了。</p>
<p><img src="http://i.imgur.com/zcCe2ep.png" alt="robots"></p>
<p>注意要0错误才可以，如果有错误的话，会有提示，改正确就可以了。</p>
<h3 id="提交站点地图"><a href="#提交站点地图" class="headerlink" title="提交站点地图"></a>提交站点地图</h3><p>还记得我们刚才创建创建sitemap.xml文件吧,现在它要派上用场了。点击左侧工具栏的站点地图</p>
<p><img src="http://i.imgur.com/jMkaqRv.png" alt="站点地图"></p>
<p>这里我已经添加了，所以你看到的和我看到界面应该不一样，然后点右上角的添加/测试站点地图。输入sitemap先点测试，如果没问题的话，再提交。</p>
<p><img src="http://i.imgur.com/gIURT3t.png" alt="添加sitemap"></p>
<h3 id="Google-抓取方式"><a href="#Google-抓取方式" class="headerlink" title="Google 抓取方式"></a>Google 抓取方式</h3><p>提交站点地图之后，点击左侧的Google 抓取方式</p>
<p><img src="http://i.imgur.com/VWnQNUs.png" alt="google抓取方式"></p>
<p>这一步很重要！这一不很重要！这一步很重要！</p>
<p><img src="http://i.imgur.com/NdZ6zFx.png" alt="抓取url"></p>
<p>在这里我们填上我们需要抓取的url,不填这表示抓取首页，抓取方式可以选择桌面，智能手机等等，自行根据需要选择。填好url之后，点击抓取.</p>
<p>然后可能会出现几种情况，如:完成、部分完成、重定向等，自由这三种情况是可以提交的。<br>提交完成后，提交至索引，根据提示操作就可以了，我的提交：</p>
<p><img src="http://i.imgur.com/etmgeUL.png" alt="提交"></p>
<p>至此，你的博客在google搜索上排名想不靠前都难了，马上上google搜索一下你的关键词和博客title测试一下吧；</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://www.arao.me/2015/hexo-next-theme-optimize-seo/" target="_blank" rel="external">动动手指，不限于NexT主题的Hexo优化（SEO篇）</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://hunao.info/2016/06/01/Hexo-Seo%E4%BC%98%E5%8C%96%E8%AE%A9%E4%BD%A0%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%9C%A8google%E6%90%9C%E7%B4%A2%E6%8E%92%E5%90%8D%E7%AC%AC%E4%B8%80/&quot;&gt;This blog&lt;/a&gt; is very helpful and practical tutorial of how to do SEO on Hexo and Next theme, as well as how to use Google Search Console as a general tool for SEO.&lt;/p&gt;
&lt;p&gt;I’ve followed the tutorial to tune up my site configs. Try to search my blogs on google!&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Hexo-SEO-优化让你的博客在google搜索排名第一&quot;&gt;&lt;a href=&quot;#Hexo-SEO-优化让你的博客在google搜索排名第一&quot; class=&quot;headerlink&quot; title=&quot;Hexo SEO 优化让你的博客在google搜索排名第一&quot;&gt;&lt;/a&gt;Hexo SEO 优化让你的博客在google搜索排名第一&lt;/h2&gt;&lt;p&gt;刚刚建买了域名建了博客，发现在google，百度毛都搜不到，真是悲伤，后来才知道原来是要seo的，所以看了一些文章，然后自己也摸索了一下，终于在让自己的博客在google搜索排名第一了!!!上图！&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://i.imgur.com/eA6Jesb.png&quot; alt=&quot;google更新了&quot;&gt;&lt;/p&gt;
&lt;p&gt;哈哈，顿时觉得自己好厉害耶!&lt;br&gt;下面给大家分享一些我的经验。&lt;/p&gt;
&lt;h3 id=&quot;本教程在NexT主题上操作，其他主题请自行测试&quot;&gt;&lt;a href=&quot;#本教程在NexT主题上操作，其他主题请自行测试&quot; class=&quot;headerlink&quot; title=&quot;本教程在NexT主题上操作，其他主题请自行测试&quot;&gt;&lt;/a&gt;本教程在NexT主题上操作，其他主题请自行测试&lt;/h3&gt;&lt;h3 id=&quot;首页title优化&quot;&gt;&lt;a href=&quot;#首页title优化&quot; class=&quot;headerlink&quot; title=&quot;首页title优化&quot;&gt;&lt;/a&gt;首页title优化&lt;/h3&gt;&lt;p&gt;更改&lt;strong&gt;index.swig&lt;/strong&gt;文件&lt;strong&gt;(your-hexo-site\themes\next\layout)&lt;/strong&gt;;&lt;/p&gt;
&lt;p&gt;将下面这段代码&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&amp;#123;% block title %&amp;#125; &amp;#123;&amp;#123; config.title &amp;#125;&amp;#125; &amp;#123;% endblock %&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;改成&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&amp;#123;% block title %&amp;#125; &amp;#123;&amp;#123; config.title &amp;#125;&amp;#125; - &amp;#123;&amp;#123; theme.description &amp;#125;&amp;#125; &amp;#123;% endblock %&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这时候你的首页会更符合&lt;strong&gt;网站名称 - 网站描述&lt;/strong&gt;这习惯。&lt;br&gt;进阶，做了 SEO 优化，把关键词也显示在title标题里，可改成&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&amp;#123;% block title %&amp;#125; &amp;#123;&amp;#123; theme.keywords &amp;#125;&amp;#125; - &amp;#123;&amp;#123; config.title &amp;#125;&amp;#125;&amp;#123;&amp;#123; theme.description &amp;#125;&amp;#125; &amp;#123;% endblock %&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;注意：别堆砌关键字，整个标题一般不超过80个字符，可以通过chinaz的seo综合查询检查。&lt;/p&gt;
&lt;h3 id=&quot;给你的博客添加sitemap站点地图&quot;&gt;&lt;a href=&quot;#给你的博客添加sitemap站点地图&quot; class=&quot;headerlink&quot; title=&quot;给你的博客添加sitemap站点地图&quot;&gt;&lt;/a&gt;给你的博客添加sitemap站点地图&lt;/h3&gt;&lt;p&gt;安装sitemap站点地图自动生成插件&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;npm install hexo-generator-sitemap --save&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;npm install hexo-generator-baidu-sitemap --save&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;在主题配置文件中添加一下配置,这里有的文章说的是在站点配置文章中添加，这个应该问题不大。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;sitemap: &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  path: sitemap.xml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;baidusitemap:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;  path: baidusitemap.xml&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;注意：上面的格式一定要正确，一定要有缩进，不然会出错，我想信很多小伙伴因为没有缩进而不能编译的。&lt;/p&gt;
&lt;p&gt;然后在&lt;strong&gt;主题配置文件&lt;/strong&gt;中修改&lt;strong&gt;url&lt;/strong&gt;为你的域名，例如我的&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;url: http://hunao.info&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;NexT主题默认的是&lt;code&gt;http://yoursite.com&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;配置好后，&lt;code&gt;hexo g&lt;/code&gt; 就能在&lt;strong&gt;your-hexo-site\public&lt;/strong&gt; 中生成&lt;strong&gt;sitemap.xml&lt;/strong&gt; 和 &lt;strong&gt;baidusitemap.xml&lt;/strong&gt;了;其中第一个是一会要提交给google的，后面那个看名字当然就是提交给Baidu的了；&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;your-hexo-site\source&lt;/code&gt;中新建文件&lt;strong&gt;robots.txt&lt;/strong&gt;,内容可以参照我的&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;User-agent: *&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Allow: /&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Allow: /archives/&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Allow: /categories/&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Allow: /tags/ &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Disallow: /vendors/&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Disallow: /js/&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Disallow: /css/&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Disallow: /fonts/&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Disallow: /vendors/&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Disallow: /fancybox/&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;其中Allow后面的就是你的menu；&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;robots.txt&lt;/strong&gt;中添加下面的代码&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;Sitemap: http://www.hunao.info/sitemap.xml&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Sitemap: http://www.hunao.info/baidusitemap.xml&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;请自行将&lt;strong&gt;www.hunao.info&lt;/strong&gt;改成自己的域名, 然后&lt;code&gt;hexo d -g&lt;/code&gt;提交一下&lt;/p&gt;
    
    </summary>
    
      <category term="hexo" scheme="https://phoenixjiangnan.github.io/categories/hexo/"/>
    
    
      <category term="Hexo SEO" scheme="https://phoenixjiangnan.github.io/tags/Hexo-SEO/"/>
    
      <category term="Hexo SEO tutorial" scheme="https://phoenixjiangnan.github.io/tags/Hexo-SEO-tutorial/"/>
    
  </entry>
  
  <entry>
    <title>The Always On Architecture - Moving Beyond Legacy Disaster Recovery</title>
    <link href="https://phoenixjiangnan.github.io/2016/10/11/distributed%20system/The-Always-On-Architecture-Moving-Beyond-Legacy-Disaster-Recovery/"/>
    <id>https://phoenixjiangnan.github.io/2016/10/11/distributed system/The-Always-On-Architecture-Moving-Beyond-Legacy-Disaster-Recovery/</id>
    <published>2016-10-11T07:49:59.000Z</published>
    <updated>2016-10-10T16:52:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post caught my eyes immediatelly when I browsed my <a href="http://feedly.com/" target="_blank" rel="external">Feedly</a>.</p>
<p>I’ve been working on Tableau Server, an on-premise data analytics and visualization platform for enterprise, for about two years. We used a Postgres failover approach in Tableau Server, as described in <a href="https://phoenixjiangnan.github.io/2016/07/26/system%20design/project%20experience/How-did-I-reduce-Tableau-Server-s-downtime-in-a-Postgres-failover-from-3-min-to-1-sec/">this post</a>, and I’ve been fixing bugs for the failover mechanism for a while. The experience and pain from fixing those bugs have made me fully realized how outdated the failover approach is, while we don’t have a good solution because we are building an expensive on-premise software with so many limitations around it. We cannot assume all customers can use AWS RDS (although Tableau is making it possible so that you can if you want to), we cannot assume customers can deploy Tableau Server in more than one data center, we cannot assume a ton of things….. Those limitations have made me quite tired of my work, because they restrict my imaginations and innovations, distort the optimal way to fix problems, impede our efforts of debugging, and prevent us from adopting best practices in the industry.</p>
<p>Yeah, if you are developing an open-source project, those problems might not be big things because your project is FREE and how can customers complain about you! </p>
<p>After some careful thoughts, I believe I need a change in my career. This post is one of the triggers.</p>
<hr>
<p><a href="http://highscalability.com/blog/2016/8/23/the-always-on-architecture-moving-beyond-legacy-disaster-rec.html" target="_blank" rel="external">The original post</a></p>
<hr>
<blockquote>
<p>Failover does not cut it anymore. You need an <code>ALWAYS ON</code> architecture with multiple data centers.<br>– Martin Van Ryswyk, VP of Engineering at DataStax</p>
</blockquote>
<p>Failover, switching to a redundant or standby system when a component fails, has a long and checkered history as a way of dealing with failure. The reason is your failover mechanism becomes a single point of failure that often fails just when it’s needed most. Having worked on a few telecom systems that used a failover strategy I know exactly how stressful failover events can be and how stupid you feel when your failover fails. If you have a double or triple fault in your system, failover is exactly the time when it will happen. </p>
<p>For a long time the only real trick we had for achieving fault tolerance was to have a hot, warm, or cold standby (disk, interface, card, server, router, generator, datacenter, etc.) and failover to it when there’s a problem. This old style of Disaster Recovery planning is no longer adequate or necessary.</p>
<p>Now, thanks to cloud infrastructures, at least at a software system level, we have an alternative: <strong>an always on architecture</strong>. Google calls this <strong>a natively multihomed architecture</strong>. You can distribute data across multiple datacenters in such away that all your datacenters are always active. Each datacenter can automatically scale capacity up and down depending on what happens to other datacenters. You know, the usual sort of cloud propaganda. Robin Schumacher makes a good case here: Long live Dear CXO – When Will What Happened to Delta Happen to You?</p>
<a id="more"></a>
<h2 id="Recent-Problems-With-Disaster-Recovery"><a href="#Recent-Problems-With-Disaster-Recovery" class="headerlink" title="Recent Problems With Disaster !Recovery"></a>Recent Problems With Disaster !Recovery</h2><p>Southwest had a service disruption a year ago and again was recently bit by a “once in a thousand-year event” that caused a service outage (doesn’t it seem like once in 1000 year events happen a lot more lately?). The first incident was blamed on “legacy systems” that could no longer handle the increased load of a now much larger airline. The most recent problem was caused by a router partially failed so the failover mechanism didn’t kick in. 2,300 flights were canceled over four days at cost of perhaps $10 million. When you do your system’s engineering review do you consider partial failures? Probably not. Yet they happen and are notoriously hard to detect and deal with.</p>
<p>Sprint has also experienced bad backup problems:</p>
<pre><code>Sprint said a fire in D.C. caused problems at Sprint&apos;s data center in Reston, VA. How a fire across the street from Sprint&apos;s switch in D.C. caused issues 20 miles away wasn&apos;t quite clear, but apparently, emergency Sprint generators in D.C. didn&apos;t kick in as they were supposed to and, as so often happens, one thing led to another.
</code></pre><p>And unless you were on Mars, you will have heard Delta recently experienced their own failover problems:</p>
<pre><code>According to the flight captain of JFK-SLC this morning, a routine scheduled switch to the backup generator this morning at 2:30am caused a fire that destroyed both the backup and the primary. Firefighters took a while to extinguish the fire. Power is now back up and 400 out of the 500 servers rebooted, still waiting for the last 100 to have the whole system fully functional
</code></pre><p>Delta has come under a lot of criticism. Why was the backup generator so close to the primary that a fire could destroy both? Why is the entire worldwide system running in a single datacenter? Why don’t they test more? Why don’t they have full failover to a backup datacenter? Why are they more interested in cutting costs the building a reliable system? Why do they still use those old mainframes? Why does a company that earns $42 billion a year have such crappy systems? It’s only 500 servers, why not convert it to a cluster already? Why does management only care about their bonuses and cutting IT costs? Isn’t that what you get from years out of outsourcing IT?</p>
<p>There’s a lot of venom, as you might expect. If you want a little background on Delta’s systems then here’s a good article: <a href="http://www.wsj.com/articles/SB10001424052702303480304579575891541812918" target="_blank" rel="external">Delta Air Lines to Take Control of Its Data Systems</a>. It appears that as of 2014 Delta spun in all its passenger-service and flight operations systems. They had 180 proprietary Delta technology applications controlling their ticketing, website, flight check-in, crew scheduling and more. And they spent about $250 million a year on IT.</p>
<h2 id="Does-The-Whole-System-Need-A-Refactoring"><a href="#Does-The-Whole-System-Need-A-Refactoring" class="headerlink" title="Does The Whole System Need A Refactoring?"></a>Does The Whole System Need A Refactoring?</h2><p>Interesting comment on the technology involved in these systems from FormerRTCoverageAA: </p>
<pre><code>My advice is for ALL the major airlines to each put in about 10 million dollars (20-30 airlines would put a fund together about 200-300 million) to modernize and work on the Interfaces between them, and the hotel and car rental systems, tours, and other functions that SABRE/Amadeus/Apollo/etc. interface to. This would fund a research consortium to look at the current technology, and DEFINE THE INTERFACES for the next generation system. Maybe HP and IBM and Microsoft and whoever else wants to play could put in some money too. The key for this consortium is to have the INTERFACES defined. Give the specifications to the vendors (HP, IBM, Microsoft, Google, Priceline, Hilton, Hertz, whoever) that want to build the next generation reservations system. Then let them have 1 year and all have to work to inter-operate on the specification (just like they do on the “old” specs today for things like teletype, and last seat availability).

This has worked well in the healthcare space in getting payers and providers to work together. Each potential vendor needs to plan to spend 10-50 million dollars on their proposed solution. Then, we have the inter-operability technology fair (I would make it 2 weeks to 1 month) and each vendor can pitch to each airline, car rental, hotel, tour company, Uber, etc. Let each vendor do what he wants as long as the requirements for the specifications are met. Let the best tech vendor win.

It’s far past time to update these systems. Otherwise, more heartache pain and probably government bailouts to come. Possibly even larger travel and freight interruptions. A longer term blow up could put an airline out of business. Remember Eastern? I do….
</code></pre><p>This all sounds like a great idea, but what could Delta do with its own architecture?</p>
<h2 id="The-Always-On-Architecture"><a href="#The-Always-On-Architecture" class="headerlink" title="The Always On Architecture"></a>The Always On Architecture</h2><p>Earlier this year I wrote on article on a paper from Google: <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44686.pdf" target="_blank" rel="external">High-Availability at Massive Scale: Building Google’s Data Infrastructure for Ads</a> that explains their history with Always On. It seems appropriate. Here’s the article:</p>
<p><img src="https://c2.staticflickr.com/2/1510/25190447426_c478660902_o.png" alt=""></p>
<p>The main idea of the paper is that <a href="https://en.wikipedia.org/wiki/Failover" target="_blank" rel="external">the typical failover</a> architecture used when moving from a single datacenter to multiple datacenters doesn’t work well in practice. What does work, where work means using fewer resources while providing high availability and consistency, is a <strong>natively multihomed architecture</strong>:</p>
<blockquote>
<p>Our current approach is to build natively multihomed systems. Such systems <strong>run hot in multiple datacenters all the time, and adaptively move load between datacenters, with the ability to handle outages of any scale completely transparently</strong>. Additionally, planned datacenter outages and maintenance events are completely transparent, causing minimal disruption to the operational systems. In the past, such events required labor-intensive efforts to move operational systems from one datacenter to another</p>
</blockquote>
<p>The use of “multihoming” in this context may be confusing because <a href="https://en.wikipedia.org/wiki/Multihoming" target="_blank" rel="external">multihoming</a> usually refers to a computer connected to more than one network. At Google scale perhaps it’s just as natural to talk about connecting to multiple datacenters.</p>
<p>Google has built several multi-homed systems to guarantee high availability (4 to 5 nines) and consistency in the presence of datacenter level outages: <a href="http://research.google.com/pubs/pub38125.html" target="_blank" rel="external">F1 / Spanner: Relational Database</a>; <a href="http://research.google.com/pubs/pub41318.html" target="_blank" rel="external">Photon: Joining Continuous Data Streams</a>; <a href="http://research.google.com/pubs/pub42851.html" target="_blank" rel="external">Mesa: Data Warehousing</a>. The approach taken by each of these systems is discussed in the paper, as are the many challenges is building a multi-homed system: Synchronous Global State; What to Checkpoint; Repeatable Input; Exactly Once Output.</p>
<p>The huge constraint here is <strong>having availability and consistency</strong>. This highlights the refreshing and continued emphasis Google puts on making even these complex systems <strong>easy for programmers to use</strong>:</p>
<blockquote>
<p>The simplicity of a multi-homed system is particularly valuable for users. Without multi-homing, failover, recovery, and dealing with inconsistency are all application problems. With multi-homing, these hard problems are solved by the infrastructure, so the application developer gets high availability and consistency for free and can focus instead on building their application.</p>
</blockquote>
<p>The biggest surprise in the paper was the idea that a <strong>multihomed system can actually take far fewer resources than a failover system</strong>:</p>
<blockquote>
<p>In a multi-homed system deployed in three datacenters with 20% total catchup capacity, the total resource footprint is 170% of steady state. This is dramatically less than the 300% required in the failover design above</p>
</blockquote>
<h2 id="What’s-Wrong-With-Failover"><a href="#What’s-Wrong-With-Failover" class="headerlink" title="What’s Wrong With Failover?"></a>What’s Wrong With Failover?</h2><blockquote>
<p>Failover-based approaches, however, do not truly achieve high availability, and can have excessive cost due to the deployment of standby resources.</p>
<p>Our teams have had several bad experiences dealing with failover-based systems in the past. Since unplanned outages are rare, failover procedures were often added as an afterthought, not automated and not well tested. On multiple occasions, teams spent days recovering from an outage, bringing systems back online component by component, recovering state with ad hoc tools like custom MapReduces, and gradually tuning the system as it tried to catch up processing the backlog starting from the initial outage. These situations not only cause extended unavailability, but are also extremely stressful for the teams running complex mission-critical systems.</p>
</blockquote>
<h2 id="How-Do-Multihomed-Systems-Work"><a href="#How-Do-Multihomed-Systems-Work" class="headerlink" title="How Do Multihomed Systems Work?"></a>How Do Multihomed Systems Work?</h2><blockquote>
<p>In contrast, multi-homed systems are designed to run in multiple datacenters as a core design property, so there is no on-the-side failover concept. A multi-homed system runs live in multiple datacenters all the time. Each datacenter processes work all the time, and work is dynamically shared between datacenters to balance load. When one datacenter is slow, some fraction of work automatically moves to faster datacenters. When a datacenter is completely unavailable, all its work is automatically distributed to other datacenters.</p>
<p>There is no failover process other than the continuous dynamic load balancing. Multi-homed systems coordinate work across datacenters using shared global state that must be updated synchronously. All critical system state is replicated so that any work can be restarted in an alternate datacenter at any point, while still guaranteeing exactly once semantics. Multi-homed systems are uniquely able to provide high availability and full consistency in the presence of datacenter level failures.</p>
<p>In any of our typical streaming system, the events being processed are based on user interactions, and logged by systems serving user traffic in many datacenters around the world. A log collection service gathers these logs globally and copies them to two or more specific logs datacenters. Each logs datacenter gets a complete copy of the logs, with the guarantee that all events copied to any one datacenter will (eventually) be copied to all logs datacenters. The stream processing systems run in one or more of the logs datacenters and processes all events. Output from the stream processing system is usually stored into some globally replicated system so that the output can be consumed reliably from anywhere.</p>
<p>In a multi-homed system, all datacenters are live and processing all the time. Deploying three datacenters is typical. In steady state, each of the three datacenters process 33% of the traffic. After a failure where one datacenter is lost, the two remaining datacenters each process 50% of the traffic. </p>
</blockquote>
<p>Obviously Delta and other companies with extensive legacy systems are in a difficult position for this kind of approach. But if you consider IT something other than a cost center, and you plan to stay around for the long haul, and whole nations rely on the quality of your infrastructure, it’s probably something you should consider. We have the technology.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post caught my eyes immediatelly when I browsed my &lt;a href=&quot;http://feedly.com/&quot;&gt;Feedly&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ve been working on Tableau Server, an on-premise data analytics and visualization platform for enterprise, for about two years. We used a Postgres failover approach in Tableau Server, as described in &lt;a href=&quot;https://phoenixjiangnan.github.io/2016/07/26/system%20design/project%20experience/How-did-I-reduce-Tableau-Server-s-downtime-in-a-Postgres-failover-from-3-min-to-1-sec/&quot;&gt;this post&lt;/a&gt;, and I’ve been fixing bugs for the failover mechanism for a while. The experience and pain from fixing those bugs have made me fully realized how outdated the failover approach is, while we don’t have a good solution because we are building an expensive on-premise software with so many limitations around it. We cannot assume all customers can use AWS RDS (although Tableau is making it possible so that you can if you want to), we cannot assume customers can deploy Tableau Server in more than one data center, we cannot assume a ton of things….. Those limitations have made me quite tired of my work, because they restrict my imaginations and innovations, distort the optimal way to fix problems, impede our efforts of debugging, and prevent us from adopting best practices in the industry.&lt;/p&gt;
&lt;p&gt;Yeah, if you are developing an open-source project, those problems might not be big things because your project is FREE and how can customers complain about you! &lt;/p&gt;
&lt;p&gt;After some careful thoughts, I believe I need a change in my career. This post is one of the triggers.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;http://highscalability.com/blog/2016/8/23/the-always-on-architecture-moving-beyond-legacy-disaster-rec.html&quot;&gt;The original post&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;Failover does not cut it anymore. You need an &lt;code&gt;ALWAYS ON&lt;/code&gt; architecture with multiple data centers.&lt;br&gt;– Martin Van Ryswyk, VP of Engineering at DataStax&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Failover, switching to a redundant or standby system when a component fails, has a long and checkered history as a way of dealing with failure. The reason is your failover mechanism becomes a single point of failure that often fails just when it’s needed most. Having worked on a few telecom systems that used a failover strategy I know exactly how stressful failover events can be and how stupid you feel when your failover fails. If you have a double or triple fault in your system, failover is exactly the time when it will happen. &lt;/p&gt;
&lt;p&gt;For a long time the only real trick we had for achieving fault tolerance was to have a hot, warm, or cold standby (disk, interface, card, server, router, generator, datacenter, etc.) and failover to it when there’s a problem. This old style of Disaster Recovery planning is no longer adequate or necessary.&lt;/p&gt;
&lt;p&gt;Now, thanks to cloud infrastructures, at least at a software system level, we have an alternative: &lt;strong&gt;an always on architecture&lt;/strong&gt;. Google calls this &lt;strong&gt;a natively multihomed architecture&lt;/strong&gt;. You can distribute data across multiple datacenters in such away that all your datacenters are always active. Each datacenter can automatically scale capacity up and down depending on what happens to other datacenters. You know, the usual sort of cloud propaganda. Robin Schumacher makes a good case here: Long live Dear CXO – When Will What Happened to Delta Happen to You?&lt;/p&gt;
    
    </summary>
    
      <category term="distributed system" scheme="https://phoenixjiangnan.github.io/categories/distributed-system/"/>
    
    
      <category term="always on architecture" scheme="https://phoenixjiangnan.github.io/tags/always-on-architecture/"/>
    
      <category term="failover" scheme="https://phoenixjiangnan.github.io/tags/failover/"/>
    
  </entry>
  
  <entry>
    <title>Repost - Job-Hopping and Salary Raise 跳槽加薪</title>
    <link href="https://phoenixjiangnan.github.io/2016/10/10/career%20and%20leadership/Repost-Job-Hopping-and-Salary-Raise-%E8%B7%B3%E6%A7%BD%E5%8A%A0%E8%96%AA/"/>
    <id>https://phoenixjiangnan.github.io/2016/10/10/career and leadership/Repost-Job-Hopping-and-Salary-Raise-跳槽加薪/</id>
    <published>2016-10-10T07:13:13.000Z</published>
    <updated>2016-10-09T16:15:31.000Z</updated>
    
    <content type="html"><![CDATA[<hr>
<p>From <a href="http://weibo.com/1134424202/E50IpFN45?from=page_1005051134424202_profile&amp;wvr=6&amp;mod=weibotime&amp;type=comment" target="_blank" rel="external">http://weibo.com/1134424202/E50IpFN45?from=page_1005051134424202_profile&amp;wvr=6&amp;mod=weibotime&amp;type=comment</a></p>
<hr>
<p>互联网行业“跳槽加薪”这件事，除了常见解释，还有一个隐性的理由。</p>
<p>对于公司内部，加薪体系通常是线性的，类似于每年+10-20%这样。但是在人才市场上存在非线性的涨幅。</p>
<p>举例来说，一位1年经验，资历尚浅的普通程序员，薪水12-15K是可以接受的。进入公司工作了两年，每年+20%，薪水18-22K。两年后，他已经从普通程序员成长为能独当一面的优秀程序员，而这个位置的市场价格是25-35K。</p>
<p>然后他跳个槽，30K，比公司给出的加薪幅度高10K。</p>
<p>公司只好招聘新人来弥补这个位置，顶替缺口的是另一位独当一面的优秀程序员，薪水同样是30K。这时围观群众纷纷嘲笑HR是傻逼，老板是臭傻逼。</p>
<p>之所以出现这种情况，是因为管理者不容易精准地动态判断，一个人什么时候该线性加薪，什么时候该非线性加薪，哪里是员工市场价出现跳跃式增长的时间点。如果不考虑这个时间点的话，每年+10%-20%已经是很不错的待遇了。而每个人每年来一次非线性加薪也是不合理的，公司分分钟破产。</p>
<p>大家都知道，跳个槽加薪多，原地不动加薪少。老板个个都是臭傻逼。但“老板个个都是臭傻逼”这件事其实不符合常识，毕竟很多“老板”其实也只是高级经理人，掏钱发工资的又不是他，得力下属都跑掉了，并不符合他的利益。</p>
<p>从结果上来说，当管理者不能判断非线性加薪的时间点时，跳个槽，由市场决定自己的新身价，是最合理的解决方案。</p>
]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;From &lt;a href=&quot;http://weibo.com/1134424202/E50IpFN45?from=page_1005051134424202_profile&amp;amp;wvr=6&amp;amp;mod=weibotime&amp;amp;type=comment&quot;
    
    </summary>
    
      <category term="career and leadership" scheme="https://phoenixjiangnan.github.io/categories/career-and-leadership/"/>
    
    
      <category term="job hopping" scheme="https://phoenixjiangnan.github.io/tags/job-hopping/"/>
    
      <category term="跳槽加薪" scheme="https://phoenixjiangnan.github.io/tags/%E8%B7%B3%E6%A7%BD%E5%8A%A0%E8%96%AA/"/>
    
  </entry>
  
  <entry>
    <title>Cache - Cache Design Patterns</title>
    <link href="https://phoenixjiangnan.github.io/2016/10/08/system%20design/cache/Cache-Cache-Design-Patterns/"/>
    <id>https://phoenixjiangnan.github.io/2016/10/08/system design/cache/Cache-Cache-Design-Patterns/</id>
    <published>2016-10-08T17:54:44.000Z</published>
    <updated>2016-10-08T02:56:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>I read a blog a couple months ago talking about four most common cache design patterns, and want to summarize that post and add my understandings here.</p>
<h2 id="Naive-Approach"><a href="#Naive-Approach" class="headerlink" title="Naive Approach"></a>Naive Approach</h2><p>The naive approach is, when update data in cache service, first remove cached data, update data source, and then reload data from data source to cache.</p>
<p>However, the logic is wrong!</p>
<p>For example, there are two concurrent operation, one being update operation and the other being query operation. After update operation removed cached data, query operation doesn’t hit the target and triggers a reload of outdated data from data store, and then the update operation modifies data store. Thus, the cache now contains dirty/expired data, and will continue to hold such dirty data.</p>
<p>I’m gonna discuss four caching strategies/design patterns today.</p>
<ul>
<li>Read Through</li>
<li>Write Through</li>
<li>Cache Aside</li>
<li>Write Behind Caching</li>
</ul>
<p>Note that they’re design patterns, neither mechanisms of databases like MySQL or Postgres, nor how Memcache or Redis works.</p>
<h2 id="Read-Write-Through"><a href="#Read-Write-Through" class="headerlink" title="Read / Write Through"></a>Read / Write Through</h2><p>This is where the application treats cache as the main data store and reads data from it and writes data to it. The cache is responsible for reading and writing this data to the database, thereby relieving the application of this responsibility.</p>
<a id="more"></a>
<h2 id="1-Read-Through"><a href="#1-Read-Through" class="headerlink" title="1. Read Through"></a>1. Read Through</h2><p><code>Read Through</code> is used in querying operations to update cached data.</p>
<p>When cached data is evicted (due to expiration or LRU eviction), it’s the application’s responsibility to load new data into cache in <code>Cache Aside</code> strategy , while it’s cache service’s responsibility in <code>Read Through</code> strategy. This function brings transparency of caching to application. </p>
<h2 id="2-Write-Through"><a href="#2-Write-Through" class="headerlink" title="2. Write Through"></a>2. Write Through</h2><p><code>Write Through</code> is used in data updating operations to update cached data.</p>
<p>Whenever application updates a piece of data</p>
<ul>
<li>if data is not in cache, cache service directly update data store</li>
<li>if data is in cache, cache service will first update itself, and then update database. A write is done synchronously both to the cache and to the backing store, conforming a single transaction.</li>
</ul>
<h2 id="3-Cache-Aside"><a href="#3-Cache-Aside" class="headerlink" title="3. Cache Aside"></a>3. Cache Aside</h2><p><a href="https://msdn.microsoft.com/en-us/library/dn589799.aspx" target="_blank" rel="external">This article</a> explains Cache Aside strategy very well. I’ll grab some content from there.</p>
<p>For caches that do not provide read/write-through functionality, it is the responsibility of the applications that use the cache to maintain the data in the cache. An application can emulate the functionality of read-through caching by implementing the cache-aside strategy. This strategy effectively loads data into the cache on demand. The following figure summarizes the steps in this process.</p>
<p><img src="https://i-msdn.sec.s-msft.com/dynimg/IC709568.png" alt=""></p>
<p>If an application updates information, it can emulate the write-through strategy as follows:</p>
<ul>
<li>Make the modification to the data store</li>
<li>Invalidate the corresponding item in the cache.</li>
</ul>
<p>When the item is next required, using the cache-aside strategy will cause the updated data to be retrieved from the data store and added back into the cache.</p>
<h3 id="Issues-and-Considerations"><a href="#Issues-and-Considerations" class="headerlink" title="Issues and Considerations"></a>Issues and Considerations</h3><p>Consider the following points when deciding how to implement this pattern:</p>
<ul>
<li><p><strong>Lifetime of Cached Data</strong>. Many caches implement an expiration policy that causes data to be invalidated and removed from the cache if it is not accessed for a specified period. For cache-aside to be effective, ensure that the expiration policy matches the pattern of access for applications that use the data. Do not make the expiration period too short because this can cause applications to continually retrieve data from the data store and add it to the cache. Similarly, do not make the expiration period so long that the cached data is likely to become stale. Remember that caching is most effective for relatively static data, or data that is read frequently.</p>
</li>
<li><p><strong>Evicting Data</strong>. Most caches have only a limited size compared to the data store from where the data originates, and they will evict data if necessary. Most caches adopt a least-recently-used policy for selecting items to evict, but this may be customizable. Configure the global expiration property and other properties of the cache, and the expiration property of each cached item, to help ensure that the cache is cost effective. It may not always be appropriate to apply a global eviction policy to every item in the cache. For example, if a cached item is very expensive to retrieve from the data store, it may be beneficial to retain this item in cache at the expense of more frequently accessed but less costly items.</p>
</li>
<li><p><strong>Priming the Cache</strong>. Many solutions prepopulate the cache with the data that an application is likely to need as part of the startup processing. The Cache-Aside pattern may still be useful if some of this data expires or is evicted.</p>
</li>
<li><p><strong>Consistency</strong>. Implementing the Cache-Aside pattern does not guarantee consistency between the data store and the cache. An item in the data store may be changed at any time by an external process, and this change might not be reflected in the cache until the next time the item is loaded into the cache. In a system that replicates data across data stores, this problem may become especially acute if synchronization occurs very frequently.</p>
</li>
<li><p><strong>Local (In-Memory) Caching</strong>. A cache could be local to an application instance and stored in-memory. Cache-aside can be useful in this environment if an application repeatedly accesses the same data. However, a local cache is private and so different application instances could each have a copy of the same cached data. This data could quickly become inconsistent between caches, so it may be necessary to expire data held in a private cache and refresh it more frequently. In these scenarios it may be appropriate to investigate the use of a shared or a distributed caching mechanism.</p>
</li>
</ul>
<h3 id="When-to-Use-this-Pattern"><a href="#When-to-Use-this-Pattern" class="headerlink" title="When to Use this Pattern"></a>When to Use this Pattern</h3><p>Use this pattern when:</p>
<ul>
<li><p>A cache does not provide native read-through and write-through operations.</p>
</li>
<li><p>Resource demand is unpredictable. This pattern enables applications to load data on demand. It makes no assumptions about which data an application will require in advance.</p>
</li>
</ul>
<p>This pattern might not be suitable:</p>
<ul>
<li>When the cached data set is static. If the data will fit into the available cache space, prime the cache with the data on startup and apply a policy that prevents the data from expiring.</li>
<li>For caching session state information in a web application hosted in a web farm. In this environment, you should avoid introducing dependencies based on client-server affinity.</li>
</ul>
<h2 id="4-Write-Behind"><a href="#4-Write-Behind" class="headerlink" title="4. Write Behind"></a>4. Write Behind</h2><p>In the Write-Behind scenario, modified cache entries are written to cache, not data source. Updates in cache are asynchronously written to the data source after a configured delay, whether after 10 seconds, 20 minutes, a day, a week or even longer. Note that this only applies to cache inserts and updates - cache entries are removed synchronously from the data source.</p>
<p>For Write-Behind caching, cache service maintains a write-behind queue of the data that must be updated in the data source. When the application updates <code>X</code> in the cache, <code>X</code> is added to the write-behind queue (if it isn’t there already; otherwise, it is replaced), and after the specified write-behind delay, cache service will update the underlying data source with the latest state of <code>X</code>. Note that the write-behind delay is relative to the first of a series of modifications—in other words, the data in the data source will never lag behind the cache by more than the write-behind delay.</p>
<p>The result is a <code>read-once and write at a configured interval</code> (that is, much less often) scenario. There are four main benefits to this type of architecture:</p>
<ul>
<li><p>The application improves in performance, because the user does not have to wait for data to be written to the underlying data source. (The data is written later, and by a different execution thread.)</p>
</li>
<li><p>The application experiences drastically reduced database load: Since the amount of both read and write operations is reduced, so is the database load. The reads are reduced by caching, as with any other caching approach. The writes, which are typically much more expensive operations, are often reduced because multiple changes to the same object within the write-behind interval are <code>coalesced</code> and only written once to the underlying data source (<code>write-coalescing</code>). Additionally, writes to multiple cache entries may be combined into a single database transaction (<code>write-combining</code>) .</p>
</li>
<li><p>The application is somewhat insulated from database failures: the Write-Behind feature can be configured in such a way that a write failure will result in the object being re-queued for write. If the data that the application is using is in the cache, the application can continue operation without the database being up.</p>
</li>
<li><p>Linear Scalability: For an application to handle more concurrent users you need only increase the number of nodes in the cluster; the effect on the database in terms of load can be tuned by increasing the write-behind interval.</p>
</li>
</ul>
<blockquote>
<p>Write-behind strategy is actually Linux file system’s Page Cache. Because all updates are written asynchronously with a delay, Linux will lose some data updates when exit unexpectedly, e.g. machine being shut down.</p>
</blockquote>
<h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>Developers can leverage all the strategies demonstrated above, and combine some of them together in when designing caching strategies. For example, you can have a Read-Through Write-Behind cache layer.</p>
<hr>
<p>References:</p>
<ul>
<li><p><a href="http://docs.oracle.com/cd/E13924_01/coh.340/e13819/readthrough.htm" target="_blank" rel="external">http://docs.oracle.com/cd/E13924_01/coh.340/e13819/readthrough.htm</a></p>
</li>
<li><p><a href="https://msdn.microsoft.com/en-us/library/dn589799.aspx" target="_blank" rel="external">https://msdn.microsoft.com/en-us/library/dn589799.aspx</a></p>
</li>
</ul>
<hr>
<p>Here’s the <a href="http://coolshell.cn/articles/17416.html" target="_blank" rel="external">original post</a></p>
<hr>
<p>看来，有很多人是不知道缓存同步的几个Design Pattern的，如：<code>Cache aside</code>, <code>Read through</code>, <code>Write through</code>, <code>Write behind caching</code>…</p>
<blockquote>
<p>看到好些人在写更新缓存数据代码时，先删除缓存，然后再更新数据库，而后续的操作会把数据再装载的缓存中。然而，这个是逻辑是错误的。试想，两个并发操作，一个是更新操作，另一个是查询操作，更新操作删除缓存后，查询操作没有命中缓存，先把老数据读出来后放到缓存中，然后更新操作更新了数据库。于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。</p>
</blockquote>
<p>我不知道为什么这么多人用的都是这个逻辑，当我在微博上发了这个贴以后，我发现好些人给了好多非常复杂和诡异的方案，所以，我想写这篇文章说一下几个缓存更新的Design Pattern（让我们多一些套路吧）。</p>
<p>这里，我们先不讨论更新缓存和更新数据这两个事是一个事务的事，或是会有失败的可能，我们先假设<code>更新数据库</code>和<code>更新缓存</code>都可以成功的情况（我们先把成功的代码逻辑先写对）。</p>
<p>更新缓存的的Design Pattern有四种：<code>Cache aside</code>, <code>Read through</code>, <code>Write through</code>, <code>Write behind caching</code>，我们下面一一来看一下这四种Pattern。</p>
<h2 id="1-Cache-Aside-Pattern"><a href="#1-Cache-Aside-Pattern" class="headerlink" title="1. Cache Aside Pattern"></a>1. Cache Aside Pattern</h2><p>这是最常用最常用的pattern了。其具体逻辑如下：</p>
<ul>
<li>失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。</li>
<li>命中：应用程序从cache中取数据，取到后返回。</li>
<li>更新：先把数据存到数据库中，成功后，再让缓存失效。</li>
</ul>
<p><code>注意，我们的更新是先更新数据库，成功后，让缓存失效。</code> </p>
<p>那么，这种方式是否可以没有文章前面提到过的那个问题呢？我们可以脑补一下。</p>
<p>一个是查询操作，一个是更新操作的并发，首先，没有了删除cache数据的操作了，而是先更新了数据库中的数据，此时，缓存依然有效，所以，并发的查询操作拿的是没有更新的数据，但是，更新操作马上让缓存的失效了，后续的查询操作再把数据从数据库中拉出来。而不会像文章开头的那个逻辑产生的问题，后续的查询操作一直都在取老的数据。</p>
<h2 id="2-Read-through-3-Write-Through-Pattern"><a href="#2-Read-through-3-Write-Through-Pattern" class="headerlink" title="2. Read through / 3. Write Through Pattern"></a>2. Read through / 3. Write Through Pattern</h2><p>我们可以看到，在上面的 Cache Aside 套路中，我们的应用代码需要维护两个数据存储，一个是缓存（Cache），一个是数据库（Repository）。所以，应用程序比较啰嗦。而 Read/Write Through 套路是把更新数据库（Repository）的操作由缓存自己代理了，所以，对于应用层来说，就简单很多了。可以理解为，应用认为后端就是一个单一的存储，而存储自己维护自己的Cache。</p>
<h3 id="Read-Through"><a href="#Read-Through" class="headerlink" title="Read Through"></a>Read Through</h3><p>Read Through 套路就是在查询操作中更新缓存，也就是说，当缓存失效的时候（过期或LRU换出），Cache Aside 是由调用方负责把数据加载入缓存，而 Read Through 则用缓存服务自己来加载，从而对应用方是透明的。</p>
<h3 id="Write-Through"><a href="#Write-Through" class="headerlink" title="Write Through"></a>Write Through</h3><p>Write Through 套路和 Read Through 相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由 Cache 自己更新数据库（这是一个同步操作）</p>
<p>下图自来Wikipedia的Cache词条。其中的Memory你可以理解为就是我们例子里的数据库。</p>
<h2 id="4-Write-Behind-Caching-Pattern"><a href="#4-Write-Behind-Caching-Pattern" class="headerlink" title="4. Write Behind Caching Pattern"></a>4. Write Behind Caching Pattern</h2><p>Write Behind 又叫 Write Back。一些了解 Linux 操作系统内核的同学对 write back 应该非常熟悉，这不就是 Linux 文件系统的 Page Cache 的算法吗？是的，你看基础这玩意全都是相通的。所以，基础很重要，我已经不是一次说过基础很重要这事了。</p>
<p>Write Back 套路，一句说就是，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是让数据的 I/O 操作飞快无比（因为直接操作内存嘛）. 因为异步，write back 还可以合并对同一个数据的多次操作，所以性能的提高是相当可观的。</p>
<p>但是，其带来的问题是，数据不是强一致性的，而且可能会丢失（我们知道 Unix/Linux 非正常关机会导致数据丢失，就是因为这个事）。在软件设计上，我们基本上不可能做出一个没有缺陷的设计，就像算法设计中的时间换空间，空间换时间一个道理，有时候，强一致性和高性能，高可用和高性性是有冲突的。软件设计从来都是取舍 Trade-Off。</p>
<p>另外，Write Back 实现逻辑比较复杂, 因为他需要 track 有哪数据是被更新了的，需要刷到持久层上。操作系统的 write back 会在仅当这个cache需要失效的时候，才会被真正持久起来，比如，内存不够了，或是进程退出了等情况，这又叫 lazy write。</p>
<p>在 wikipedia 上有一张 write back 的流程图，基本逻辑如下：</p>
<p>Write-back_with_write-allocation</p>
<h2 id="再多唠叨一些"><a href="#再多唠叨一些" class="headerlink" title="再多唠叨一些"></a>再多唠叨一些</h2><ol>
<li><p>上面讲的这些 Design Pattern，其实并不是软件架构里的 mysql 数据库和 memcache/redis 的更新策略，这些东西都是计算机体系结构里的设计，比如CPU的缓存，硬盘文件系统中的缓存，硬盘上的缓存，数据库中的缓存。基本上来说，这些缓存更新的设计模式都是非常老古董的，而且历经长时间考验的策略，所以这也就是，工程学上所谓的Best Practice，遵从就好了。</p>
</li>
<li><p>有时候，我们觉得能做宏观的系统架构的人一定是很有经验的，其实，宏观系统架构中的很多设计都来源于这些微观的东西。比如，云计算中的很多虚拟化技术的原理，和传统的虚拟内存不是很像么？Unix 下的那些 I/O 模型，也放大到了架构里的同步异步的模型，还有 Unix 发明的管道不就是数据流式计算架构吗？TCP 的好些设计也用在不同系统间的通讯中，仔细看看这些微观层面，你会发现有很多设计都非常精妙……所以，请允许我在这里放句观点鲜明的话——如果你要做好架构，首先你得把计算机体系结构以及很多老古董的基础技术吃透了。</p>
</li>
<li><p>在软件开发或设计中，我非常建议在之前先去参考一下已有的设计和思路，看看相应的 guideline，best practice 或 design pattern，吃透了已有的这些东西，再决定是否要重新发明轮子。千万不要似是而非地，想当然的做软件设计。</p>
</li>
<li><p>上面，我们没有考虑缓存（Cache）和持久层（Repository）的整体事务的问题。比如，更新 Cache 成功，更新数据库失败了怎么吗？或是反过来。关于这个事，如果你需要强一致性，你需要使用“两阶段提交协议”——prepare, commit/rollback，比如 Java 7 的 XAResource，还有 MySQL 5.7 的 XA Transaction，有些cache也支持XA，比如EhCache。当然，XA这样的强一致性的玩法会导致性能下降，关于分布式的事务的相关话题，你可以看看《分布式系统的事务处理》一文。</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I read a blog a couple months ago talking about four most common cache design patterns, and want to summarize that post and add my understandings here.&lt;/p&gt;
&lt;h2 id=&quot;Naive-Approach&quot;&gt;&lt;a href=&quot;#Naive-Approach&quot; class=&quot;headerlink&quot; title=&quot;Naive Approach&quot;&gt;&lt;/a&gt;Naive Approach&lt;/h2&gt;&lt;p&gt;The naive approach is, when update data in cache service, first remove cached data, update data source, and then reload data from data source to cache.&lt;/p&gt;
&lt;p&gt;However, the logic is wrong!&lt;/p&gt;
&lt;p&gt;For example, there are two concurrent operation, one being update operation and the other being query operation. After update operation removed cached data, query operation doesn’t hit the target and triggers a reload of outdated data from data store, and then the update operation modifies data store. Thus, the cache now contains dirty/expired data, and will continue to hold such dirty data.&lt;/p&gt;
&lt;p&gt;I’m gonna discuss four caching strategies/design patterns today.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read Through&lt;/li&gt;
&lt;li&gt;Write Through&lt;/li&gt;
&lt;li&gt;Cache Aside&lt;/li&gt;
&lt;li&gt;Write Behind Caching&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that they’re design patterns, neither mechanisms of databases like MySQL or Postgres, nor how Memcache or Redis works.&lt;/p&gt;
&lt;h2 id=&quot;Read-Write-Through&quot;&gt;&lt;a href=&quot;#Read-Write-Through&quot; class=&quot;headerlink&quot; title=&quot;Read / Write Through&quot;&gt;&lt;/a&gt;Read / Write Through&lt;/h2&gt;&lt;p&gt;This is where the application treats cache as the main data store and reads data from it and writes data to it. The cache is responsible for reading and writing this data to the database, thereby relieving the application of this responsibility.&lt;/p&gt;
    
    </summary>
    
      <category term="system design" scheme="https://phoenixjiangnan.github.io/categories/system-design/"/>
    
      <category term="cache" scheme="https://phoenixjiangnan.github.io/categories/system-design/cache/"/>
    
    
      <category term="cache design patterns" scheme="https://phoenixjiangnan.github.io/tags/cache-design-patterns/"/>
    
      <category term="read through" scheme="https://phoenixjiangnan.github.io/tags/read-through/"/>
    
      <category term="write through" scheme="https://phoenixjiangnan.github.io/tags/write-through/"/>
    
      <category term="cache aside" scheme="https://phoenixjiangnan.github.io/tags/cache-aside/"/>
    
      <category term="write back" scheme="https://phoenixjiangnan.github.io/tags/write-back/"/>
    
      <category term="write behind" scheme="https://phoenixjiangnan.github.io/tags/write-behind/"/>
    
  </entry>
  
  <entry>
    <title>Bloom Filter - Introduction</title>
    <link href="https://phoenixjiangnan.github.io/2016/10/07/data%20structures%20and%20algorithms/bloom%20filter/Bloom-Filter-Introduction/"/>
    <id>https://phoenixjiangnan.github.io/2016/10/07/data structures and algorithms/bloom filter/Bloom-Filter-Introduction/</id>
    <published>2016-10-07T18:50:33.000Z</published>
    <updated>2016-10-07T03:56:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>Bloom Filter has become a hot topic in interviewing.</p>
<blockquote>
<p>A bloom filter is a space-efficient probabilistic data structure designed to tell you an element either <code>possibly in set</code> or <code>definitely not in set</code>. That’s saying, <code>false positive matches are possible</code>, but <code>false negatives are not</code>. Elements can be added to the set, but not removed (though this can be addressed with a <code>counting filter</code>). The more elements that are added to the set, the larger the probability of false positives.</p>
</blockquote>
<h2 id="How-to-store-a-value"><a href="#How-to-store-a-value" class="headerlink" title="How to store a value"></a>How to store a value</h2><blockquote>
<p>An empty <code>Bloom Filter</code> is <code>an bit array of m bits</code>, all set to 0. </p>
</blockquote>
<p>To store a value, there must also be either </p>
<ul>
<li><p><code>k different hash functions</code> defined, each of which maps or hashes some set element to one of the m array positions with a uniform random distribution.</p>
<p>  Typically, <code>k</code> is a constant, much smaller than <code>m</code> (<code>k</code> &lt;&lt; <code>m</code>), which is proportional to the number of elements to be added; the precise choice of <code>k</code> and the constant of proportionality of m are determined by the intended false positive rate of the filter. Feed the value to each of the <code>k</code> hash functions to get <code>k</code> array positions.</p>
</li>
<li><p>or one hash function which hashes bits of the value and returns <code>k</code> array positions.</p>
<ul>
<li>there’re several ways to achieve this, we’ll talk about this later</li>
</ul>
</li>
</ul>
<p>To add an element,  Set the bits at all these positions to <code>1</code>.</p>
<h2 id="How-to-query-a-value"><a href="#How-to-query-a-value" class="headerlink" title="How to query a value"></a>How to query a value</h2><p>To query for an element (test whether it is in the set), get the <code>k</code> array positions by either</p>
<ul>
<li>feeding it to each of the <code>k</code> hash functions </li>
<li>or that single hash function which returns <code>k</code> positions.</li>
</ul>
<p>If any of the bits at these positions is <code>0</code>, the element is definitely not in the set – if it were, then all the bits would have been set to <code>1</code> when it was inserted.</p>
<p>If all are <code>1</code>, then either the element is in the set, or the bits have by chance been set to <code>1</code> during the insertion of other elements, resulting in a false positive. In a simple Bloom Filter, there is no way to distinguish between the two cases, but more advanced techniques can address this problem.</p>
<a id="more"></a>
<h2 id="Space-and-Time-Advantage"><a href="#Space-and-Time-Advantage" class="headerlink" title="Space and Time Advantage"></a>Space and Time Advantage</h2><h3 id="Space"><a href="#Space" class="headerlink" title="Space"></a>Space</h3><p>While risking false positives, <code>Bloom Filter</code> has a strong space advantage over other data structures for representing sets, such as self-balancing binary search trees, tries, hash tables, or simple arrays or linked lists of the entries. Most of those require storing at least the data items themselves, which can require anywhere from a small number of bits, for small integers, to an arbitrary number of bits, such as for strings (tries are an exception, since they can share storage between elements with equal prefixes). </p>
<p>However, Bloom Filter does not store the data items at all, and a separate solution must be provided for the actual storage. Linked structures incur an additional linear space overhead for pointers. A Bloom Filter with <code>1%</code> error and an optimal value of <code>k</code>, in contrast, requires only about <code>9.6</code> bits per element, regardless of the size of the elements. This advantage comes partly from its compactness, inherited from arrays, and partly from its probabilistic nature. The <code>1%</code> false-positive rate can be reduced by a factor of ten by adding only about <code>4.8</code> bits per element.</p>
<p>However, if the number of potential values is small and many of them can be in the set, the Bloom Filter is easily surpassed by the deterministic bit array, which requires only one bit for each potential element. Note also that hash tables gain a space and time advantage if they begin ignoring collisions and store only whether each bucket contains an entry; in this case, they have effectively become Bloom Filters with <code>k = 1</code></p>
<h3 id="Time"><a href="#Time" class="headerlink" title="Time"></a>Time</h3><p>Bloom Filters also have the unusual property that the time needed either to add items or to check whether an item is in the set is a fixed constant, <code>O(k)</code>, completely independent of the number of items already in the set. </p>
<hr>
<p>No other constant-space set data structure has this property, but the average access time of sparse hash tables can make them faster in practice than some Bloom Filters. In a hardware implementation, however, the Bloom Filter shines because its <code>k</code> lookups are independent and can be parallelized.</p>
<p>To understand its space efficiency, it is instructive to compare the general Bloom Filter with its special case when <code>k = 1</code>. If <code>k = 1</code>, then in order to keep the false positive rate sufficiently low, a small fraction of bits should be set, which means the array must be very large and contain long runs of zeros. The information content of the array relative to its size is low. The generalized Bloom filter (when <code>k &gt; 1</code>) allows many more bits to be set while still maintaining a low false positive rate; if the parameters (<code>k</code> and <code>m</code>) are chosen well, about half of the bits will be set, and these will be apparently random, minimizing redundancy and maximizing information content.</p>
<h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><p>I’ll discuss the implementation details of these Bloom Filters later.</p>
<h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><p><a href="https://github.com/apache/hadoop/tree/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom" target="_blank" rel="external">Hadoop Github Mirror</a></p>
<ul>
<li><p><a href="https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/BloomFilter.java" target="_blank" rel="external">Bloom Filter</a></p>
</li>
<li><p><a href="https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java" target="_blank" rel="external">Counting Bloom Filter</a></p>
</li>
<li><p><a href="https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java" target="_blank" rel="external">Dynamic Bloom Filter</a></p>
</li>
<li><p><a href="https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java" target="_blank" rel="external">Retouched Bloom Filter</a></p>
</li>
<li><p><a href="https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/bloom/TestBloomFilters.java" target="_blank" rel="external">Test Bloom Filter</a></p>
</li>
<li><p><a href="https://github.com/apache/hadoop/blob/f67237cbe7bc48a1b9088e990800b37529f1db2a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/bloom/BloomFilterCommonTester.java" target="_blank" rel="external">BloomFilterCommonTester</a></p>
</li>
</ul>
<h3 id="Cassandra"><a href="#Cassandra" class="headerlink" title="Cassandra"></a>Cassandra</h3><ul>
<li><a href="https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/BloomFilter.java" target="_blank" rel="external">https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/BloomFilter.java</a></li>
</ul>
<hr>
<p>Reference:</p>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/Bloom_filter" target="_blank" rel="external">https://en.wikipedia.org/wiki/Bloom_filter</a></p>
</li>
<li><p><a href="http://stackoverflow.com/questions/6533582/advantage-of-using-0x01-instead-of-1-for-an-integer-variable" target="_blank" rel="external">http://stackoverflow.com/questions/6533582/advantage-of-using-0x01-instead-of-1-for-an-integer-variable</a></p>
</li>
<li><p><a href="http://billmill.org/bloomfilter-tutorial/" target="_blank" rel="external">http://billmill.org/bloomfilter-tutorial/</a></p>
</li>
<li><p><a href="http://www.javamex.com/tutorials/collections/bloom_filter_java.shtml" target="_blank" rel="external">http://www.javamex.com/tutorials/collections/bloom_filter_java.shtml</a></p>
</li>
<li><p><a href="http://www.sanfoundry.com/java-program-implement-bloom-filter/" target="_blank" rel="external">http://www.sanfoundry.com/java-program-implement-bloom-filter/</a></p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Bloom Filter has become a hot topic in interviewing.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A bloom filter is a space-efficient probabilistic data structure designed to tell you an element either &lt;code&gt;possibly in set&lt;/code&gt; or &lt;code&gt;definitely not in set&lt;/code&gt;. That’s saying, &lt;code&gt;false positive matches are possible&lt;/code&gt;, but &lt;code&gt;false negatives are not&lt;/code&gt;. Elements can be added to the set, but not removed (though this can be addressed with a &lt;code&gt;counting filter&lt;/code&gt;). The more elements that are added to the set, the larger the probability of false positives.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;How-to-store-a-value&quot;&gt;&lt;a href=&quot;#How-to-store-a-value&quot; class=&quot;headerlink&quot; title=&quot;How to store a value&quot;&gt;&lt;/a&gt;How to store a value&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;An empty &lt;code&gt;Bloom Filter&lt;/code&gt; is &lt;code&gt;an bit array of m bits&lt;/code&gt;, all set to 0. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To store a value, there must also be either &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;k different hash functions&lt;/code&gt; defined, each of which maps or hashes some set element to one of the m array positions with a uniform random distribution.&lt;/p&gt;
&lt;p&gt;  Typically, &lt;code&gt;k&lt;/code&gt; is a constant, much smaller than &lt;code&gt;m&lt;/code&gt; (&lt;code&gt;k&lt;/code&gt; &amp;lt;&amp;lt; &lt;code&gt;m&lt;/code&gt;), which is proportional to the number of elements to be added; the precise choice of &lt;code&gt;k&lt;/code&gt; and the constant of proportionality of m are determined by the intended false positive rate of the filter. Feed the value to each of the &lt;code&gt;k&lt;/code&gt; hash functions to get &lt;code&gt;k&lt;/code&gt; array positions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;or one hash function which hashes bits of the value and returns &lt;code&gt;k&lt;/code&gt; array positions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there’re several ways to achieve this, we’ll talk about this later&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To add an element,  Set the bits at all these positions to &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&quot;How-to-query-a-value&quot;&gt;&lt;a href=&quot;#How-to-query-a-value&quot; class=&quot;headerlink&quot; title=&quot;How to query a value&quot;&gt;&lt;/a&gt;How to query a value&lt;/h2&gt;&lt;p&gt;To query for an element (test whether it is in the set), get the &lt;code&gt;k&lt;/code&gt; array positions by either&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;feeding it to each of the &lt;code&gt;k&lt;/code&gt; hash functions &lt;/li&gt;
&lt;li&gt;or that single hash function which returns &lt;code&gt;k&lt;/code&gt; positions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If any of the bits at these positions is &lt;code&gt;0&lt;/code&gt;, the element is definitely not in the set – if it were, then all the bits would have been set to &lt;code&gt;1&lt;/code&gt; when it was inserted.&lt;/p&gt;
&lt;p&gt;If all are &lt;code&gt;1&lt;/code&gt;, then either the element is in the set, or the bits have by chance been set to &lt;code&gt;1&lt;/code&gt; during the insertion of other elements, resulting in a false positive. In a simple Bloom Filter, there is no way to distinguish between the two cases, but more advanced techniques can address this problem.&lt;/p&gt;
    
    </summary>
    
      <category term="data structures and algorithms" scheme="https://phoenixjiangnan.github.io/categories/data-structures-and-algorithms/"/>
    
      <category term="bloom filter" scheme="https://phoenixjiangnan.github.io/categories/data-structures-and-algorithms/bloom-filter/"/>
    
    
      <category term="bloom filter" scheme="https://phoenixjiangnan.github.io/tags/bloom-filter/"/>
    
  </entry>
  
  <entry>
    <title>Apache Kafka, Samza, and the Unix Philosopy of Distributed Data</title>
    <link href="https://phoenixjiangnan.github.io/2016/10/06/distributed%20system/kafka/Apache-Kafka-Samza-and-the-Unix-Philosopy-of-Distributed-Data/"/>
    <id>https://phoenixjiangnan.github.io/2016/10/06/distributed system/kafka/Apache-Kafka-Samza-and-the-Unix-Philosopy-of-Distributed-Data/</id>
    <published>2016-10-07T02:55:32.000Z</published>
    <updated>2016-10-06T12:09:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>Here’s another blog from Confluent’s website. What’s interesting about this article is that it compares the design philosophy between Unix and monolith database, and refers to Kafka as a distributed version of Unix pipeline. The latter point is exceptionally interesting and brings a whole new view of Kafka to me.</p>
<p>After working fulltime in the IT industry for almost two years, I’ve seen repeatedly that design of modern system borrows ideas from the design of Linux/Unix. Besides Kafka and Unix pipeline, another example is the <a href="http://docs.oracle.com/cd/E13924_01/coh.340/e13819/readthrough.htm" target="_blank" rel="external">Write Behind Caching Pattern</a> in modern system actually refers to <a href="https://www.thomas-krenn.com/en/wiki/Linux_Page_Cache_Basics" target="_blank" rel="external">Linux’s Page Cache</a></p>
<hr>
<p><a href="http://www.confluent.io/blog/apache-kafka-samza-and-the-unix-philosophy-of-distributed-data/" target="_blank" rel="external">Original Post</a> by Martin Kleppmann. August 1, 2015.</p>
<hr>
<p><img src="http://www.confluent.io/wp-content/uploads/2016/08/unixphil-01.png" alt="unixphil-01"></p>
<p>One of the things I realised while doing research for my book is that contemporary software engineering still has a lot to learn from the 1970s. As we’re in such a fast-moving field, we often have a tendency of dismissing older ideas as irrelevant – and consequently, we end up having to learn the same lessons over and over again, the hard way. Although computers have got faster, data has got bigger and requirements have become more complex, many old ideas are actually still highly relevant today. In this blog post I’d like to highlight one particular set of old ideas that I think deserves more attention today: <code>the Unix philosophy</code>. I’ll show how this philosophy is very different from the design approach of mainstream databases, and explore what it would look like if modern distributed data systems learnt a thing or two from Unix.</p>
<p>In particular, I’m going to argue that there are a lot of similarities between <code>Unix pipes</code> and <code>Apache Kafka</code>, and that this similarity enables good architectural styles for large-scale applications. But before we get into that, let me remind you of the foundations of the <code>Unix philosophy</code>. You’ve probably seen the power of Unix tools before – but to get started, let me give you a concrete example that we can talk about. Say you have a web server that writes an entry to a log file every time it serves a request. For example, using the nginx default access log format, one line of the log might look like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] &quot;GET /css/typography.css HTTP/1.1&quot; 200 3377 &quot;http://martin.kleppmann.com/&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36&quot;</div></pre></td></tr></table></figure>
<p>(That is actually one line, it’s only broken up into multiple lines here for readability.) This line of the log indicates that on 27 February 2015 at 17:55:11 UTC, the server received a request for the file /css/typography.css from the client IP address 216.58.210.78. It then goes on to note various other details, including the browser’s user-agent string. Various tools can take these log files and produce pretty reports about your website traffic, but for the sake of the exercise, let’s build our own, using basic Unix tools. Let’s determine the 5 most popular URLs on our website. To start with, we need to extract the path of the URL that was requested, for which we can use <code>awk</code>. <code>awk</code> doesn’t know about the format of nginx logs – it just treats the log file as text. By default, <code>awk</code> takes one line of input at a time, splits it by whitespace, and makes the whitespace-separated components available as variables <code>$1</code>, <code>$2</code>, etc. In the nginx log example, the requested URL path is the seventh whitespace-separated component: </p>
<a id="more"></a>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-03.png" alt=""></p>
<p>Now that you’ve extracted the path, you can determine the 5 most popular pages on your website as follows:</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">awk '&#123;<span class="built_in">print</span> $<span class="number">7</span>&#125;' access.log | # Split by whitespace, <span class="number">7</span>th field is request <span class="built_in">path</span></div><div class="line">    sort                    | # Make occurrences of the same URL appear consecutively <span class="keyword">in</span> file</div><div class="line">    uniq -c                 | # <span class="built_in">Replace</span> consecutive occurrences of the same URL with a count</div><div class="line">    sort -rn                | # Sort by number of occurrences, descending</div><div class="line">    head -n <span class="number">5</span>                 # Output top <span class="number">5</span> URLs</div></pre></td></tr></table></figure>
<p>The output of that series of commands looks something like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">4189 /favicon.ico</div><div class="line">3631 /2013/05/24/improving-security-of-ssh-private-keys.html</div><div class="line">2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html</div><div class="line">1369 /</div><div class="line">915 /css/typography.css</div></pre></td></tr></table></figure>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-04.png" alt=""></p>
<p>Although the above command line looks a bit obscure if you’re unfamiliar with Unix tools, it is incredibly powerful. It will process gigabytes of log files in a matter of seconds, and you can easily modify the analysis to suit your needs. For example, if you want to count top client IP addresses instead of top pages, change the awk argument to <code>{print $1}</code>. Many data analyses can be done in a few minutes using some combination of <code>awk</code>, <code>sed</code>, <code>grep</code>, <code>sort</code>, <code>uniq</code> and <code>xargs</code>, and they perform surprisingly well. This is no coincidence: it is a direct result of the design philosophy of Unix. </p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-05.png" alt=""></p>
<p>The Unix philosophy is a set of principles that emerged gradually during the design and implementation of Unix systems during the late 1960s and ‘70s. There are various interpretations of the Unix philosophy, but two points that particularly stand out were described by Doug McIlroy, Elliot Pinson and Berk Tague as follows in 1978:</p>
<ul>
<li>Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new <code>features.</code></li>
<li>Expect the output of every program to become the input to another, as yet unknown, program.</li>
</ul>
<p>These principles are the foundation for chaining together programs into pipelines that can accomplish complex processing tasks. The key idea here is that a program does not know or care where its input is coming from, or where its output is going to: it may be a file, or another program that’s part of the operating system, or another program written by someone else entirely.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-06.png" alt=""></p>
<p>The tools that come with the operating system are generic, but they are designed such that they can be <strong>composed</strong> together into larger programs that can perform application-specific tasks. The benefits that the designers of Unix derived from this design approach sound quite like the ideas of the Agile and DevOps movements that appeared decades later: scripting and automation, rapid prototyping, incremental iteration, being friendly to experimentation, and breaking down large projects into manageable chunks. Plus ça change.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-08.png" alt=""></p>
<p>When you join two commands with the pipe character in your shell, the shell starts both programs at the same time, and attaches the output of the first process to the second process’ input. This attachment mechanism uses the <code>pipe</code> syscall provided by the operating system. Note that this wiring is not done by the programs themselves, but by the shell – this allows them to be <a href="https://en.wikipedia.org/wiki/Loose_coupling" target="_blank" rel="external">loosely coupled</a>, and not worry about where their input is coming from, or where their output is going.</p>
<p>The pipe had been invented in 1964 by Doug McIlroy, who first described it like this in an internal Bell Labs memo: “We should have some ways of connecting programs like [a] garden hose – screw in another segment when it becomes necessary to massage data in another way.” Dennis Richie later wrote up his perspective on the memo.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-10.png" alt=""></p>
<p>They also realised early that the inter-process communication mechanism (pipes) can look very similar to the mechanism for reading and writing files. We now call this input redirection (using the contents of a file as input to a process) and output redirection (writing the output of a process to a file). The reason that Unix programs can be composed so flexibly is that they all conform to the same interface: most programs have one stream for input data (stdin) and two output streams (stdout for regular output data, and stderr for errors and diagnostic messages).</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-12.png" alt=""></p>
<p>Programs may also do other things besides reading <code>stdin</code> and writing <code>stdout</code>, such as reading and writing files, communicating over the network, or drawing a graphical user interface. However, the <code>stdin</code>/<code>stdout</code> communication is considered to be the main way how data flows from one Unix tool to another. And the great thing about the <code>stdin</code>/<code>stdout</code> interface is that anyone can implement it easily, in any programming language. You can develop your own tool that conforms to this interface, and it will play nicely with all the standard tools that ship as part of the operating system.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-13.png" alt=""></p>
<p>For example, when analysing a web server log file, perhaps you want to find out how many visitors you have from each country. The log doesn’t tell you the country, but it does tell you the IP address, which you can translate into a country using an IP geolocation database. Such a database isn’t included with your operating system by default, but you can write your own tool that takes IP addresses on <code>stdin</code> and outputs country codes on <code>stdout</code>. Once you’ve written that tool, you can include it in the data processing pipeline we discussed previously, and it will work just fine. This may seem painfully obvious if you’ve been working with Unix for a while, but I’d like to emphasise how remarkable this is: your own code runs on equal terms with the tools provided by the operating system. Apps with graphical user interfaces or web apps cannot simply be extended and wired together like this. You can’t just pipe Gmail into a separate search engine app, and post results to a wiki. Today it’s an exception, not the norm, to have programs that work together as smoothly as Unix tools do.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-14.png" alt=""></p>
<p>Change of scene. Around the same time as Unix was being developed, the <code>relational data model</code> was proposed, which in time became SQL, and was implemented in many popular databases. Many databases actually run on Unix systems. Does that mean they also follow the Unix philosophy?</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-15.png" alt=""></p>
<p>The dataflow in most database systems is very different from Unix tools. Rather than using <code>stdin</code> and <code>stdout</code> as communication channels, there is a <strong>database server</strong>, and several <strong>clients</strong>. The clients send queries to read or write data on the server, the server handles the queries and sends responses to the clients. This relationship is fundamentally asymmetric: <code>clients and servers are distinct roles</code>.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-16.png" alt=""></p>
<p>What about the <code>composability</code> and <code>extensibility</code> that we find in Unix systems? Clients can do anything they like (since they are application code), but database servers are mostly in the business of storing and retrieving your data. Letting you run arbitrary code is not their top priority. That said, many databases do provide some ways of extending the database server with your own code. For example, many relational databases let you write stored procedures in their own, rudimentary procedural language such as <code>PL/SQL</code> (and some let you run code in a general-purpose programming language such as JavaScript). However, the things you can do in stored procedures are limited. Other extension points in some databases are support for custom data types (this was one of the early design goals of Postgres), or pluggable storage engines. Essentially, these are plugin APIs: you can run your code in the database server, provided that your module adheres to a plugin API exposed by the database server for a particular purpose. This kind of extensibility is not the same as the arbitrary composability we saw with Unix tools. The plugin API is totally controlled by the database server, and subordinate to it. Your extension code is a guest in the database server’s home, not an equal partner.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-17.png" alt=""></p>
<p>A consequence of this design is that you can’t just pipe one database into another, even if they have the same data model. Nor can you insert your own code into the database’s internal processing pipelines (unless the server has specifically provided an extension point for you, such as triggers). I feel the design of databases is very self-centered. A database seems to assume that it’s the centre of your universe: the only place where you might want to store and query your data, the source of truth, and the destination for all queries. The closest you can get to piping data in and out of it is through bulk-loading and bulk-dumping (backup) operations, but those operations don’t really use any of the database’s features, such as query planning and indexes. If a database was designed according to the Unix philosophy, it would be based on a small number of core primitives that you could easily combine, extend and replace at will. Instead, databases are tremendously complicated, monolithic beasts. While Unix acknowledges that the operating system will never do everything you might want, and thus encourages you to extend it, databases try to implement all the features you may need in a single program.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-18.png" alt=""></p>
<p>Perhaps that design is fine in simple applications where a single database is indeed sufficient. However, many complex applications find that they have to use their data in various different ways: they need fast random access for OLTP, big sequential scans for analytics, inverted indexes for full-text search, graph indexes for connected data, machine learning systems for recommendation engines, a push mechanism for notifications, various different cached representations of the data for fast reads, and so on. A general-purpose database may try to do all of those things in one product (“one size fits all”), but in all likelihood it will not perform as well as a tool that is specialized for one particular purpose. In practice, you can often get the best results by combining various different data storage and indexing systems: for example, you may take the same data and store it in a relational database for random access, in Elasticsearch for full-text search, in a columnar format in Hadoop for analytics, and cached in a denormalized form in memcached. When you need to integrate different databases, the lack of Unix-style composability is a severe limitation. (I’ve done some work on piping data out of Postgres into other applications, but there’s still a long way to go before we can simply pipe any database into any other database.)</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-19.png" alt=""></p>
<p>We said that Unix tools are <code>composable</code> because they all implement the same interface of <code>stdin</code>, <code>stdout</code> and <code>stderr</code> – and each of these is a file descriptor, i.e. a stream of bytes that you can read or write like a file. This interface is simple enough that anyone can easily implement it, but it is also powerful enough that you can use it for anything. Because all Unix tools implement the same interface, we call it a uniform interface. That’s why you can pipe the output of <code>gunzip</code> to <code>wc</code> without a second thought, even though the authors of those two tools probably never spoke to each other. It’s like lego bricks, which all implement the same pattern of knobbly bits and grooves, allowing you to stack any lego brick on any other, regardless of their shape, size or colour.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-20.png" alt=""></p>
<p>The uniform interface of file descriptors in Unix doesn’t just apply to the input and output of processes, but it’s a very broadly applied pattern. If you open a file on the filesystem, you get a file descriptor. Pipes and unix sockets provide file descriptors that are a communication channel to another process on the same machine. On Linux, the virtual files in <code>/dev</code> are the interfaces of device drivers, so you might be talking to a USB port or even a GPU. The virtual files in <code>/proc</code> are an API for the kernel, but since they’re exposed as files, you can access them with the same tools as regular files. Even a TCP connection to a process on another machine is a file descriptor, although the BSD sockets API (which is most commonly used to establish TCP connections) is arguably not as Unixy as it could be. Plan 9 shows that even the network could have been cleanly integrated into the same uniform interface. To a first approximation, everything on Unix is a file. This uniformity means the logic of Unix tools is separated from the wiring, making it more composable. sed doesn’t need to care whether it’s talking to a pipe to another process, or a socket, or a device driver, or a real file on the filesystem. It’s all the same.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-21.png" alt=""></p>
<p>A file is <strong>a stream of bytes</strong>, perhaps with an end-of-file (EOF) marker at some point, indicating that the stream has ended (a stream can be of arbitrary length, and a process may not know in advance how long its input is going to be). A few tools (e.g. <code>gzip</code>) operate purely on byte streams, and don’t care about the structure of the data. But most tools need to parse their input in order to do anything useful with it. For this, most Unix tools use ASCII, with each record on one line, and fields separated by tabs or spaces, or maybe commas. Files are totally obvious to us today, which shows that a byte stream turned out to be a good uniform interface. However, the implementors of Unix could have decided to do it very differently. For example, it could have been a function callback interface, using a schema to pass records from process to process. Or it could have been shared memory (like System V IPC or mmap, which came along later). Or it could have been a bit stream rather than a byte stream. In a sense, a byte stream is a lowest common denominator – the simplest possible interface. Everything can be expressed in terms of a stream of bytes, and it’s fairly agnostic to the transport medium (pipe from another process, file on disk, TCP connection, tape, etc). But this is also a disadvantage, as we shall discuss later.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-22.png" alt=""></p>
<p>We’ve seen that Unix developed some very good design principles for software development, and that databases have taken a very different route. I would love to see a future in which we can learn from both paths of development, and combine the best ideas from each. How can we make 21st-century data systems better by learning from the Unix philosophy? In the rest of this post I’d like to explore what it might look like if we bring the Unix philosophy to the world of databases.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-23.png" alt=""></p>
<p>First, let’s acknowledge that Unix is not perfect. Although I think the simple, uniform interface of byte streams was very successful at enabling an ecosystem of flexible, composable, powerful tools, Unix has some limitations:</p>
<ul>
<li><p>It’s designed for use on a single machine. As our applications get every more data and traffic, and have higher uptime requirements, moving to distributed systems is becoming increasingly inevitable. Although a TCP connection can be made to look somewhat like a file, I don’t think that’s the right answer: it only works if both sides of the connection are up, and it has somewhat messy edge case semantics. TCP is good, but by itself it’s too low-level to serve as a distributed pipe implementation.</p>
</li>
<li><p>A Unix pipe is designed to have a single sender process, and a single recipient. You can’t use pipes to send output to several processes, or to collect input from several processes. (You can branch a pipeline with <code>tee</code>, but a pipe itself is always one-to-one.)</p>
</li>
<li><p>ASCII text (or rather, UTF-8) is great for making data easily explorable, but it quickly gets messy. Every process needs to be set up with its own input parsing: first breaking the byte stream into records (usually separated by newline, though some advocate <code>0x1e</code>, the ASCII record separator). Then a record needs to be broken up into fields, like the <code>$7</code> in the <code>awk</code> example at the beginning. Separator characters that appear in the data need to be escaped somehow. Even a fairly simple tool like xargs has about half a dozen command-line options to specify how its input should be parsed. Text-based interfaces work tolerably well, but in retrospect, I am pretty sure that a richer data model with explicit schemas would have worked better.</p>
</li>
<li><p>Unix processes are generally assumed to be fairly short-running. For example, if a process in the middle of a pipeline crashes, there is no way for it to resume processing from its input pipe – the entire pipeline fails and must be re-run from scratch. That’s no problem if the commands run only for a few seconds, but if an application is expected to run continuously for years, better fault tolerance is needed.</p>
</li>
</ul>
<p>I think we can find a solution that overcomes these downsides, while retaining the Unix philosophy’s benefits. </p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-24.png" alt=""></p>
<p>The cool thing is that this solution already exists, and is implemented in Kafka and Samza, two open source projects that work together to provide distributed stream processing. As you probably already know from other posts on this blog, Kafka is a scalable distributed message broker, and Samza is a framework that lets you write code to consume and produce data streams.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-25.png" alt="streams"></p>
<p>In fact, when you look at it through the Unix lens, Kafka looks quite like the pipe that connects the output of one process to the input of another. And Samza looks quite like a standard library that helps you read <code>stdin</code> and write <code>stdout</code> (and a few helpful additions, such as a deployment mechanism, state management, metrics, and monitoring). The style of stream processing jobs that you can write with Kafka and Samza closely follows the Unix tradition of small, composable tools:</p>
<ul>
<li>In Unix, the operating system kernel provides the pipe, a transport mechanism for getting a stream of bytes from one process to another.</li>
<li>In stream processing, Kafka provides publish-subscribe streams, a transport mechanism for getting messages from one stream processing job to another.</li>
</ul>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-26.png" alt=""></p>
<p>Kafka addresses the downsides of Unix pipes that we discussed previously:</p>
<ul>
<li><p>The single-machine limitation is lifted: Kafka itself is distributed by default, and any stream processors that use it can also be distributed across multiple machines.</p>
</li>
<li><p>A Unix pipe connects exactly one process output with exactly one process input, whereas a stream in Kafka can have many producers and many consumers. Many inputs is important for services that are distributed across multiple machines, and many outputs makes Kafka more like a broadcast channel. This is very useful, since it allows the same data stream to be consumed independently for several different purposes (including monitoring and audit purposes, which are often outside of the application itself). Kafka consumers can come and go without affecting other consumers.</p>
</li>
<li><p>Kafka also provides good fault tolerance: data is replicated across multiple Kafka nodes, so if one node fails, another node can automatically take over. If a stream processor node fails and is restarted, it can resume processing at its last checkpoint.</p>
</li>
<li><p>Rather than a stream of bytes, Kafka provides a stream of messages, which saves the first step of input parsing (breaking the stream of bytes into a sequence of records). Each message is just an array of bytes, so you can use your favourite serialisation format for individual messages: JSON, XML, Avro, Thrift or Protocol Buffers are all reasonable choices. It’s well worth standardising on one encoding, and Confluent provides particularly good schema management support for Avro. This allows applications to work with objects that have meaningful field names, and not have to worry about input parsing or output escaping. It also provides good support for schema evolution without breaking compatibility.</p>
</li>
</ul>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-27.png" alt=""></p>
<p>There are a few more things that Kafka does differently from Unix pipes, which are worth calling out briefly:</p>
<ul>
<li><p>As mentioned, Unix pipes provide a byte stream, whereas Kafka provides a stream of messages. This is especially important if several processes are concurrently writing to the same stream: in a byte stream, the bytes from different writers can be interleaved, leading to an unparseable mess. Since messages are coarser-grained and self-contained, they can be safely interleaved, making it safe for multiple processes to concurrently write to the same stream.</p>
</li>
<li><p>Unix pipes are just a small in-memory buffer, whereas Kafka durably writes all messages to disk. In this regard, Kafka is less like a pipe, and more like one process writing to a temporary file, while several other processes continuously read that file using tail -f (each consumer tails the file independently). Kafka’s approach provides better fault tolerance, since it allows a consumer to fail and restart without skipping messages. Kafka automatically splits those ‘temporary’ files into segments and garbage-collects old segments on a configurable schedule.</p>
</li>
<li><p>In Unix, if the consuming process of a pipe is slow to read the data, the buffer fills up and the sending process is blocked from writing to the pipe. This is a kind of backpressure. In Kafka, the producer and consumer are more decoupled: a slow consumer has its input buffered, so it doesn’t slow down the producer or other consumers. As long as the buffer fits within Kafka’s available disk space, the slow consumer can catch up later. This makes the system less sensitive to individual slow components, and more robust overall.</p>
</li>
<li><p>A data stream in Kafka is called a <code>topic</code>, and you can refer to it by name (which makes it more like a Unix named pipe. A pipeline of Unix programs is usually started all at once, so the pipes normally don’t need explicit names. On the other hand, a long-running application usually has bits added, removed or replaced gradually over time, so you need names in order to tell the system what you want to connect to. Naming also helps with discovery and management.</p>
</li>
</ul>
<p>Despite those differences, I still think it makes sense to think of Kafka as Unix pipes for distributed data. For example, one thing they have in common is that Kafka keeps messages in a fixed order (like Unix pipes, which keep the byte stream in a fixed order). This is a very useful property for event log data: the order in which things happened is often meaningful and needs to be preserved. Other types of message broker, like AMQP and JMS, do not have this ordering property.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-28.png" alt=""></p>
<p>So we’ve got Unix tools and stream processors that look quite similar. Both read some input stream, modify or transform it in some way, and produce an output stream that is somehow derived from the input. Importantly, the processing does not modify the input itself: it remains immutable. If you run <code>awk</code> on some input file, the file remains unmodified (unless you explicitly choose to overwrite it). Also, most Unix tools are deterministic, i.e. if you give them the same input, they always produce the same output. This means you can re-run the same command as many times as you want, and gradually iterate your way towards a working program. It’s great for experimentation, because you can always go back to your original data if you mess up the processing. This deterministic and side-effect-free processing looks a lot like functional programming. That doesn’t mean you have to use a functional programming language like Haskell (although you’re welcome to do so if you want), but you still get many of the benefits of functional code.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-29.png" alt=""></p>
<p>The Unix-like design principles of Kafka enable building composable systems at a large scale. In a large organisation, different teams can each publish their data to Kafka. Each team can independently develop and maintain stream processing jobs that consume streams and produce new streams. Since a stream can have any number of independent consumers, no coordination is required to set up a new consumer. We’ve been calling this idea a stream data platform. In this kind of architecture, the data streams in Kafka act as the communication channel between different teams’ systems. Each team focusses on making their particular part of the system do one thing well. While Unix tools can be composed to accomplish a data processing task, distributed streaming systems can be composed to comprise the entire operation of a large organisation. A Unixy approach manages the complexity of a large system by encouraging loose coupling: thanks to the uniform interface of streams, different components can be developed and deployed independently. Thanks to the fault tolerance and buffering of the pipe (Kafka), when a problem occurs in one part of the system, it remains localised. And schema management allows changes to data structures to be made safely, so that each team can move fast without breaking things for other teams.</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/blog-files/unixphil-30.png" alt=""></p>
<p>To wrap up this post, let’s consider a real-life example of how this works at LinkedIn. As you may know, companies can post their job openings on LinkedIn, and jobseekers can browse and apply for those jobs. What happens if a LinkedIn member (user) views one of those job postings? It’s very useful to know who has looked at which jobs, so the service that handles job views publishes an event to Kafka, saying something like “member 123 viewed job 456 at time 789”. Now that this information is in Kafka, it can be used for many good purposes:</p>
<ul>
<li><p>Monitoring systems: Companies pay LinkedIn to post their job openings, so it’s important that the site is working correctly. If the rate of job views drops unexpectedly, alarms should go off, because it indicates a problem that needs to be investigated.</p>
</li>
<li><p>Relevance and recommendations: It’s annoying for users to see the same thing over and over again, so it’s good to track how many times the users has seen a job posting, and feed that into the scoring process. Keeping track of who viewed what also allows for collaborative filtering recommendations (people who viewed X also viewed Y).</p>
</li>
<li><p>Preventing abuse: LinkedIn doesn’t want people to be able to scrape all the jobs, submit spam, or otherwise violate the terms of service. Knowing who is doing what is the first step towards detecting and blocking abuse.<br>Job poster analytics: The companies who post their job openings want to see stats (in the style of Google Analytics) about who is viewing their postings, for example so that they can test which wording attracts the best candidates.</p>
</li>
<li><p>Import into Hadoop and Data Warehouse: For LinkedIn’s internal business analytics, for senior management’s dashboards, for crunching numbers that are reported to Wall Street, for evaluating A/B tests, and so on.</p>
</li>
</ul>
<p>All of those systems are complex in their own right, and are maintained by different teams. Kafka provides a fault-tolerant, scalable implementation of a pipe. A stream data platform based on Kafka allows all of these various systems to be developed independently, and to be connected and composed in a robust way. If you enjoyed this post, you’ll love my book <a href="http://dataintensive.net/" target="_blank" rel="external">Designing Data-Intensive Applications</a>, published by O’Reilly. Thank you to Jay Kreps, Gwen Shapira, Michael Noll, Ewen Cheslack-Postava, Jason Gustafson, and Jeff Hartley for feedback on a draft of this post, and thanks also to Jay for providing the LinkedIn job view example.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Here’s another blog from Confluent’s website. What’s interesting about this article is that it compares the design philosophy between Unix and monolith database, and refers to Kafka as a distributed version of Unix pipeline. The latter point is exceptionally interesting and brings a whole new view of Kafka to me.&lt;/p&gt;
&lt;p&gt;After working fulltime in the IT industry for almost two years, I’ve seen repeatedly that design of modern system borrows ideas from the design of Linux/Unix. Besides Kafka and Unix pipeline, another example is the &lt;a href=&quot;http://docs.oracle.com/cd/E13924_01/coh.340/e13819/readthrough.htm&quot;&gt;Write Behind Caching Pattern&lt;/a&gt; in modern system actually refers to &lt;a href=&quot;https://www.thomas-krenn.com/en/wiki/Linux_Page_Cache_Basics&quot;&gt;Linux’s Page Cache&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;http://www.confluent.io/blog/apache-kafka-samza-and-the-unix-philosophy-of-distributed-data/&quot;&gt;Original Post&lt;/a&gt; by Martin Kleppmann. August 1, 2015.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&quot;http://www.confluent.io/wp-content/uploads/2016/08/unixphil-01.png&quot; alt=&quot;unixphil-01&quot;&gt;&lt;/p&gt;
&lt;p&gt;One of the things I realised while doing research for my book is that contemporary software engineering still has a lot to learn from the 1970s. As we’re in such a fast-moving field, we often have a tendency of dismissing older ideas as irrelevant – and consequently, we end up having to learn the same lessons over and over again, the hard way. Although computers have got faster, data has got bigger and requirements have become more complex, many old ideas are actually still highly relevant today. In this blog post I’d like to highlight one particular set of old ideas that I think deserves more attention today: &lt;code&gt;the Unix philosophy&lt;/code&gt;. I’ll show how this philosophy is very different from the design approach of mainstream databases, and explore what it would look like if modern distributed data systems learnt a thing or two from Unix.&lt;/p&gt;
&lt;p&gt;In particular, I’m going to argue that there are a lot of similarities between &lt;code&gt;Unix pipes&lt;/code&gt; and &lt;code&gt;Apache Kafka&lt;/code&gt;, and that this similarity enables good architectural styles for large-scale applications. But before we get into that, let me remind you of the foundations of the &lt;code&gt;Unix philosophy&lt;/code&gt;. You’ve probably seen the power of Unix tools before – but to get started, let me give you a concrete example that we can talk about. Say you have a web server that writes an entry to a log file every time it serves a request. For example, using the nginx default access log format, one line of the log might look like this:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] &amp;quot;GET /css/typography.css HTTP/1.1&amp;quot; 200 3377 &amp;quot;http://martin.kleppmann.com/&amp;quot; &amp;quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36&amp;quot;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;(That is actually one line, it’s only broken up into multiple lines here for readability.) This line of the log indicates that on 27 February 2015 at 17:55:11 UTC, the server received a request for the file /css/typography.css from the client IP address 216.58.210.78. It then goes on to note various other details, including the browser’s user-agent string. Various tools can take these log files and produce pretty reports about your website traffic, but for the sake of the exercise, let’s build our own, using basic Unix tools. Let’s determine the 5 most popular URLs on our website. To start with, we need to extract the path of the URL that was requested, for which we can use &lt;code&gt;awk&lt;/code&gt;. &lt;code&gt;awk&lt;/code&gt; doesn’t know about the format of nginx logs – it just treats the log file as text. By default, &lt;code&gt;awk&lt;/code&gt; takes one line of input at a time, splits it by whitespace, and makes the whitespace-separated components available as variables &lt;code&gt;$1&lt;/code&gt;, &lt;code&gt;$2&lt;/code&gt;, etc. In the nginx log example, the requested URL path is the seventh whitespace-separated component: &lt;/p&gt;
    
    </summary>
    
      <category term="distributed system" scheme="https://phoenixjiangnan.github.io/categories/distributed-system/"/>
    
      <category term="kafka" scheme="https://phoenixjiangnan.github.io/categories/distributed-system/kafka/"/>
    
    
      <category term="kafka" scheme="https://phoenixjiangnan.github.io/tags/kafka/"/>
    
      <category term="unix pipeline" scheme="https://phoenixjiangnan.github.io/tags/unix-pipeline/"/>
    
  </entry>
  
  <entry>
    <title>Throttling and Traffic Shaping</title>
    <link href="https://phoenixjiangnan.github.io/2016/10/05/system%20design/Throttling-and-Traffic-Shaping/"/>
    <id>https://phoenixjiangnan.github.io/2016/10/05/system design/Throttling-and-Traffic-Shaping/</id>
    <published>2016-10-06T06:49:42.000Z</published>
    <updated>2016-10-06T15:45:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Throttling-and-Traffic-Shaping"><a href="#Throttling-and-Traffic-Shaping" class="headerlink" title="Throttling and Traffic Shaping"></a>Throttling and Traffic Shaping</h2><blockquote>
<p>Throttling is, to control the consumption of resources used by an instance of an application, an individual tenant, or an entire service. </p>
</blockquote>
<p>Throttling can allow the system to continue to function and meet service level agreements, even when an increase in demand places an extreme load on resources.</p>
<p>The system could implement several throttling strategies, including:</p>
<ul>
<li><p><code>Service Metering</code> - that is, rejecting requests from an individual user who has already accessed system APIs more than n times per second over a given period of time. This requires that the system meters the use of resources for each tenant or user running an application.</p>
</li>
<li><p><code>Leaky bucket as a queue</code> - that is, using load leveling to smooth the volume of activity (also called <code>Queue-based Load Leveling pattern</code>). </p>
</li>
<li><p><code>Leaky bucket as a priority queue</code> - in a multitenant environment, <code>Leaky bucket as a queue</code> will reduce the performance for every tenant. </p>
<ul>
<li><p>If the system must support a mix of tenants with different SLAs, the work for high-value tenants might be performed immediately. </p>
</li>
<li><p>Requests for other tenants can be held back, and handled when the backlog has eased. Deferring operations being performed on behalf of lower priority applications or tenants. These operations can be suspended or curtailed, with an exception generated to inform the tenant that the system is busy and that the operation should be retried later.</p>
</li>
</ul>
</li>
<li><p><code>Token bucket</code> - that is, using tokens to limit the volume of activity</p>
</li>
<li><p><code>Disable non-critical services</code> Disabling or degrading the functionality of selected nonessential services so that essential services can run unimpeded with sufficient resources. For example, if the application is streaming video output, it could switch to a lower resolution.</p>
</li>
</ul>
<p>There are two most commonly used algorithms:</p>
<ol>
<li>Leaky Bucket for rate controll</li>
<li>Token Bucket for concurrency control</li>
</ol>
<a id="more"></a>
<h2 id="Leaky-Bucket-Algorithm"><a href="#Leaky-Bucket-Algorithm" class="headerlink" title="Leaky Bucket Algorithm"></a>Leaky Bucket Algorithm</h2><blockquote>
<p>Leaky Bucket Algorithm as a Queue, a.k.a Queue-Based Load Leveling Pattern</p>
</blockquote>
<h3 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h3><ul>
<li>What’s the max latency time a request can experience:<ul>
<li>V/(T - O) : V - the bucket’s volume, T - the request input rate, O - the output rate to process requests </li>
</ul>
</li>
</ul>
<h3 id="Pros"><a href="#Pros" class="headerlink" title="Pros:"></a>Pros:</h3><ul>
<li>Easy to implement</li>
<li>Best used to control processing rate</li>
</ul>
<h3 id="Cons"><a href="#Cons" class="headerlink" title="Cons:"></a>Cons:</h3><ul>
<li>Leaky bucket algorithms cannot effectively take advantages of resources. The leaking rate is fixed, so it cannot process a bulky traffic that exceeds its threshold even though there are plenty of resources. (Token Bucket Algorithm can do that)</li>
</ul>
<h3 id="Use-Case"><a href="#Use-Case" class="headerlink" title="Use Case:"></a>Use Case:</h3><ul>
<li>To address system callback flood</li>
</ul>
<h2 id="Token-Bucket-Algorithm"><a href="#Token-Bucket-Algorithm" class="headerlink" title="Token Bucket Algorithm"></a>Token Bucket Algorithm</h2><blockquote>
<p>What a token bucket limits is the traffic within a predefined time window. From the API level, the traffic that we always talk about is <code>QPS (Queries per sec)</code> and <code>TPS (Transactions per sec)</code>, and they are just the traffic in a 1-sec time window.</p>
</blockquote>
<p>The token bucket algorithm can be conceptually understood as follows:</p>
<ul>
<li>A token is added to the bucket every <code>1/r</code>  seconds.</li>
<li>The current number of tokens in the bucket is <code>c</code>. <ul>
<li>Strategy 1: The bucket can hold at the most <code>b</code> tokens. <code>c</code> &lt;= <code>b</code>. If a token arrives when the bucket is full, it is discarded.</li>
<li>Strategy 2: The bucket can hold unlimited tokens</li>
</ul>
</li>
<li>When n bytes/requests arrive, n tokens will be removed from the bucket, and the bytes/requests are sent to the network.</li>
</ul>
<p>There are four strategies for situations when <code>n</code> &gt; <code>c</code>.</p>
<ol>
<li>(simple and rudimentary) Consider the packet to be non-conformant, and discard the packet for now (availiable for request resubmit)</li>
<li>(naive and impractical) Have the packet wait until enough tokens accumulated<ul>
<li>simple waiting may block the workflow</li>
<li>maybe lower the priority of the packet, but it still possibily will not be processed forever</li>
</ul>
</li>
<li>(good for batch processing) Break the packet into smaller ones</li>
<li>(google guava supports) Grant the packet <code>n</code> tokens for now, but will have to delay its further requests in order to make it up. This requires a service metering to keep track of hosts’ token usage. Google Guava <code>RateLimiter</code> supports this feature.</li>
</ol>
<h3 id="Statistics-1"><a href="#Statistics-1" class="headerlink" title="Statistics"></a>Statistics</h3><ul>
<li>What’s the average allowed hit rate<ul>
<li>the rate to issue tokens <code>r</code></li>
</ul>
</li>
<li>How much is the max tolerable flood peak<ul>
<li>if the flood peak comes when the token buckets are full, the volume of max tolerable flood peak = V + r. V - token bucket size, r - the rate to issue tokens</li>
</ul>
</li>
</ul>
<h3 id="Pros-1"><a href="#Pros-1" class="headerlink" title="Pros:"></a>Pros:</h3><ul>
<li>Best used to control max concurrency</li>
</ul>
<h3 id="Cons-1"><a href="#Cons-1" class="headerlink" title="Cons:"></a>Cons:</h3><ul>
<li>It really depends on which above strategy you choose</li>
</ul>
<h3 id="Use-Case-1"><a href="#Use-Case-1" class="headerlink" title="Use Case:"></a>Use Case:</h3><ul>
<li>To address user requests flood</li>
</ul>
<h2 id="Difference-between-Leaky-Bucket-and-Token-Bucket"><a href="#Difference-between-Leaky-Bucket-and-Token-Bucket" class="headerlink" title="Difference between Leaky Bucket and Token Bucket"></a>Difference between Leaky Bucket and Token Bucket</h2><blockquote>
<p>Leaky bucket strictly forces a fixed maximum rate of processing. In some circumstances, leaky bucket cannot effectively use the internet resources, because it only grants fixed rate of processing and thus cannot handle traffic floods.</p>
<p>While token bucket not only limit the average rate of processing, it also allows systems to handle sudden flood peaks.</p>
</blockquote>
<p>Pratically, leaky bucket and token bucket algorithms are put together to provide a more powerful yet more flexible control over web traffic.</p>
<hr>
<p>References:</p>
<ul>
<li><p><a href="http://jm.taobao.org/2016/05/19/how-to-withstand-the-world-s-largest-traffic/" target="_blank" rel="external">http://jm.taobao.org/2016/05/19/how-to-withstand-the-world-s-largest-traffic/</a></p>
</li>
<li><p><a href="https://msdn.microsoft.com/en-us/library/dn589798.aspx" target="_blank" rel="external">https://msdn.microsoft.com/en-us/library/dn589798.aspx</a></p>
</li>
<li><p><a href="http://www.cnblogs.com/LBSer/p/4083131.html" target="_blank" rel="external">http://www.cnblogs.com/LBSer/p/4083131.html</a></p>
</li>
</ul>
<ul>
<li><a href="https://blog.jamespan.me/2015/10/19/traffic-shaping-with-token-bucket/" target="_blank" rel="external">https://blog.jamespan.me/2015/10/19/traffic-shaping-with-token-bucket/</a></li>
</ul>
<ul>
<li><a href="https://msdn.microsoft.com/en-us/library/dn589783.aspx" target="_blank" rel="external">https://msdn.microsoft.com/en-us/library/dn589783.aspx</a></li>
</ul>
<ul>
<li><a href="https://github.com/google/guava/blob/v18.0/guava/src/com/google/common/util/concurrent/RateLimiter.java" target="_blank" rel="external">https://github.com/google/guava/blob/v18.0/guava/src/com/google/common/util/concurrent/RateLimiter.java</a></li>
<li><a href="https://github.com/google/guava/blob/v18.0/guava/src/com/google/common/util/concurrent/SmoothRateLimiter.java" target="_blank" rel="external">https://github.com/google/guava/blob/v18.0/guava/src/com/google/common/util/concurrent/SmoothRateLimiter.java</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Throttling-and-Traffic-Shaping&quot;&gt;&lt;a href=&quot;#Throttling-and-Traffic-Shaping&quot; class=&quot;headerlink&quot; title=&quot;Throttling and Traffic Shaping&quot;&gt;&lt;/a&gt;Throttling and Traffic Shaping&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Throttling is, to control the consumption of resources used by an instance of an application, an individual tenant, or an entire service. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Throttling can allow the system to continue to function and meet service level agreements, even when an increase in demand places an extreme load on resources.&lt;/p&gt;
&lt;p&gt;The system could implement several throttling strategies, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Service Metering&lt;/code&gt; - that is, rejecting requests from an individual user who has already accessed system APIs more than n times per second over a given period of time. This requires that the system meters the use of resources for each tenant or user running an application.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Leaky bucket as a queue&lt;/code&gt; - that is, using load leveling to smooth the volume of activity (also called &lt;code&gt;Queue-based Load Leveling pattern&lt;/code&gt;). &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Leaky bucket as a priority queue&lt;/code&gt; - in a multitenant environment, &lt;code&gt;Leaky bucket as a queue&lt;/code&gt; will reduce the performance for every tenant. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the system must support a mix of tenants with different SLAs, the work for high-value tenants might be performed immediately. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Requests for other tenants can be held back, and handled when the backlog has eased. Deferring operations being performed on behalf of lower priority applications or tenants. These operations can be suspended or curtailed, with an exception generated to inform the tenant that the system is busy and that the operation should be retried later.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Token bucket&lt;/code&gt; - that is, using tokens to limit the volume of activity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;Disable non-critical services&lt;/code&gt; Disabling or degrading the functionality of selected nonessential services so that essential services can run unimpeded with sufficient resources. For example, if the application is streaming video output, it could switch to a lower resolution.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are two most commonly used algorithms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Leaky Bucket for rate controll&lt;/li&gt;
&lt;li&gt;Token Bucket for concurrency control&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="system design" scheme="https://phoenixjiangnan.github.io/categories/system-design/"/>
    
    
      <category term="throttling" scheme="https://phoenixjiangnan.github.io/tags/throttling/"/>
    
      <category term="traffic shaping" scheme="https://phoenixjiangnan.github.io/tags/traffic-shaping/"/>
    
      <category term="leaky bucket" scheme="https://phoenixjiangnan.github.io/tags/leaky-bucket/"/>
    
      <category term="token bucket" scheme="https://phoenixjiangnan.github.io/tags/token-bucket/"/>
    
  </entry>
  
  <entry>
    <title>Putting Apache Kafka To Use: A Practical Guide to Building a Stream Data Platform (Part 2)</title>
    <link href="https://phoenixjiangnan.github.io/2016/10/03/distributed%20system/kafka/Putting-Apache-Kafka-To-Use-A-Practical-Guide-to-Building-a-Stream-Data-Platform-Part-2/"/>
    <id>https://phoenixjiangnan.github.io/2016/10/03/distributed system/kafka/Putting-Apache-Kafka-To-Use-A-Practical-Guide-to-Building-a-Stream-Data-Platform-Part-2/</id>
    <published>2016-10-04T06:21:05.000Z</published>
    <updated>2016-10-03T15:24:31.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>I’m preparing to start a new journey, which I’ll annouce soon. The new opportunity is about data.</p>
<p><code>Apache Kafka</code> has been a hot topic in the data field for a while, and, of course, I cannot taking on data problems without it. While learning and reading more about Kafka, I found <a href="http://www.confluent.io/blog/" target="_blank" rel="external">Conluent’s official tech blog</a> has been an amazingly useful place to find out materials I need - because the company is started by founders of Kafka in Linkedin.</p>
<p>Thus, I decided to repost a few great blogs from Confluent, add my own notes, and publish them here.</p>
<p>All posts are <a href="https://phoenixjiangnan.github.io/categories/distributed-system/kafka/">here</a></p>
</blockquote>
<hr>
<blockquote>
<p>This post is Part II of a couple blogs from Concluent’s CEO Jay Kreps. Part II has been really amazing to present you an overall view of all major technical details of Kafka.</p>
<p><em><a href="http://www.confluent.io/blog/stream-data-platform-2/" target="_blank" rel="external">Original Post</a> from Jay Kreps. February 25, 2015.</em></p>
</blockquote>
<hr>
<p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870613-png/blog-files/data-systems-sdp.png" alt=""></p>
<p>This is the second part of our guide on streaming data and Apache Kafka. In part one I talked about the uses for real-time data streams and explained our idea of a stream data platform. The remainder of this guide will contain specific advice on how to go about building a stream data platform in your organization.</p>
<p>This advice is drawn from our experience building and implementing Kafka at LinkedIn and rolling it out across all the data types and systems there. It also comes from four years working with tech companies in Silicon Valley to build Kafka-based stream data platforms in their organizations.</p>
<p>This is meant to be a living document. As we learn new techniques, or new tools become available, I’ll update it.</p>
<h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>Much of the advice in this guide covers techniques that will scale to hundreds or thousands of well formed data streams. No one starts with that, of course. Usually you start with one or two trial applications, often ones that have scalability requirements that make other systems less suitable. Even in this kind of limited deployment, though, the techniques described in this guide will help you to start off with good practices, which is critical as your usage expands.</p>
<p>Starting with something more limited is good, it let’s you get a hands on feel for what works and what doesn’t, so that, when broader adoption comes, you are well prepared for it.</p>
<h2 id="Recommendations"><a href="#Recommendations" class="headerlink" title="Recommendations"></a>Recommendations</h2><p>I’ll give a set of general recommendations for streaming data and Kafka and then discuss some specifics of different types of data.</p>
<h3 id="1-Limit-The-Number-of-Clusters"><a href="#1-Limit-The-Number-of-Clusters" class="headerlink" title="1. Limit The Number of Clusters"></a>1. Limit The Number of Clusters</h3><p>In early experimentation phases it is normal to end up with a few different Kafka clusters as adoption occurs organically in different parts of the organization.</p>
<a id="more"></a>
<blockquote>
<p>However part of the promise of this approach to data management is having <code>a central repository</code> with the full set of data streams your organization generates. This works best when data is all in the same place.</p>
</blockquote>
<p>This is similar to the recommendations given in data warehousing where the goal is to concentrate data in a central warehouse for simplicity and to enable uses that join together multiple data sources.</p>
<p>Likewise we have seen that storing stream data in the fewest number of Kafka clusters feasible has a great deal of value in simplifying system architecture. This means fewer integration points for data consumers, fewer things to operate, lower incremental cost for adding new applications, and makes it easier to reason about data flow.</p>
<p><code>The fewest number of clusters may not be one cluster.</code> There are several reasons to end up with multiple clusters:</p>
<blockquote>
<ul>
<li>To keep activity local to a datacenter. As described later we recommend that all applications connect to a cluster in their local datacenter with mirroring between data centers done between these local data centers.</li>
<li>For security reasons. Kafka does not yet have security controls which often means implementing network level security and physically segregating data types.</li>
<li>For SLA control. Kafka has some multi-tenancy features but this story is not complete.</li>
</ul>
</blockquote>
<p>Our job as Kafka engineers is to remove the restrictions that force new cluster creation, but until we’ve done that beware of the above limitations.</p>
<h3 id="2-Pick-A-Single-Data-Format"><a href="#2-Pick-A-Single-Data-Format" class="headerlink" title="2. Pick A Single Data Format"></a>2. Pick A Single Data Format</h3><p>Apache Kafka does not enforce any particular format for event data beyond a simple key/value model. It will work equally well with <code>XML</code>, <code>JSON</code>, or <code>Avro</code>. Our general philosophy is that it is not the role of data infrastructure systems to enforce this kind of policy, that is really an organizational choice.</p>
<blockquote>
<p>However, though your infrastructure shouldn’t make this choice for you, you should make a choice! Mandating a single, company-wide data format for events is critical. The overall simplicity of integration comes not only from having stream data in a single system—Kafka—but also by making all data look similar and follow similar conventions. If each individual or application chooses a representation of their own preference—say some use JSON, others XML, and others CSV—the result is that any system or process which uses multiple data streams has to munge and understand each of these. Local optimization—choosing your favorite format for data you produce—leads to huge global sub-optimization since now each system needs to write N adaptors, one for each format it wants to ingest.</p>
</blockquote>
<p>An analogy borrowed from a friend can help to explain why such a mundane thing as data format is worth fussing about. One of the few great successes in the integration of applications is the Unix command line tools. The Unix toolset all works together reasonably well despite the fact that the individual commands were written by different people over a long period of time. The standard for integrating these tools is newline delimited ASCII text, these can be strung together with a <code>|</code> which transmits a record stream using standard input and standard output. The stream data platform is actually not that far removed from this itself. It is a kind of modern Unix pipe implemented at the data center level and designed to support our new world of distributed, continually running programs.</p>
<p>Though surely newline delimited text is an inadequate format to standardize on these days, imagine how useless the Unix toolchain would be if each tool invented its own format: you would have to translate between formats every time you wanted to pipe one command to another.</p>
<p>Picking a single format, making sure that all tools and integrations use it, and holding firm on the use of this format across the board, is likely the single most important thing to do in the early implementation of your stream data platform. This stuff is fairly new, so if you are adopting it now sticking to the simplicity of a uniform data format should be easy.</p>
<h3 id="3-The-Mathematics-of-Simplicity"><a href="#3-The-Mathematics-of-Simplicity" class="headerlink" title="3. The Mathematics of Simplicity"></a>3. The Mathematics of Simplicity</h3><p>Together these two recommendations—<code>limiting the number of clusters</code> and <code>standardizing on a single data format</code>—bring a very real kind of simplicity to data flow in an organization.</p>
<p>By centralizing on a single infrastructure platform for data exchange which provides a single abstraction—the real-time stream—we dramatically simplify the data flow picture. Connecting all systems directly would look something like this:</p>
<p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870603-png/blog-files/data-systems-point-to-point.png" alt="data-systems-point-to-point"></p>
<p>Whereas having this central stream data platform looks something like this:</p>
<p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870613-png/blog-files/data-systems-sdp.png" alt="data-systems-sdp"></p>
<p>This doesn’t just look simpler. In the first picture we are on a path to build two pipelines for data for each pair of systems, whereas in the second we are just building an input and output connector for each system to the stream data pipeline. If we have 10 systems to fully integrate this is the difference between 200 pipelines and 20 (if each system did both input and output).</p>
<p>But this is not just about systems and pipelines. Data also has to be adapted between systems. Relational databases have one data model, Hadoop another, and things like document stores still others. Providing a pipeline for raw bytes between systems would not really reduce complexity if each system produced and consumed in its own format. We would be left with a Tower of Babel where the RDBMS needs a different format plug-in for each possible source system. Instead, by having a single data format in our stream data platform we need only adapt each system to this data format and we limit the format conversions in the same way we did the number of systems.</p>
<p>This is not to imply that we will never want to process or transform data as it flows between systems—that, after all, is exactly what stream processing is all about—but we want to eliminate low-value syntactic conversions. Semantic changes, enrichment, and filtering, to produce derived data streams will still be quite important.</p>
<h3 id="4-Use-Avro-as-Your-Data-Format"><a href="#4-Use-Avro-as-Your-Data-Format" class="headerlink" title="4. Use Avro as Your Data Format"></a>4. Use Avro as Your Data Format</h3><p>Any format, be it XML, JSON, or ASN.1, provided it is used consistently across the board, is better than a mishmash of ad hoc choices.</p>
<blockquote>
<p>But if you are starting fresh with Kafka, you should pick the best format to standardize on. There are many criteria here: <code>efficiency</code>, <code>ease of use</code>, <code>support in different programming languages</code>, and so on. In our own use, and in working with a few dozen companies, we have found <code>Apache Avro</code> to be easily the most successful format for stream data.</p>
</blockquote>
<p><code>Avro</code> has a <code>JSON</code> like data model, but can be represented as either <code>JSON</code> or in a compact binary form. It comes with a very sophisticated schema description language that describes data.</p>
<p>We think <code>Avro</code> is the best choice for a number of reasons:</p>
<blockquote>
<ul>
<li>It has a direct mapping to and from JSON</li>
<li>It has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage.</li>
<li>It is very fast.</li>
<li>It has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream.</li>
<li>It has a rich, extensible schema language defined in pure JSON</li>
<li>It has the best notion of compatibility for evolving your data over time.</li>
</ul>
</blockquote>
<p>Though it may seem like a minor thing handling this kind of metadata turns out to be one of the most critical and least appreciated aspects in keeping data high quality and easily useable at organizational scale.</p>
<p>One of the critical features of <code>Avro</code> is the ability to define a schema for your data. For example an event that represents the sale of a product might look like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;time&quot;: 1424849130111,</div><div class="line">  &quot;customer_id&quot;: 1234,</div><div class="line">  &quot;product_id&quot;: 5678,</div><div class="line">  &quot;quantity&quot;:3,</div><div class="line">  &quot;payment_type&quot;: &quot;mastercard&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>It might have a schema like this that defines these five fields:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;type&quot;: &quot;record&quot;,</div><div class="line">  &quot;doc&quot;:&quot;This event records the sale of a product&quot;,</div><div class="line">  &quot;name&quot;: &quot;ProductSaleEvent&quot;,</div><div class="line">  &quot;fields&quot; : [</div><div class="line">    &#123;&quot;name&quot;:&quot;time&quot;, &quot;type&quot;:&quot;long&quot;, &quot;doc&quot;:&quot;The time of the purchase&quot;&#125;,</div><div class="line">    &#123;&quot;name&quot;:&quot;customer_id&quot;, &quot;type&quot;:&quot;long&quot;, &quot;doc&quot;:&quot;The customer&quot;&#125;,</div><div class="line">    &#123;&quot;name&quot;:&quot;product_id&quot;, &quot;type&quot;:&quot;long&quot;, &quot;doc&quot;:&quot;The product&quot;&#125;,</div><div class="line">    &#123;&quot;name&quot;:&quot;quantity&quot;, &quot;type&quot;:&quot;int&quot;&#125;,</div><div class="line">    &#123;&quot;name&quot;:&quot;payment&quot;,</div><div class="line">     &quot;type&quot;:&#123;&quot;type&quot;:&quot;enum&quot;,</div><div class="line">	     &quot;name&quot;:&quot;payment_types&quot;,</div><div class="line">             &quot;symbols&quot;:[&quot;cash&quot;,&quot;mastercard&quot;,&quot;visa&quot;]&#125;,</div><div class="line">     &quot;doc&quot;:&quot;The method of payment&quot;&#125;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>A real event, of course, would probably have more fields and hopefully better doc strings, but this gives their flavor.</p>
<blockquote>
<p>Here is how these schemas will be put to use. You will associate a schema like this with each Kafka topic. You can think of the schema much like the schema of a relational database table, giving the requirements for data that is produced into the topic as well as giving instructions on how to interpret data read from the topic.</p>
</blockquote>
<p>The schemas end up serving a number of critical purposes:</p>
<ol>
<li>They let the producers or consumers of data streams know the right fields are need in an event and what type each field is.</li>
<li>They document the usage of the event and the meaning of each field in the “doc” fields.</li>
<li>They protect downstream data consumers from malformed data, as only valid data will be permitted in the topic.</li>
</ol>
<p>The value of schemas is something that doesn’t become obvious when there is only one topic of data and perhaps a single writer and maybe a proof-of-concept reader. However when critical data streams are flowing through the pipeline and dozens or hundreds of systems depend on this, simple tools for reasoning about data have enormous impact.</p>
<p>But first, you may be asking why we need schemas at all? Isn’t the modern world of big data all about unstructured data, dumped in whatever form is convenient, and parsed later when it is queried?</p>
<h4 id="The-Need-For-Schemas"><a href="#The-Need-For-Schemas" class="headerlink" title="The Need For Schemas"></a>The Need For Schemas</h4><p>I will argue that schemas—when done right—can be a huge boon, keep your data clean, and make everyone more agile. Much of the reaction to schemas comes from two factors</p>
<blockquote>
<ul>
<li>historical limitations in relational databases that make schema changes difficult</li>
<li>the immaturity of much of the modern distributed infrastructure which simply hasn’t had the time yet to get to the semantic layer of modeling done.</li>
</ul>
</blockquote>
<p>Here is the case for schemas, point-by-point.</p>
<h5 id="1-Robustness"><a href="#1-Robustness" class="headerlink" title="1. Robustness"></a>1. Robustness</h5><p>One of the primary advantages of this type of architecture where data is modeled as streams is that applications are decoupled. Applications produce a stream of events capturing what occurred without knowledge of which things subscribe to these streams.</p>
<p>But in such a world, how can you reason about the correctness of the data? It isn’t feasible to test each application that produces a type of data against each thing that uses that data, many of these things may be off in Hadoop or in other teams with little communication. Testing all combinations is infeasible. In the absence of any real schema, new producers to a data stream will do their best to imitate existing data but jarring inconsistencies arise—certain magical string constants aren’t copied consistently, important fields are omitted, and so on.</p>
<h5 id="2-Clarity-and-Semantics"><a href="#2-Clarity-and-Semantics" class="headerlink" title="2. Clarity and Semantics"></a>2. Clarity and Semantics</h5><p>Worse, the actual meaning of the data becomes obscure and often misunderstood by different applications because there is no real canonical documentation for the meaning of the fields. One person interprets a field one way and populates it accordingly and another interprets it differently.</p>
<p>Invariably you end up with a sort of informal plain english “schema” passed around between users of the data via wiki or over email which is then promptly lost or obsoleted by changes that don’t update this informal definition. We found this lack of documentation lead to people guessing as to the meaning of fields, which inevitably leads to bugs and incorrect data analysis when these guesses are wrong.</p>
<p>Keeping an up-to-date doc string for each field means there is always a canonical definition of what that value means.</p>
<h5 id="3-Compatibility"><a href="#3-Compatibility" class="headerlink" title="3. Compatibility"></a>3. Compatibility</h5><p>Schemas also help solve one of the hardest problems in organization-<code>wide data flow: modeling and handling change in data format</code>. Schema definitions just capture a point in time, but your data needs to evolve with your business and with your code. There will always be new fields, changes in how data is represented, or new data streams.</p>
<p>This is a problem that databases mostly ignore. A database table has a single schema for all it’s rows. But this kind of rigid definition won’t work if you are writing many applications that all change at different times and evolve the schema of shared data streams. If you have dozens of applications all using a central data stream they simply cannot all update at once.</p>
<p>And managing these changes gets more complicated as more people use the data and the number of different data streams grows. Surely adding a new field is a safe change, but is removing a field? What about renaming an existing field? What about changing a field from a string to a number?</p>
<p>These problems become particularly serious because of Hadoop or any other system that stores the events. Hadoop has the ability to load data “as is” either with <code>Avro</code> or in a columnar file format like Parquet or ORC. Thus the loading of data from data streams can be made quite automatic, but what happens when there is a format change? Do you need to re-process all your historical data to convert it to the new format? That can be quite a large effort when hundreds of TBs of data are involved. How do you know if a given change will require this? Do you guess and wait to see what will break when the change goes to production?</p>
<p>Schemas make it possible for systems with flexible data format like Hadoop or Cassandra to track upstream data changes and simply propagate these changes into their own storage without expensive reprocessing. Schemas give a mechanism for reasoning about which format changes will be compatible and (hence won’t require reprocessing) and which won’t.</p>
<h5 id="4-Schemas-are-a-Conversation"><a href="#4-Schemas-are-a-Conversation" class="headerlink" title="4. Schemas are a Conversation"></a>4. Schemas are a Conversation</h5><p>I actually buy many arguments for flexible types. Dynamically typed languages have an important role to play. And arguably databases, when used by a single application in a service-oriented fashion, don’t need to enforce a schema, since, after all, the service that owns the data is the real “schema” enforcer to the rest of the organization.</p>
<p><code>However data streams are different; they are a broadcast channel.</code> Unlike an application’s database, the writer of the data is, almost by definition, not the reader. And worse, there are many readers, often in different parts of the organization. These two groups of people, the writers and the readers, need a concrete way to describe the data that will be exchanged between them and schemas provide exactly this.</p>
<h5 id="5-Schemas-Eliminate-The-Manual-Labor-of-Data-Science"><a href="#5-Schemas-Eliminate-The-Manual-Labor-of-Data-Science" class="headerlink" title="5. Schemas Eliminate The Manual Labor of Data Science"></a>5. Schemas Eliminate The Manual Labor of Data Science</h5><p>It is almost a truism that data science, which I am using as a short-hand here for “putting data to effective use”, is 80% parsing, validation, and low-level data munging. Data scientists complain that their training spent too much time on statistics and algorithms and too little on regular expressions, xml parsing, and practical data munging skills. This is quite true in most organizations, but it is somewhat disappointing that there are people with PhDs in Physics spending their time trying to regular-expression date fields out of mis-formatted CSV data (that inevitably has commas inside the fields themselves).</p>
<p>This problem is particularly silly because the nonsense data isn’t forced upon us by some law of physics, this data doesn’t just arise out of nature. Whenever you have one team whose job is to parse out garbage data formats and try to munge together inconsistent inputs into something that can be analyzed, there is another corresponding team whose job is to generate that garbage data. And once a few people have built complex processes to parse the garbage, that garbage format will be enshrined forever and never changed. Had these two teams talked about what data was needed for analysis and what data was available for capture, the entire problem could have been prevented.</p>
<p>The advantage isn’t limited to parsing. Much of what is done in this kind of data wrangling is munging disparate representations of data from various systems to look the same. It will turn out that similar business activities are captured in dramatically different ways in different parts of the same business. Building post hoc transformations can attempt to coerce these to look similar enough to perform analysis. However the same thing is possible at data capture time by just defining an enterprise-wide schema for common activities. If sales occur in 14 different business units it is worth figuring out if there is some commonality among these that can be enforced so that analysis can be done over all sales without post-processing. Schemas won’t automatically enforce this kind of thoughtful data modeling but they do give a tool by which you can enforce a standard like this.</p>
<h4 id="At-LinkedIn"><a href="#At-LinkedIn" class="headerlink" title="At LinkedIn"></a>At LinkedIn</h4><p>We put this idea of schemafied event data into practice at large scale at LinkedIn. User activity events, metrics data, stream processing output, data computed in Hadoop, and database changes were all represented as streams of <code>Avro</code> events.</p>
<p>These events were automatically loaded into <code>Hadoop</code>. When a new Kafka topic was added that data would automatically flow into <code>Hadoop</code> and a corresponding <code>Hive</code> table would be created using the event schema. When the schema evolved that metadata was propagated into <code>Hadoop</code>. When someone wanted to create a new data stream, or evolve the schema for an existing one, the schema for that stream would undergo a quick review by a group of people who cared about data quality. This review would ensure this stream didn’t duplicate an existing event and that things like dates and field names followed the same conventions, and so on. Once the schema change was reviewed it would automatically flow throughout the system. This leads to a much more consistent, structured representation of data throughout the organization.</p>
<p>Other companies we have worked with have largely come to the same conclusion. Many started with loosely structured <code>JSON</code> data streams with no schemas or contracts as these were the easiest to implement. But over time almost all have realized that this loose definition simply doesn’t scale beyond a dozen people and that some kind of stronger metadata is needed to preserve data quality.</p>
<h4 id="Back-to-Avro"><a href="#Back-to-Avro" class="headerlink" title="Back to Avro"></a>Back to Avro</h4><blockquote>
<p>Okay that concludes the case for schemas. We chose <code>Avro</code> as a schema representation language after evaluating all the common options—<code>JSON</code>, <code>XML</code>, <code>Thrift</code>, <code>protocol buffers</code>, etc. We recommend it because it is the best thought-out of these for this purpose. It has a pure JSON representation for readability but also a binary representation for efficient storage. It has an exact compatibility model that enables the kind of compatibility checks described above. It’s data model maps well to <code>Hadoop</code> data formats and <code>Hive</code> as well as to other data systems. It also has bindings to all the common programming languages which makes it convenient to use programmatically.</p>
</blockquote>
<p>Good overviews of Avro can be found <a href="http://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html" target="_blank" rel="external">here</a> and <a href="http://radar.oreilly.com/2014/11/the-problem-of-managing-schemas.html" target="_blank" rel="external">here</a>.</p>
<p>We have built tools for implementing <code>Avro</code> with Kafka or other systems as part of the <a href="http://confluent.io/product" target="_blank" rel="external">Confluent Platform</a>, you can read more about this schema support <a href="http://confluent.io/docs/current/schema-registry/docs/index.html" target="_blank" rel="external">here</a>.</p>
<h4 id="Effective-Avro"><a href="#Effective-Avro" class="headerlink" title="Effective Avro"></a>Effective Avro</h4><p>Here are some recommendations specific to <code>Avro</code>:</p>
<blockquote>
<ul>
<li>Use enumerated values whenever possible instead of magic strings. Avro allows specifying the set of values that can be used in the schema as an enumeration. This avoids typos in data producer code making its way into the production data set that will be recorded for all time.</li>
<li>Require documentation for all fields. Even seemingly obvious fields often have non-obvious details. Try to get them all written down in the schema so that anyone who needs to really understand the meaning of the field need not go any further.</li>
<li>Avoid non-trivial union types and recursive types. These are Avro features that map poorly to most other systems. Since our goal is an intermediate format that maps well to other systems we want to avoid any overly advanced features.</li>
<li>Enforce reasonable schema and field naming conventions. Since these schemas will map into Hadoop having common fields like customer_id named the same across events will be very helpful in making sure that joins between these are easy to do. A reasonable scheme might be something like PageViewEvent, OrderEvent, ApplicationBounceEvent, etc.</li>
</ul>
</blockquote>
<h3 id="5-Share-Event-Schemas"><a href="#5-Share-Event-Schemas" class="headerlink" title="5. Share Event Schemas"></a>5. Share Event Schemas</h3><p>Whenever you see a common activity across multiple systems, try to use a common schema for this activity. Doing so often requires a small amount of thought, but it saves a lot of work in using the data.</p>
<blockquote>
<p>An example of this that is common to all businesses is application errors. Application errors can generally be modeled in a fairly general way (say an error has a stack trace, an application name, an error message, and so on) and doing so lets the ErrorEvent stream capture the full stream of errors across the company. This means tools that process, alert, analyze, or report on errors will automatically extend to each new system that emits data to this stream. Had each application derived it’s own data format for errors than each error consumer would need to somehow munge all the disparate error streams into a common format for processing or analytics.</p>
</blockquote>
<p>This experience is common. Any time you can make similar things look similar by data modeling it is almost free to do so—you just need a schema—but every time you do this in post processing you need to maintain code to do this post-processing indefinitely.</p>
<blockquote>
<p>A corollary to this is to avoid system or application names in event names. When adding event capture to a system, named, say, “CRS”, there is a tendency to name every event with CRS as part of the name (“CRSOrderEvent”, “CRSResendEvent”, etc). However our experience was that systems tend to get replaced, while many many applications will end up feeding off the event stream. If you put the system name in the event stream name the source system can never change, or the new replacement system will have to produce data with the old name. Instead, name events in a system and application agnostic way—just use the high-level business activity they represent. So if CRS is an order management system then just <code>OrderEvent</code> is sufficient.</p>
</blockquote>
<h2 id="Pure-Event-Streams"><a href="#Pure-Event-Streams" class="headerlink" title="Pure Event Streams"></a>Pure Event Streams</h2><blockquote>
<p>Kafka’s data model is built to represent event streams.</p>
<p>A stream in Kafka is modeled by a <code>topic</code>, which is the logical name given to that data. Each message has a key, which is used to partition data over the cluster as well as a body which would contain the <code>Avro</code> record data (or whichever format you have chosen).</p>
</blockquote>
<p>Kafka maintains a configurable history of the stream. This can be managed with an SLA (e.g. retain 7 days) or by size (e.g retain 100 GB) or by key (e.g. retain at least that last update for each key).</p>
<p>Let’s begin with pure event data—the activities taking place inside the company. In a web company these might be clicks, impression, and various user actions. FedEx might have package deliveries, package pick ups, driver positions, notifications, transfers and so on.</p>
<p>These type of events can be represented with a single logical stream per action type. <code>For simplicity I recommend naming the Avro schema and the topic the same thing, e.g. PageViewEvent.</code></p>
<blockquote>
<p>If the event has a natural primary key you can use that to partition data in Kafka, otherwise the Kafka client will automatically load balance data for you.</p>
<p>Pure event streams will always be retained by size or time. You can choose to keep a month or 100GB per stream or whatever policy you define.</p>
</blockquote>
<p>We experimented at various times with mixing multiple events in a single topic and found this generally lead to undue complexity. <strong>Instead, give each event it’s own topic and consumers can always subscribe to multiple such topics to get a mixed feed when they want that.</strong></p>
<p>By having a single schema for each topic you will have a much easier time mapping a topic to a <code>Hive</code> table in <code>Hadoop</code>, a database table in a relational DB or other structured stores.</p>
<h2 id="Application-Logs"><a href="#Application-Logs" class="headerlink" title="Application Logs"></a>Application Logs</h2><p>The term “logs” is somewhat undefined. It sometimes means error messages, stack traces, and warnings in semi-formated english such as a server might record in the course of request processing. It sometimes means fairly structured request logs like might come out of Apache HTTPD. It sometimes means event data which might be dumped to a log file.</p>
<blockquote>
<p>For this section I will use “logs” to refer to the semi-structured application logs. Structured logs like request logs and other activity or event data should just be treated like any other event as described and should have a schema per activity that capture exactly the fields that make up that event.</p>
</blockquote>
<p>However there can be some value in capturing application logs in Kafka as well.</p>
<blockquote>
<p>At LinkedIn, all application logs were published to Kafka via a custom log4j appender for Java. These were loaded into Hadoop for batch analysis as well as being delivered to real-time tools that would subscribe to the stream of application logs for reporting on sudden error spikes or changes after new code was pushed. These errors were also joined back to the stream of service requests in a stream processing system so we could get a wholistic picture of utilization, latency, errors, and the call patterns amongst our services.</p>
</blockquote>
<h2 id="System-Metrics"><a href="#System-Metrics" class="headerlink" title="System Metrics"></a>System Metrics</h2><p>We also published a stream of statistics about applications and servers. These had a common format across all applications. They captured things like unix performance statistics (the kind of I/O and CPU load you would get out of iostat or top) as well as application defined gauges and counters captured using things like JMX.</p>
<p>This all went into a central feed of monitoring statistics that fed the company wide monitoring platform. Any new system could integrate by publishing its statistics, and all statistics were available in a company-wide monitoring store.</p>
<h2 id="Derived-Streams"><a href="#Derived-Streams" class="headerlink" title="Derived Streams"></a>Derived Streams</h2><p>Mostly so far we have talked about producing streams of events into Kafka. These events are things happening in applications or data systems. I’ll call these “primary” data streams.</p>
<p>However there is another type of data stream, a <code>derived</code> stream. These are streams that were computed off other data streams. This computation could be done in real-time as events occurred, either in an application or in a stream processing system, or it could be done periodically in Hadoop. These derived streams often do some kind of enrichment, say adding on new attributes not present in the original event.</p>
<p>Derived streams require no particular handling. They can be computed using simple programs that directly consume from Kafka and write back derived results or they can be computed using a stream processing system. Regardless which route is taken the output stream is just another Kafka topic so the consumer of the data need not be concerned with the mechanism used to produce it. A batch computed stream from Hadoop will look no different from a stream coming from a stream processing system, except that it will be higher latency.</p>
<h2 id="Hadoop-Data-Loads"><a href="#Hadoop-Data-Loads" class="headerlink" title="Hadoop Data Loads"></a>Hadoop Data Loads</h2><p>There are many ways to load data from Kafka into Hadoop and there are many aspects of doing this well.</p>
<blockquote>
<p>One of the most critical is doing it in a fully automated way. Since Hadoop will likely want to load data from all the data streams, you don’t want to be doing any custom set-up or mappings between your Kafka topics and your Hadoop data sets and Hive tables.</p>
</blockquote>
<p>We have packaged a simple system for doing this called <code>Camus</code> that came out of LinkedIn. It is described in more detail <a href="http://confluent.io/docs/current/camus/docs/index.html" target="_blank" rel="external">here</a>.</p>
<h2 id="Hadoop-Data-Publishing"><a href="#Hadoop-Data-Publishing" class="headerlink" title="Hadoop Data Publishing"></a>Hadoop Data Publishing</h2><p>The opposite of loading data into Hadoop is just as common. After all, the purpose of Hadoop is to act as a computational engine, and whatever it computes must go somewhere for serving. Often this piping can be quite complex as the Hadoop cluster may not be physically co-located with the serving system, and even if it is you often don’t want Hadoop writing directly to a database used for serving live requests as it will easily overwhelm such a system.</p>
<p>So the stream data platform is a great place to publish these derived streams from Hadoop. The stream data platform can handle the distribution of data across data centers. As far as the recipient is concerned this is just another stream which happens to receive updates only periodically.</p>
<p>This allows the same plugins that load data from a stream processor to also be used for loading Hadoop data. So an analytical job can begin its life in Hive and later migrate to a lower latency stream processing platform without needing to rewrite the serving layer.</p>
<h2 id="Database-Changes"><a href="#Database-Changes" class="headerlink" title="Database Changes"></a>Database Changes</h2><p>Database changes require some particular discussion. Database data is somewhat different from pure event streams in that it models updates—that is, rows that change.</p>
<p>The first and arguably most important issue is how changes are captured from the database. There are two common methods for doing this:</p>
<ol>
<li>Polling for changes</li>
<li>Direct log integration with the database</li>
</ol>
<p>Polling for changes requires little integration with the database so it is the easiest to implement. Polling requires some kind of last modified timestamp that can be used to detect new values so it requires some co-operation from the schema. There are also a number of gotchas in implementing correct change capture by polling. First, long running transactions can lead to rows that commit out of timestamp order when using simple time; this means that rows can appear in the near past. Many databases support some kind of logical change number that can help alleviate this problem. This method also doesn’t guarantee that every change is captured, when multiple updates occur on a single row in between polling intervals only the last of these is delivered. It also doesn’t capture deleted rows.</p>
<p>All the limitations of polling are fixed by direct integration with the database log, but the mechanism for integration is very database specific. MySQL has a binlog, Postgres has logical replication, Oracle has a number of products including Change Capture, Streams, XStreams, and Golden Gate, MongoDB has the oplog. These features range from deeply internal features like the MySQL binlog to full productized apis like XStreams. These log mechanisms will capture each change and have lower overhead than polling.</p>
<p>This is an area Confluent will be doing more work in the future.</p>
<h2 id="Retaining-Database-Changes"><a href="#Retaining-Database-Changes" class="headerlink" title="Retaining Database Changes"></a>Retaining Database Changes</h2><blockquote>
<p>For pure event data, Kafka often retains just a short window of events, say a week of data. However for database change streams, systems will want to do full restores off of this Kafka changelog. Kafka does have a relevant feature that can help with this called <code>Log Compaction</code>. </p>
<p><code>Log compaction ensures</code> that rather than discarding data by time, Kafka will retain at least the final update for each key. This means that any client reading the full log from Kafka will get a full copy of the data and not need to disturb the database. This is useful for cases where there are many subscribers that may need to restore a full copy of data to prevent them from overwhelming the source database.</p>
</blockquote>
<h2 id="Extract-Database-Data-As-is-Then-Transform"><a href="#Extract-Database-Data-As-is-Then-Transform" class="headerlink" title="Extract Database Data As-is, Then Transform"></a>Extract Database Data As-is, Then Transform</h2><p>Often databases have odd schemas specific to idiosyncrasies of their query pattern or internal implementation. Perhaps it stores data in odd key-value blobs. We would generally like to clean up this type of data for usage.</p>
<p>There are three ways we could do this clean-up:</p>
<ol>
<li>As part of the extraction process</li>
<li>As a stream processor that reads the original data stream and produces a “cleaned” stream with a more sane schema</li>
<li>In one of the destination system</li>
</ol>
<p>Pushing the clean-up to the consumer is not ideal as there can be many consumers so the work ends up being done over and over.</p>
<p>Clean up as part of the extraction is tempting, but often leads to problems. One person’s clean-up is another business logic and not all clean-ups are reversible so important aspects of the source data may be lost in the cleaning process.</p>
<p><strong>Our finding was that publishing the original data stream, what actually happened, had value; any additional clean-up could then be layered on top of that as a new stream of its own. This seems wasteful at first, but the reality is that this kind of storage is so cheap that it is often not a significant cost.</strong></p>
<h2 id="Stream-Processing"><a href="#Stream-Processing" class="headerlink" title="Stream Processing"></a>Stream Processing</h2><p>One of the goals of the stream data platform is being able to stream data between data systems. The other goal is to enable processing of data streams as data arrives.</p>
<p>Stream processing is easily modeled in the stream data platform as just a transformation between streams. A stream processing job continually reads from one or more data streams and outputs one or more data streams of output. These kind of processors can be strung together into a graph of flowing data:</p>
<p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870623-png/blog-files/dag.png" alt="dag"></p>
<p>The particular method used to implement the processes that do the transformation is actually something of an implementation detail to the users of the output, though obviously it is an important detail to the implementor of the process.</p>
<p>Publishing data back into Kafka like this provides a number of benefits. First it decouples parts of the processing graph. One set of processing jobs may be written by one team and another by another. They may be built using different technologies. Most importantly we don’t want a slow downstream processor to be able to cause back-pressure to seize up anything that feeds data to it. Kafka acts as this buffer between the processors that can let an organization happily share data.</p>
<p>The most basic approach is to directly use the Kafka APIs to read input data streams, process that input and produce output streams. This can be done in a simple program in any programming language. Kafka allows you to scale these out by running multiple instances of these programs, it will spread the load across these instances. Kafka guarantees at-least once delivery of data and these programs will inherit that guarantee.</p>
<p>The advantage of the simple, framework free approach is that it is simple to operate and reason about and available in any language that has good Kafka clients.</p>
<p>However there are several stream processing systems that can potentially provide additional features. Used in this fashion as processing between Kafka topics they generally can’t give stronger guarantees or improve performance beyond what Kafka itself provides (though they can certainly make both worse). However building complex real-time processing can often be made simpler with a processing framework.</p>
<p>There are three common frameworks for stream processing:</p>
<ul>
<li><a href="http://storm.apache.org/" target="_blank" rel="external">Storm</a></li>
<li><a href="http://samza.apache.org/" target="_blank" rel="external">Samza</a></li>
<li><a href="https://spark.apache.org/streaming/" target="_blank" rel="external">Spark Streaming</a></li>
</ul>
<p>Coincidentally all are Apache projects beginning with the letter “S”! Of the two Storm and Samza are somewhat comparable, being message at a time stream processing systems, while Spark is more of a mini-batch framework that applies the (very nice) Spark abstraction to smaller batches of data. There are comparisons between these systems <a href="http://samza.apache.org/learn/documentation/0.7.0/comparisons/storm.html" target="_blank" rel="external">here</a> as well as <a href="http://samza.apache.org/learn/documentation/0.7.0/comparisons/spark-streaming.html" target="_blank" rel="external">here</a> and <a href="http://www.javacodegeeks.com/2015/02/streaming-big-data-storm-spark-samza.html" target="_blank" rel="external">here</a>.</p>
<p>So when should you use one of these stream processing frameworks?</p>
<p>Where these frameworks really shine is in areas where there will be lots of complex transformations. If there will be only a small number of processes doing transformations the cost of adopting a complex framework may not pay off, and the framework may come with operational and performance costs of their own. However if there will be a large number of transformations, making these easier to write should justify the additional operational burden.</p>
<p>Over time we think these frameworks will get more mature and more code will move into this stream processing domain, so the future of stream processing frameworks is quite bright.</p>
<h2 id="Have-Any-Streaming-Experiences-to-Share"><a href="#Have-Any-Streaming-Experiences-to-Share" class="headerlink" title="Have Any Streaming Experiences to Share?"></a>Have Any Streaming Experiences to Share?</h2><p>That is it for my current list of data stream do’s and don’ts. If you have additional recommendations to add to this, pass them on.</p>
<p>Meanwhile we’re working on trying to put a lot of these best practices into software as part of the Confluent Platform which you can find out more about here.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h3 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h3&gt;&lt;p&gt;I’m preparing to start a new journey, which I’ll annouce soon. The new opportunity is about data.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Apache Kafka&lt;/code&gt; has been a hot topic in the data field for a while, and, of course, I cannot taking on data problems without it. While learning and reading more about Kafka, I found &lt;a href=&quot;http://www.confluent.io/blog/&quot;&gt;Conluent’s official tech blog&lt;/a&gt; has been an amazingly useful place to find out materials I need - because the company is started by founders of Kafka in Linkedin.&lt;/p&gt;
&lt;p&gt;Thus, I decided to repost a few great blogs from Confluent, add my own notes, and publish them here.&lt;/p&gt;
&lt;p&gt;All posts are &lt;a href=&quot;https://phoenixjiangnan.github.io/categories/distributed-system/kafka/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;This post is Part II of a couple blogs from Concluent’s CEO Jay Kreps. Part II has been really amazing to present you an overall view of all major technical details of Kafka.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;http://www.confluent.io/blog/stream-data-platform-2/&quot;&gt;Original Post&lt;/a&gt; from Jay Kreps. February 25, 2015.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&quot;http://cdn2.hubspot.net/hub/540072/file-3062870613-png/blog-files/data-systems-sdp.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;This is the second part of our guide on streaming data and Apache Kafka. In part one I talked about the uses for real-time data streams and explained our idea of a stream data platform. The remainder of this guide will contain specific advice on how to go about building a stream data platform in your organization.&lt;/p&gt;
&lt;p&gt;This advice is drawn from our experience building and implementing Kafka at LinkedIn and rolling it out across all the data types and systems there. It also comes from four years working with tech companies in Silicon Valley to build Kafka-based stream data platforms in their organizations.&lt;/p&gt;
&lt;p&gt;This is meant to be a living document. As we learn new techniques, or new tools become available, I’ll update it.&lt;/p&gt;
&lt;h2 id=&quot;Getting-Started&quot;&gt;&lt;a href=&quot;#Getting-Started&quot; class=&quot;headerlink&quot; title=&quot;Getting Started&quot;&gt;&lt;/a&gt;Getting Started&lt;/h2&gt;&lt;p&gt;Much of the advice in this guide covers techniques that will scale to hundreds or thousands of well formed data streams. No one starts with that, of course. Usually you start with one or two trial applications, often ones that have scalability requirements that make other systems less suitable. Even in this kind of limited deployment, though, the techniques described in this guide will help you to start off with good practices, which is critical as your usage expands.&lt;/p&gt;
&lt;p&gt;Starting with something more limited is good, it let’s you get a hands on feel for what works and what doesn’t, so that, when broader adoption comes, you are well prepared for it.&lt;/p&gt;
&lt;h2 id=&quot;Recommendations&quot;&gt;&lt;a href=&quot;#Recommendations&quot; class=&quot;headerlink&quot; title=&quot;Recommendations&quot;&gt;&lt;/a&gt;Recommendations&lt;/h2&gt;&lt;p&gt;I’ll give a set of general recommendations for streaming data and Kafka and then discuss some specifics of different types of data.&lt;/p&gt;
&lt;h3 id=&quot;1-Limit-The-Number-of-Clusters&quot;&gt;&lt;a href=&quot;#1-Limit-The-Number-of-Clusters&quot; class=&quot;headerlink&quot; title=&quot;1. Limit The Number of Clusters&quot;&gt;&lt;/a&gt;1. Limit The Number of Clusters&lt;/h3&gt;&lt;p&gt;In early experimentation phases it is normal to end up with a few different Kafka clusters as adoption occurs organically in different parts of the organization.&lt;/p&gt;
    
    </summary>
    
      <category term="distributed system" scheme="https://phoenixjiangnan.github.io/categories/distributed-system/"/>
    
      <category term="kafka" scheme="https://phoenixjiangnan.github.io/categories/distributed-system/kafka/"/>
    
    
      <category term="kafka" scheme="https://phoenixjiangnan.github.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Putting Apache Kafka To Use - A Practical Guide to Building a Stream Data Platform (Part 1)</title>
    <link href="https://phoenixjiangnan.github.io/2016/10/03/distributed%20system/kafka/Putting-Apache-Kafka-To-Use-A-Practical-Guide-to-Building-a-Stream-Data-Platform-Part-1/"/>
    <id>https://phoenixjiangnan.github.io/2016/10/03/distributed system/kafka/Putting-Apache-Kafka-To-Use-A-Practical-Guide-to-Building-a-Stream-Data-Platform-Part-1/</id>
    <published>2016-10-04T01:47:07.000Z</published>
    <updated>2016-10-03T15:19:28.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>I’m preparing to start a new journey, which I’ll annouce soon. The new opportunity is about data.</p>
<p><code>Apache Kafka</code> has been a hot topic in the data field for a while, and, of course, I cannot taking on data problems without it. While learning and reading more about Kafka, I found <a href="http://www.confluent.io/blog/" target="_blank" rel="external">Conluent’s official tech blog</a> has been an amazingly useful place to find out materials I need - because the company is started by founders of Kafka in Linkedin.</p>
<p>Thus, I decided to repost a few great blogs from Confluent, add my own notes, and publish them here.</p>
<p>All posts are <a href="https://phoenixjiangnan.github.io/categories/distributed-system/kafka/">here</a></p>
</blockquote>
<hr>
<blockquote>
<p>This post is Part I of a couple blogs from Concluent’s CEO Jay Kreps. In this blog, he discussed the reason he led to develop Kafka in Linkedin, the problems Kafka can solve, and Kafka’s role in modern enterprise data system.</p>
<p>From my perspective, Part I is not as helpful as Part II w.r.t. technical details. But it certainly presents the fundamental background of how Kafka came to the world.</p>
<p><a href="http://www.confluent.io/blog/stream-data-platform-1/" target="_blank" rel="external">Original Post</a> from Jay Kreps. February 25, 2015.</p>
</blockquote>
<hr>
<p>These days you hear a lot about “stream processing”, “event data”, and “real-time”, often related to technologies like Kafka, Storm, Samza, or Spark’s Streaming module. Though there is a lot of excitement, not everyone knows how to fit these technologies into their technology stack or how to put it to use in practical applications.</p>
<p>This guide is going to discuss our experience with real-time data streams: how to build a home for real-time data within your company, and how to build applications that make use of that data. All of this is based on real experience: we spent the last five years building Apache Kafka, transitioning LinkedIn to a fully stream-based architecture, and helping a number of Silicon Valley tech companies do the same thing.</p>
<p>The first part of the guide will give a high-level overview of what we came to call a <code>stream data platform</code>: a central hub for real-time streams of data. It will cover the what and why of this idea.</p>
<p>The second part will dive into a lot of specifics and give advice on how to put this into practice effectively.</p>
<p>But first, what is a stream data platform?</p>
<h2 id="The-Stream-Data-Platform-A-Clean-Well-lighted-Place-For-Events"><a href="#The-Stream-Data-Platform-A-Clean-Well-lighted-Place-For-Events" class="headerlink" title="The Stream Data Platform: A Clean, Well-lighted Place For Events"></a>The Stream Data Platform: A Clean, Well-lighted Place For Events</h2><p>We built Apache Kafka at LinkedIn with a specific purpose in mind: to serve as a central repository of data streams. But why do this? There were two motivations.</p>
<p>The first problem was how to transport data between systems. We had lots of data systems: relational OLTP databases, Hadoop, Teradata, a search system, monitoring systems, OLAP stores, and derived key-value stores. Each of these needed reliable feeds of data in a geographically distributed environment. I’ll call this problem <code>data integration</code>, though we could also call it <code>ETL</code>.</p>
<p>The second part of this problem was the need to do richer analytical data processing—the kind of thing that would normally happen in a data warehouse or Hadoop cluster—but with very low latency. I call this “stream processing” though others might call it “messaging” or CEP or something similar.</p>
<a id="more"></a>
<p>I’ll talk a little about how these ideas developed at LinkedIn. At first we didn’t realize that these problems were connected at all. Our approach was very ad hoc: we built jerry-rigged piping between systems and applications on an as needed basis and shoe-horned any asynchronous processing into request-response web services. Over time this set-up got more and more complex as we ended up building pipelines between all kinds of different systems:</p>
<p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870508-png/blog-files/data-flow-ugly.png" alt="data-flow-ugly"></p>
<p>Each of the pipelines was problematic in different ways. Our pipeline for log data was scalable but lossy and could only deliver data with high latency. Our pipeline between Oracle instances was fast, exact, and real-time, but not available to any other systems. Our pipeline of Oracle data for Hadoop was periodic CSV dumps—high throughput, but batch. Our pipeline of data to our search system was low latency, but unscalable and tied directly to the database. Our messaging systems were low latency but unreliable and unscalable.</p>
<p>As we added data centers geographically distributed around the world we had to build out geographical replication for each of these data flows. As each of these systems scaled, the supporting pipelines had to scale with them. Building simple duct tape pipelines had been easy enough but scaling these and operationalizing them was an enormous effort. I felt that my team, which was supposed to be made up of distributed systems engineers, was really acting more as distributed system plumbers.</p>
<p>Worse, the complexity meant that the data was always unreliable. Our reports were untrustworthy, derived indexes and stores were questionable, and everyone spent a lot of time battling data quality issues of all kinds. I remember an incident where we checked two systems that had similar data and found a discrepancy; we checked a third to try to determine which of these was correct and found that it matched neither.</p>
<p>At the same time we weren’t just shipping data from place to place; we also wanted to do things with it. Hadoop had given us a platform for batch processing, data archival, and ad hoc processing, and this had been enormously successful, but we lacked an analogous platform for low-latency processing. Many applications— especially our monitoring systems, search indexing pipeline, analytics, and security and fraud analysis—required latency of no more than a few seconds. These types of applications had no natural home in our infrastructure stack.</p>
<p>So in 2010 we decided to build a system that would focus on capturing data as streams and use this as both the integration mechanism between systems and also allow real-time processing of these same data streams. This was the origin of Apache Kafka.</p>
<p>We imagined something like this:</p>
<p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870518-png/blog-files/stream_data_platform.png" alt=""></p>
<p>For a long time we didn’t really have a name for what we were doing (we just called it “Kafka stuff” or “the global commit log thingy”) but over time we came to call this kind of data “stream data”, and the concept of managing this centrally a “stream data platform”.</p>
<p>Our resulting system architecture went from the ugly spaghetti of pipelines I described before to a much cleaner stream-centric system:</p>
<p>A modern stream-centric data architecture built around Apache Kafka A modern stream-centric data architecture built around Apache Kafka</p>
<p>In this setup Kafka acts as a kind of universal pipeline for data. Each system can feed into this central pipeline or be fed by it; applications or stream processors can tap into it to create new, derived streams, which in turn can be fed back into the various systems for serving. Continuous feeds of well-formed data act as a kind of lingua franca across systems, applications, and data centers.</p>
<p>For example if a user updates their profile that update might flow into our stream processing layer where it would be processed to standardize their company information, geography, and other attributes. From there that stream might flow into search indexes and our social graph for querying, into a recommendation system for job matching; all of this would happen in milliseconds. This same flow would load into Hadoop to provide that data to the warehouse environment.</p>
<p>This usage at LinkedIn grew to phenomenal scale. Today at LinkedIn Kafka handles over 500 billion events per day spread over a number of data centers. It became the backbone for data flow between systems of all kinds, the core pipeline for Hadoop data, and the hub for stream processing.</p>
<p>Since Kafka was open source this usage spread beyond LinkedIn into companies of all kinds doing similar things.</p>
<p>In the rest of this article I’m going to outline a few details about this stream-centric world view, how it works, and what problems it solves.</p>
<h2 id="Streaming-Data"><a href="#Streaming-Data" class="headerlink" title="Streaming Data"></a>Streaming Data</h2><p>Most of what a business does can be thought of as streams of events. Sometimes this is obvious. Retail has streams of orders, sales, shipments, price adjustments, returns, and so on. Finance has orders, stock prices, and other financial time series. Web sites have streams of clicks, impressions, searches, and so on. Big software systems have streams of requests, errors, machine metrics, and logs. Indeed one view of a business is as a kind of data processing system that takes various input streams and produces corresponding output streams (and maybe some physical goods along the way).</p>
<p>This view of data can seem a little foreign to people who are more accustomed to thinking of data as rows in databases rather than as events, so let’s look at a few practical aspects of event data.</p>
<h2 id="The-Rise-of-Events-and-Event-Streams"><a href="#The-Rise-of-Events-and-Event-Streams" class="headerlink" title="The Rise of Events and Event Streams"></a>The Rise of Events and Event Streams</h2><p>Your database stores the current state of your data. But the current state is always caused by some actions that took place in the past. The actions are the events. Your inventory table is the state that results from the purchase and sale events that have been made, bank balances are the result of credits and debits, and the latency graph for your web server is an aggregation of the stream of HTTP request times.</p>
<p>Much of what people refer to when they talk about “big data” is really the act of capturing these events that previously weren’t recorded anywhere and putting them to use for analysis, optimization, and decision making. In some sense these events are the other half of the story the database tables don’t tell: they are the story of what the business did.</p>
<p>Event data has always been present in finance, where stock ticks, market indicators, trades, and other time series data are naturally thought of as event streams.</p>
<p>But the tech industry popularized the most modern incarnation of technology for capture and use of this data. Google transformed the stream of ad clicks and ad impressions into a multi-billion dollar business. In the web space event data is often called “log data”, because, lacking any proper infrastructure for their events, log files are often where the events are put. Systems like Hadoop are often described as being for <code>log processing</code>, but that usage might be better described as batch event storage and processing.</p>
<p>Web companies were probably the earliest to do this because the process of capturing event data in a web site is very easy: a few lines of code can add tracking that records what users on a website did. As a result a single page load or mobile screen on a popular website is likely recording dozens or even hundreds of these events for analysis and monitoring.</p>
<p>You will sometimes hear about “machine generated data”, but this is just event data by another name. In some sense virtually all data is machine generated, since it is made by computer systems.</p>
<p>Likewise there is a lot of talk about device data and the “internet of things”. This is a phrase that means a lot of things to different people, but a large part of the promise has to do with applying the same data collection and analytics of big web systems to industrial devices and consumer goods. In other words, more event streams.</p>
<h2 id="Databases-Are-Event-Streams"><a href="#Databases-Are-Event-Streams" class="headerlink" title="Databases Are Event Streams"></a>Databases Are Event Streams</h2><p>Event streams are an obvious fit for log data or things like “orders”, “sales”, “clicks” or “trades” that are obviously event-like. But, like most people, you probably keep much of your data in databases, whether relational databases like Oracle, MySQL, and Postgres, or newer distributed databases like MongoDB, Cassandra, and Couchbase. These would seem at first to be far removed from the world of events or streams.</p>
<p>But, in fact, data in databases can also be thought of as an event stream. The easiest way to understand the event stream representation of a database is to think about the process of creating a backup or standby copy of a database. A naive approach to doing this might be to dump out the contents of your database periodically, and load this up into the standby database. If we do this only infrequently, and our data isn’t too large, than taking a full dump of all the data may be quite feasible. In fact many backup and ETL procedures do exactly this. However this approach won’t scale as we increase the frequency of the data capture: if we do a full dump of data twice a day, it will take twice the system resources, and if we do it hourly, 24 times as much. The obvious approach to make this more efficient is to take a “diff” of what has changed and just fetch rows that have been newly created, updated, or deleted since our last diff was taken. Using this method, if we take our diffs twice as often, the diffs themselves will get (roughly) half as big, and the system resources will remain more or less the same as we increase the frequency of our data capture.</p>
<p>Why not take this process to the limit and take our diffs more and more frequently? If we do this what we will be left with is a continuous sequence of single row changes. This kind of event stream is called change capture, and is a common part of many databases systems (Oracle has XStreams and GoldenGate, MySQL has binlog replication, and Postgres has Logical Log Streaming Replication).</p>
<p>By publishing the database changes into the stream data platform you add this to the other set of event streams. You can use these streams to synchronize other systems like a Hadoop cluster, a replica database, or a search index, or you can feed these changes into applications or stream processors to directly compute new things off the changes. These changes are in turn published back as streams that are available to all the integrated systems.</p>
<h2 id="What-Is-a-Stream-Data-Platform-For"><a href="#What-Is-a-Stream-Data-Platform-For" class="headerlink" title="What Is a Stream Data Platform For?"></a>What Is a Stream Data Platform For?</h2><blockquote>
<p>A stream data platform has two primary uses:</p>
<ol>
<li><p>Data Integration: The stream data platform captures streams of events or data changes and feeds these to other data systems such as relational databases, key-value stores, Hadoop, or the data warehouse.</p>
</li>
<li><p>Stream processing: It enables continuous, real-time processing and transformation of these streams and makes the results available system-wide.</p>
</li>
</ol>
</blockquote>
<p>In its first role, the stream data platform is a central hub for data streams. Applications that integrate don’t need to be concerned with the details of the original data source, all streams look the same. It also acts as a buffer between these systems—the publisher of data doesn’t need to be concerned with the various systems that will eventually consume and load the data. This means consumers of data can come and go and are fully decoupled from the source.</p>
<p>If you adopt a new system you can do this by tapping into your existing data streams rather than instrumenting each individual source system and application for each possible destination. The streams all look the same whether they originated in log files, a database, Hadoop, a stream processing system, or wherever else. This makes adding a new data system a much cheaper proposition—it need only integrate with the stream data platform not with every possible data source and sink directly.</p>
<p>A similar story is important for Hadoop which wants to be able to maintain a full copy of all the data in your organization and act as a “data lake” or “enterprise data hub”. Directly integrating each data source with HDFS is a hugely time consuming proposition, and the end result only makes that data available to Hadoop. This type of data capture isn’t suitable for real-time processing or syncing other real-time applications. Likewise this same pipeline can run in reverse: Hadoop and the data warehouse environment can publish out results that need to flow into appropriate systems for serving in customer-facing applications.</p>
<p>The stream processing use case plays off the data integration use case. All the streams that are captured for loading into Hadoop for archival are equally available for continuous “stream processing” as data is captured in the stream. The results of the stream processing are just a new, derived stream. This stream looks just like any other stream and is available for loading in all the data systems that have integrated with the stream data platform.</p>
<p>This stream processing can be done using simple application code that taps into the stream of events and publishes out a new stream of events. But this type of application code can be made easier with the help of a stream processing framework—such as Storm, Samza, or Spark Streaming—that helps provide richer processing primitives. These frameworks are just gaining prominence now, but each integrates well with Apache Kafka.</p>
<p>Stream processing acts as both a way to develop applications that need low-latency transformations but it is also directly part of the data integration usage as well: integrating systems often requires some munging of data streams in between.</p>
<h2 id="What-Does-a-Stream-Data-Platform-Need-To-Do"><a href="#What-Does-a-Stream-Data-Platform-Need-To-Do" class="headerlink" title="What Does a Stream Data Platform Need To Do?"></a>What Does a Stream Data Platform Need To Do?</h2><p>I’ve discussed a number of different use cases. Each of these use cases has a corresponding event stream, but each stream has slightly different requirements—some need to be fast, some high-throughput, some need to scale out, etc. If we want to make a single platform that can handle all of these uses what will it need to do?</p>
<p><strong>I think the following are the key requirements for a stream data platform:</strong></p>
<blockquote>
<ul>
<li>It must be reliable enough to handle critical updates such as replicating the changelog of a database to a replica store like a search index, delivering this data in order and without loss.</li>
<li>It must support throughput high enough to handle large volume log or event data streams.</li>
<li>It must be able to buffer or persist data for long periods of time to support integration with batch systems such as Hadoop that may only perform their loads and processing periodically.</li>
<li>It must provide data with latency low enough for real-time applications.</li>
<li>It must be possible to operate it as a central system that can scale to carry the full load of the organization and operate with hundreds of applications built by disparate teams all plugged into the same central nervous system.</li>
<li>It has to support close integration with stream processing systems.</li>
</ul>
</blockquote>
<p>These requirements are necessary for this system to truly bring simplicity to data flow. The goal of the stream data platform is to sit at the heart of the company and manage these data streams. If the system cannot provide sufficient reliability guarantees or scale to large volume data then data will again end up fragmented over multiple systems. If the system cannot support both batch and real-time consumption, then again data will be fragmented. And if the system does not support operations at company-wide scale then silos will arise.</p>
<h2 id="What-is-Apache-Kafka"><a href="#What-is-Apache-Kafka" class="headerlink" title="What is Apache Kafka?"></a>What is Apache Kafka?</h2><p>Apache Kafka is a distributed system designed for streams. It is built to be fault-tolerant, high-throughput, horizontally scalable, and allows geographically distributing data streams and processing.</p>
<p>Kafka is often categorized as a messaging system, and it serves a similar role, but provides a fundamentally different abstraction. The key abstraction in Kafka is a structured commit log of updates:</p>
<p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870538-png/blog-files/commit_log-copy.png" alt="commit_log"></p>
<p>A producer of data sends a stream of records which are appended to this log, and any number of consumers can continually stream these updates off the tail of the log with millisecond latency. Each of these data consumers has its own position in the log and advances independently. This allows a reliable, ordered stream of updates to be distributed to each consumer.</p>
<p>The log can be sharded and spread over a cluster of machines, and each shard is replicated for fault-tolerance. This gives a model for parallel, ordered consumption which is key to Kafka’s use as a change capture system for database updates (which must be delivered in order).</p>
<p>Kafka is built as a modern distributed system. Data is replicated and partitioned over a cluster of machines that can grow and shrink transparently to the applications using the cluster. Consumers of data can be scaled out over a pool of machines as well and automatically adapt to failures in the consuming processes.</p>
<p>A key aspect of Kafka’s design is that it handles persistence well. A Kafka broker can store many TBs of data. This allows usage patterns that would be impossible in a traditional database:</p>
<ul>
<li>A Hadoop cluster or other offline system that is fed off Kafka can go down for maintenance and come back hours or days later confident that all changes have been safely persisted in the up-stream Kafka cluster.</li>
<li>When synchronizing from database tables it is possible to initialize a “full dump” of the database so that downstream consumers of data have access to the full data set.</li>
</ul>
<p>These features make Kafka applicable well beyond the uses of traditional enterprise messaging systems.</p>
<h2 id="Event-driven-Applications"><a href="#Event-driven-Applications" class="headerlink" title="Event-driven Applications"></a>Event-driven Applications</h2><p>Since we built Kafka as an open source project we have had the opportunity to work closely with companies who put it to use and to see the general pattern of Kafka adoption: how it first is adopted and how its role evolves over time in their architecture.</p>
<p>The initial adoption is usually for a single particularly large-scale use case: Log data, feeds into Hadoop, or other data streams beyond the capabilities of their existing messaging systems or infrastructure.</p>
<p>From there, though, the usage spreads. Though the initial use case may have been feeding a Hadoop cluster, once there is a continual feed of events available, the use cases for processing these events in real-time quickly emerge. Existing applications will end up tapping into the event streams to react to what is happening more intelligently, and new applications will be built to harness intelligence derived off these streams.</p>
<p>For example at LinkedIn we originally began capturing a stream of views to jobs displayed on the website as one of many feeds to deliver to Hadoop and our relational data warehouse. However this ETL-centric use case soon became one of many and the stream of job views over time began to be used by a variety of systems:</p>
<p><img src="http://cdn2.hubspot.net/hub/540072/file-3062870548-png/blog-files/job-view.png" alt="job-view"></p>
<p>Note that the application that showed jobs didn’t need any particular modification to integrate with these other uses. It just produced the stream of jobs that were viewed. The other applications tapped into this stream to add their own processing. Likewise when job views began happening in other applications—mobile applications—these are just added to the global feed of events, the downstream processors don’t need to integrate with new upstream sources.</p>
<h2 id="How-Does-a-Stream-Data-Platform-Relate-To-Existing-Things"><a href="#How-Does-a-Stream-Data-Platform-Relate-To-Existing-Things" class="headerlink" title="How Does a Stream Data Platform Relate To Existing Things"></a>How Does a Stream Data Platform Relate To Existing Things</h2><p>Let’s talk briefly about the relationship this stream data platform concept has with other things in the world.</p>
<h3 id="1-Messaging"><a href="#1-Messaging" class="headerlink" title="1. Messaging"></a>1. Messaging</h3><p>A stream data platform is similar to an enterprise messaging system—it receives messages and distributes them to interested subscribers. There are three important differences:</p>
<ul>
<li><p>Messaging systems are typically run in one-off deployments for different applications. The purpose of the stream data platform is very much as a central data hub.</p>
</li>
<li><p>Messaging systems do a poor job of supporting integration with batch systems, such as a data warehouse or a Hadoop cluster, as they have limited data storage capacity.</p>
</li>
<li><p>Messaging systems do not provide semantics that are easily compatible with rich stream processing.</p>
</li>
</ul>
<p>In other words a data stream data platform is a messaging system whose role has been rethought at a company-wide scale.</p>
<h3 id="2-Data-Integration-Tools"><a href="#2-Data-Integration-Tools" class="headerlink" title="2. Data Integration Tools"></a>2. Data Integration Tools</h3><p>A data stream data platform does a lot to make integration between systems easier. However its role is different from a tool like Informatica. A stream data platform is a true platform that any other system can choose to tap into and many applications can build around.</p>
<p>One practical area of overlap is that by making data available in a uniform format in a single place with a common stream abstraction, many of the routine data clean-up tasks can be avoided entirely. I’ll dive into this more in the second part of this article.</p>
<h3 id="3-Enterprise-Service-Buses"><a href="#3-Enterprise-Service-Buses" class="headerlink" title="3. Enterprise Service Buses"></a>3. Enterprise Service Buses</h3><p>I think a data stream data platform embodies many of the ideas of an enterprise service bus, but with better implementation. The challenges of Enterprise Service Bus adoption has been the coupling of transformations of data with the bus itself. Some of the challenges of Enterprise Service Bus adoption are that much of the logic required for transformation are baked into the message bus itself without a good model for multi-tenant cloud like deployment and operation of this logic.</p>
<p>The advantage of a stream data platform is that transformation is fundamentally decoupled from the stream itself. This code can live in applications or stream processing tasks, allowing teams to iterate at their own pace without a central bottleneck for application development.</p>
<h3 id="4-Change-Capture-Systems"><a href="#4-Change-Capture-Systems" class="headerlink" title="4. Change Capture Systems"></a>4. Change Capture Systems</h3><p>Databases have long had similar log mechanisms such as Golden Gate. However these mechanisms are limited to database changes only and are not a general purpose event capture platform. They tend to focus primarily on the replication between databases, often between instances of the same database system (e.g. Oracle-to-Oracle).</p>
<h3 id="5-Data-Warehouses-and-Hadoop"><a href="#5-Data-Warehouses-and-Hadoop" class="headerlink" title="5. Data Warehouses and Hadoop"></a>5. Data Warehouses and Hadoop</h3><p>A stream data platform doesn’t replace your data warehouse; in fact, quite the opposite: it feeds it data. It acts as a conduit for data to quickly flow into the warehouse environment for long-term retention, ad hoc analysis, and batch processing. That same pipeline can run in reverse to publish out derived results from nightly or hourly batch processing.</p>
<h3 id="6-Stream-Processing-Systems"><a href="#6-Stream-Processing-Systems" class="headerlink" title="6. Stream Processing Systems"></a>6. Stream Processing Systems</h3><p>Stream processing frameworks such as Storm, Samza, or Spark Streaming can be an excellent addition to the data stream data platform. They attempt to add richer processing semantics to subscribers and can make implementing data transformation easier.</p>
<p>Of course data transformation doesn’t require a specialized system. Normal application code can subscribe to streams, process them, and write back derived streams, just as one does in one of these fancier systems. However these frameworks can potentially make this kind of processing easier.</p>
<h2 id="What-Does-This-Look-Like-In-Practice"><a href="#What-Does-This-Look-Like-In-Practice" class="headerlink" title="What Does This Look Like In Practice?"></a>What Does This Look Like In Practice?</h2><p>One of the interesting things about this concept is that it isn’t just an idea, we have actually had the opportunity to “do the experiment”. We spent the last five years building Kafka and helping companies put streaming data to use. At a number of Silicon Valley companies today you can see this concept in action—everything from user activity to database changes to administrative actions like restarting a process are captured in real-time streams that are subscribed to and processed in real-time.</p>
<p>What is interesting about this is that what begins as simple plumbing quickly evolves into something much more. These data streams begin to act as a kind of central nervous system that applications organize themselves around.</p>
<h2 id="Next-Steps"><a href="#Next-Steps" class="headerlink" title="Next Steps"></a>Next Steps</h2><p>We think this technology is changing how data is put to use in companies. We are building the Confluent Platform, a set of tools aimed at helping companies adopt and use Apache Kafka in this way. We think the Confluent Platform represents the best place to get started if you are thinking about putting streaming data to use in your organization.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h3 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h3&gt;&lt;p&gt;I’m preparing to start a new journey, which I’ll annouce soon. The new opportunity is about data.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Apache Kafka&lt;/code&gt; has been a hot topic in the data field for a while, and, of course, I cannot taking on data problems without it. While learning and reading more about Kafka, I found &lt;a href=&quot;http://www.confluent.io/blog/&quot;&gt;Conluent’s official tech blog&lt;/a&gt; has been an amazingly useful place to find out materials I need - because the company is started by founders of Kafka in Linkedin.&lt;/p&gt;
&lt;p&gt;Thus, I decided to repost a few great blogs from Confluent, add my own notes, and publish them here.&lt;/p&gt;
&lt;p&gt;All posts are &lt;a href=&quot;https://phoenixjiangnan.github.io/categories/distributed-system/kafka/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;This post is Part I of a couple blogs from Concluent’s CEO Jay Kreps. In this blog, he discussed the reason he led to develop Kafka in Linkedin, the problems Kafka can solve, and Kafka’s role in modern enterprise data system.&lt;/p&gt;
&lt;p&gt;From my perspective, Part I is not as helpful as Part II w.r.t. technical details. But it certainly presents the fundamental background of how Kafka came to the world.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.confluent.io/blog/stream-data-platform-1/&quot;&gt;Original Post&lt;/a&gt; from Jay Kreps. February 25, 2015.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;These days you hear a lot about “stream processing”, “event data”, and “real-time”, often related to technologies like Kafka, Storm, Samza, or Spark’s Streaming module. Though there is a lot of excitement, not everyone knows how to fit these technologies into their technology stack or how to put it to use in practical applications.&lt;/p&gt;
&lt;p&gt;This guide is going to discuss our experience with real-time data streams: how to build a home for real-time data within your company, and how to build applications that make use of that data. All of this is based on real experience: we spent the last five years building Apache Kafka, transitioning LinkedIn to a fully stream-based architecture, and helping a number of Silicon Valley tech companies do the same thing.&lt;/p&gt;
&lt;p&gt;The first part of the guide will give a high-level overview of what we came to call a &lt;code&gt;stream data platform&lt;/code&gt;: a central hub for real-time streams of data. It will cover the what and why of this idea.&lt;/p&gt;
&lt;p&gt;The second part will dive into a lot of specifics and give advice on how to put this into practice effectively.&lt;/p&gt;
&lt;p&gt;But first, what is a stream data platform?&lt;/p&gt;
&lt;h2 id=&quot;The-Stream-Data-Platform-A-Clean-Well-lighted-Place-For-Events&quot;&gt;&lt;a href=&quot;#The-Stream-Data-Platform-A-Clean-Well-lighted-Place-For-Events&quot; class=&quot;headerlink&quot; title=&quot;The Stream Data Platform: A Clean, Well-lighted Place For Events&quot;&gt;&lt;/a&gt;The Stream Data Platform: A Clean, Well-lighted Place For Events&lt;/h2&gt;&lt;p&gt;We built Apache Kafka at LinkedIn with a specific purpose in mind: to serve as a central repository of data streams. But why do this? There were two motivations.&lt;/p&gt;
&lt;p&gt;The first problem was how to transport data between systems. We had lots of data systems: relational OLTP databases, Hadoop, Teradata, a search system, monitoring systems, OLAP stores, and derived key-value stores. Each of these needed reliable feeds of data in a geographically distributed environment. I’ll call this problem &lt;code&gt;data integration&lt;/code&gt;, though we could also call it &lt;code&gt;ETL&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The second part of this problem was the need to do richer analytical data processing—the kind of thing that would normally happen in a data warehouse or Hadoop cluster—but with very low latency. I call this “stream processing” though others might call it “messaging” or CEP or something similar.&lt;/p&gt;
    
    </summary>
    
      <category term="distributed system" scheme="https://phoenixjiangnan.github.io/categories/distributed-system/"/>
    
      <category term="kafka" scheme="https://phoenixjiangnan.github.io/categories/distributed-system/kafka/"/>
    
    
      <category term="kafka" scheme="https://phoenixjiangnan.github.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Spring Boot - Introduction to Spring Boot</title>
    <link href="https://phoenixjiangnan.github.io/2016/09/30/spring/spring%20boot/Spring-Boot-Introduction-to-Spring-Boot/"/>
    <id>https://phoenixjiangnan.github.io/2016/09/30/spring/spring boot/Spring-Boot-Introduction-to-Spring-Boot/</id>
    <published>2016-10-01T06:18:56.000Z</published>
    <updated>2016-09-30T15:35:58.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Spring-io-Spring-Boot"><a href="#Spring-io-Spring-Boot" class="headerlink" title="Spring.io - Spring Boot"></a>Spring.io - Spring Boot</h2><p>I’m taking some important parts from it and summarizing them as my learning notes.</p>
<p>Spring has been notorious for its heaviness when starting a project. Lots of people avoid this framework because of that. Many lighter dependency injection frameworks emerge as a replacement for Spring, <a href="https://github.com/google/guice" target="_blank" rel="external">Google’s Guice</a> being one of the most representative ones. </p>
<p>I’ve talked to quite a few folks who touched Spring a few years back and then switched to some better solutions because of the pain of managing the sophisticated configs. They hadn’t heard of Spring Boot, and were surprised about this project when I introduced it to them.</p>
<p>Here, let’s discuss some of the very basic yet fundamental aspects of Spring Boot.</p>
<hr>
<p>The following content is from the official guidance of <a href="http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#getting-started" target="_blank" rel="external">Spring Boot</a></p>
<hr>
<h2 id="Introducing-Spring-Boot"><a href="#Introducing-Spring-Boot" class="headerlink" title="Introducing Spring Boot"></a>Introducing Spring Boot</h2><p>Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can <code>just run</code>. We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. </p>
<blockquote>
<p>Most Spring Boot applications need very little Spring configuration.</p>
</blockquote>
<p>You can use Spring Boot to create Java applications that can be started using <code>java -jar</code> or more traditional war deployments. We also provide a command line tool that runs <code>spring scripts</code>.</p>
<p>Some featured characteristics are:</p>
<blockquote>
<ul>
<li>Provide a radically faster and widely accessible getting started experience for all Spring development.<ul>
<li>Create stand-alone Spring applications</li>
<li>Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files)</li>
<li>Provide opinionated <code>starter</code> POMs (Project Object Model) to simplify your Maven configuration</li>
<li>Automatically configure Spring whenever possible</li>
<li>Provide production-ready features such as metrics, health checks and externalized configuration</li>
<li>Absolutely <code>no code generation</code> and <code>no requirement for XML configuration</code></li>
</ul>
</li>
</ul>
</blockquote>
<p>Spring Boot offers four main features that will change the way you develop Spring applications:</p>
<blockquote>
<ul>
<li><code>Spring Boot starters</code> — Spring Boot starters aggregate common groupings of dependencies into single dependencies that can be added to a project’s Maven or Gradle build.</li>
<li><code>Autoconfiguration</code> — Spring Boot’s autoconfiguration feature leverages Spring 4’s support for conditional configuration to make reasonable guesses about the beans your application needs and automatically configure them.</li>
<li><code>Command-line interface (CLI)</code> — Spring Boot’s CLI takes advantage of the Groovy programming language along with autoconfiguration to further simplify Spring application development.</li>
<li><code>Actuator</code> — The Spring Boot Actuator adds certain management features to a Spring Boot application.</li>
</ul>
</blockquote>
<a id="more"></a>
<h3 id="1-Adding-starter-dependencies"><a href="#1-Adding-starter-dependencies" class="headerlink" title="1 Adding starter dependencies"></a>1 Adding starter dependencies</h3><p>There are two ways to bake a cake. The ambitious baker will mix flour, eggs, sugar, baking powder, salt, butter, vanilla, and milk into a batter. Or you can buy a prepackaged box of cake mix that includes most of the ingredients you’ll need and only mix in a few wet ingredients like water, eggs, and vegetable oil.</p>
<p>Much as a prepackaged cake mix aggregates many of the ingredients of a cake recipe into a single ingredient, Spring Boot starters aggregate the various dependencies of an application into a single dependency.</p>
<p>If you’re building your project with Gradle, you’ll need (at least) the following dependencies in <code>build.gradle</code>:</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">dependencies</span> &#123;</div><div class="line">  <span class="keyword">compile</span>(<span class="string">"org.springframework:spring-web:4.0.6.RELEASE"</span>)</div><div class="line">  <span class="keyword">compile</span>(<span class="string">"org.springframework:spring-webmvc:4.0.6.RELEASE"</span>)</div><div class="line">  <span class="keyword">compile</span>(<span class="string">"com.fasterxml.jackson.core:jackson-databind:2.2.2"</span>)</div><div class="line">  <span class="keyword">compile</span>(<span class="string">"org.springframework:spring-jdbc:4.0.6.RELEASE"</span>)</div><div class="line">  <span class="keyword">compile</span>(<span class="string">"org.springframework:spring-tx:4.0.6.RELEASE"</span>)</div><div class="line">  <span class="keyword">compile</span>(<span class="string">"com.h2database:h2:1.3.174"</span>)</div><div class="line">  <span class="keyword">compile</span>(<span class="string">"org.thymeleaf:thymeleaf-spring4:2.1.2.RELEASE"</span>)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Fortunately, Gradle makes it possible to express dependencies succinctly. Even so, a lot of work went into creating this list, and more will go into maintaining it. How can you know if these dependencies will play well together? As the application grows and evolves, dependency management will become even more challenging.</p>
<p>But if you’re using the prepackaged dependencies from Spring Boot starters, the Gradle dependency list can be a little shorter:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">dependencies &#123;</div><div class="line">  compile("org.springframework.boot:spring-boot-starter-web:</div><div class="line">           1.1.4.RELEASE")</div><div class="line">  compile("org.springframework.boot:spring-boot-starter-jdbc:</div><div class="line">           1.1.4.RELEASE")</div><div class="line">  compile("com.h2database:h2:1.3.174")</div><div class="line">  compile("org.thymeleaf:thymeleaf-spring4:2.1.2.RELEASE")</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>As you can see, Spring Boot’s web and JDBC starters replaced several of the finergrained dependencies. You still need to include the H2 and Thymeleaf dependencies, but the other dependencies are rolled up into the starter dependencies. Aside from making the dependency list shorter, you can feel confident that the versions of dependencies provided by the starters are compatible with each other.</p>
<p>E.g.</p>
<table>
<thead>
<tr>
<th>Starter</th>
<th>Provides</th>
</tr>
</thead>
<tbody>
<tr>
<td>spring-boot-starter-ac tuator</td>
<td>spring-boot-starter, spring-boot- actuator, spring-core</td>
</tr>
<tr>
<td></td>
</tr>
<tr>
<td>spring-boot-starter-amqp</td>
<td>spring-boot-starter, spring-boot-rabbit, spring-core, spring-tx</td>
</tr>
<tr>
<td>spring-boot-starter-aop</td>
<td>spring-boot-starter, spring-aop, AspectJ Runtime, AspectJ Weaver, spring-core</td>
</tr>
</tbody>
</table>
<p>Taking advantage of Maven’s and Gradle’s transitive dependency resolution, the starters declare several dependencies in their own pom.xml file. When you add one of these starter dependencies to your Maven or Gradle build, the starter’s dependencies are resolved transitively.</p>
<h3 id="2-Autoconfiguration"><a href="#2-Autoconfiguration" class="headerlink" title="2 Autoconfiguration"></a>2 Autoconfiguration</h3><p>Whereas <code>Spring Boot starters cut down the size of your build&#39;s dependency list</code>, <code>Spring Boot autoconfiguration cuts down on the amount of Spring configuration</code>. It does this by considering other factors in your application and making assumptions about what Spring configuration you’ll need.</p>
<p>Spring Boot starters can trigger autoconfiguration. For instance, all you need to do to use Spring MVC in your Spring Boot application is to add <code>the web starter</code> as a dependency in the build. When you add the web starter to your project’s build, it will transitively pull in Spring MVC dependencies. When Spring Boot’s web autoconfiguration detects Spring MVC in the classpath, it will automatically configure several beans to support Spring MVC, including view resolvers, resource handlers, and message converters (among others). All that’s left for you to do is write the controller classes to handle the requests.</p>
<h3 id="3-The-Spring-Boot-CLI"><a href="#3-The-Spring-Boot-CLI" class="headerlink" title="3 The Spring Boot CLI"></a>3 The Spring Boot CLI</h3><p>The Spring Boot CLI takes the magic provided by Spring Boot <code>starters</code> and <code>autoconfiguration</code> and spices it up a little with Groovy. It reduces the Spring development process to the point where you can run one or more Groovy scripts through a CLI and see it run. In the course of running the application, the CLI will also automatically import Spring types and resolve dependencies.</p>
<p>One of the most interesting examples used to illustrate Spring Boot CLI is contained in the following Groovy script:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@RestController</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Hi</span> </span>&#123;</div><div class="line">    <span class="meta">@RequestMapping</span>(<span class="string">"/"</span>)</div><div class="line">    <span class="function">String <span class="title">hi</span><span class="params">()</span> </span>&#123;</div><div class="line">        <span class="string">"Hi!"</span> </div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Believe it or not, that is a complete (albeit simple) Spring application that can be executed through the Spring Boot CLI. Including whitespace, it’s 82 characters in length. You can paste it into your Twitter client and tweet it to your friends.</p>
<p>Eliminate the unnecessary whitespace and you get this 64-character one-liner:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@RestController</span> <span class="class"><span class="keyword">class</span> <span class="title">Hi</span></span>&#123;<span class="meta">@RequestMapping</span>(<span class="string">"/"</span>)<span class="function">String <span class="title">hi</span><span class="params">()</span></span>&#123;<span class="string">"Hi!"</span>&#125;&#125;</div></pre></td></tr></table></figure>
<p>This version is so brief that you can paste it twice into a single tweet on Twitter. But it’s still a complete and runnable (if feature-poor) Spring application. If you have the Spring Boot CLI installed, you can run it with the following command line:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ spring run Hi.groovy</div></pre></td></tr></table></figure>
<h3 id="4-The-Actuator"><a href="#4-The-Actuator" class="headerlink" title="4 The Actuator"></a>4 The Actuator</h3><p>The Spring Boot Actuator brings a handful of useful features to a Spring Boot project, including:</p>
<blockquote>
<ul>
<li>Management endpoint</li>
<li>Sensible error handling and a default mapping for an <code>/error</code> endpoint</li>
<li>An <code>/info</code> endpoint that can communicate information about an application</li>
<li>An audit events framework when Spring Security is in play</li>
</ul>
</blockquote>
<p>All of these features are useful, but the management endpoints are the most immediately useful and interesting features of the Actuator.</p>
<p>Spring Boot includes a number of additional features to help you monitor and manage your application when it’s pushed to production. You can choose to manage and monitor your application using HTTP endpoints, with JMX or even by remote shell (SSH or Telnet). Auditing, health and metrics gathering can be automatically applied to your application.</p>
<p>Actuator HTTP endpoints are only available with a Spring MVC-based application. In particular, it will not work with Jersey unless you enable Spring MVC as well.</p>
<table>
<thead>
<tr>
<th>ID</th>
<th style="text-align:center">Description</th>
<th style="text-align:right">Sensitive Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>actuator</td>
<td style="text-align:center">Provides a hypermedia-based “discovery page” for the other endpoints. Requires Spring HATEOAS to be on the classpath.</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>autoconfig</td>
<td style="text-align:center">Displays an auto-configuration report showing all auto-configuration candidates and the reason why they ‘were’ or ‘were not’ applied.</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>beans</td>
<td style="text-align:center">Displays a complete list of all the Spring beans in your application.</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>configprops</td>
<td style="text-align:center">Displays a collated list of all @ConfigurationProperties.</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>dump</td>
<td style="text-align:center">Performs a thread dump.</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>env</td>
<td style="text-align:center">Exposes properties from Spring’s ConfigurableEnvironment.</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>flyway</td>
<td style="text-align:center">Shows any Flyway database migrations that have been applied.</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>health</td>
<td style="text-align:center">Shows application health information (when the application is secure, a simple ‘status’ when accessed over an unauthenticated connection or full message details when authenticated).</td>
<td style="text-align:right">false</td>
</tr>
<tr>
<td>info</td>
<td style="text-align:center">Displays arbitrary application info.</td>
<td style="text-align:right">false</td>
</tr>
<tr>
<td>liquibase</td>
<td style="text-align:center">Shows any Liquibase database migrations that have been applied.</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>metrics</td>
<td style="text-align:center">Shows ‘metrics’ information for the current application.</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>mappings</td>
<td style="text-align:center">Displays a collated list of all @RequestMapping paths.</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>shutdown</td>
<td style="text-align:center">Allows the application to be gracefully shutdown (not enabled by default).</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>trace</td>
<td style="text-align:center">Displays trace information (by default the last 100 HTTP requests).</td>
<td style="text-align:right">true</td>
</tr>
</tbody>
</table>
<p>If you are using Spring MVC, the following additional endpoints can also be used:</p>
<table>
<thead>
<tr>
<th>ID</th>
<th style="text-align:center">Description</th>
<th style="text-align:right">Sensitive Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>docs</td>
<td style="text-align:center">Displays documentation, including example requests and responses, for the Actuator’s endpoints. Requires spring-boot-actuator-docs to be on the classpath.</td>
<td style="text-align:right">false</td>
</tr>
<tr>
<td>heapdump</td>
<td style="text-align:center">Returns a GZip compressed hprof heap dump file.</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>jolokia</td>
<td style="text-align:center">Exposes JMX beans over HTTP (when Jolokia is on the classpath).</td>
<td style="text-align:right">true</td>
</tr>
<tr>
<td>logfile</td>
<td style="text-align:center">Returns the contents of the logfile (if logging.file or logging.path properties have been set). Supports the use of the HTTP Range header to retrieve part of the log file’s content.</td>
<td style="text-align:right">true</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Depending on how an endpoint is exposed, the sensitive property may be used as a security hint. For example, sensitive endpoints will require a username/password when they are accessed over HTTP (or simply disabled if web security is not enabled).</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Spring-io-Spring-Boot&quot;&gt;&lt;a href=&quot;#Spring-io-Spring-Boot&quot; class=&quot;headerlink&quot; title=&quot;Spring.io - Spring Boot&quot;&gt;&lt;/a&gt;Spring.io - Spring Boot&lt;/h2&gt;&lt;p&gt;I’m taking some important parts from it and summarizing them as my learning notes.&lt;/p&gt;
&lt;p&gt;Spring has been notorious for its heaviness when starting a project. Lots of people avoid this framework because of that. Many lighter dependency injection frameworks emerge as a replacement for Spring, &lt;a href=&quot;https://github.com/google/guice&quot;&gt;Google’s Guice&lt;/a&gt; being one of the most representative ones. &lt;/p&gt;
&lt;p&gt;I’ve talked to quite a few folks who touched Spring a few years back and then switched to some better solutions because of the pain of managing the sophisticated configs. They hadn’t heard of Spring Boot, and were surprised about this project when I introduced it to them.&lt;/p&gt;
&lt;p&gt;Here, let’s discuss some of the very basic yet fundamental aspects of Spring Boot.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The following content is from the official guidance of &lt;a href=&quot;http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#getting-started&quot;&gt;Spring Boot&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Introducing-Spring-Boot&quot;&gt;&lt;a href=&quot;#Introducing-Spring-Boot&quot; class=&quot;headerlink&quot; title=&quot;Introducing Spring Boot&quot;&gt;&lt;/a&gt;Introducing Spring Boot&lt;/h2&gt;&lt;p&gt;Spring Boot makes it easy to create stand-alone, production-grade Spring based Applications that you can &lt;code&gt;just run&lt;/code&gt;. We take an opinionated view of the Spring platform and third-party libraries so you can get started with minimum fuss. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Most Spring Boot applications need very little Spring configuration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can use Spring Boot to create Java applications that can be started using &lt;code&gt;java -jar&lt;/code&gt; or more traditional war deployments. We also provide a command line tool that runs &lt;code&gt;spring scripts&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Some featured characteristics are:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Provide a radically faster and widely accessible getting started experience for all Spring development.&lt;ul&gt;
&lt;li&gt;Create stand-alone Spring applications&lt;/li&gt;
&lt;li&gt;Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files)&lt;/li&gt;
&lt;li&gt;Provide opinionated &lt;code&gt;starter&lt;/code&gt; POMs (Project Object Model) to simplify your Maven configuration&lt;/li&gt;
&lt;li&gt;Automatically configure Spring whenever possible&lt;/li&gt;
&lt;li&gt;Provide production-ready features such as metrics, health checks and externalized configuration&lt;/li&gt;
&lt;li&gt;Absolutely &lt;code&gt;no code generation&lt;/code&gt; and &lt;code&gt;no requirement for XML configuration&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Spring Boot offers four main features that will change the way you develop Spring applications:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Spring Boot starters&lt;/code&gt; — Spring Boot starters aggregate common groupings of dependencies into single dependencies that can be added to a project’s Maven or Gradle build.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Autoconfiguration&lt;/code&gt; — Spring Boot’s autoconfiguration feature leverages Spring 4’s support for conditional configuration to make reasonable guesses about the beans your application needs and automatically configure them.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Command-line interface (CLI)&lt;/code&gt; — Spring Boot’s CLI takes advantage of the Groovy programming language along with autoconfiguration to further simplify Spring application development.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Actuator&lt;/code&gt; — The Spring Boot Actuator adds certain management features to a Spring Boot application.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="spring" scheme="https://phoenixjiangnan.github.io/categories/spring/"/>
    
      <category term="spring boot" scheme="https://phoenixjiangnan.github.io/categories/spring/spring-boot/"/>
    
    
      <category term="spring boot" scheme="https://phoenixjiangnan.github.io/tags/spring-boot/"/>
    
  </entry>
  
  <entry>
    <title>Hexo - How to backup configs and blogs of Github Pages in a Github repository</title>
    <link href="https://phoenixjiangnan.github.io/2016/09/29/hexo/Hexo-How-to-backup-configs-and-blogs-of-Github-Pages-in-a-Github-repository/"/>
    <id>https://phoenixjiangnan.github.io/2016/09/29/hexo/Hexo-How-to-backup-configs-and-blogs-of-Github-Pages-in-a-Github-repository/</id>
    <published>2016-09-30T00:50:02.000Z</published>
    <updated>2016-09-29T10:00:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>Github Pages is great - it allows you to set up static website on it and use it as a blog.</p>
<p>I’ve been using <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a> as the blogging platform and static website generator, and <a href="http://theme-next.iissnan.com/" target="_blank" rel="external">NexT</a> as its theme. One thing I really need when using Hexo is to back up all my blogs which are in markdown format, as well as Hexo configuration file, a bunch website configuration files, and the theme configuration file. The reason being that 1) self-managed blog generator is very easy to be messed up, and 2) I need to re-setup everything when switching computers, both cases requiring reconfigure Hexo and restore all contents and configurations.</p>
<p>So wouldn’t it be nice that we can use your <code>&lt;username&gt;.github.io</code> repository for Github Pages, and another repository, say <code>Hexo</code>, for all your blogs and configurations?</p>
<p>It’s a bit tricky, and I’m gonna show you how to do it.</p>
<h3 id="Backgrounds-What-files-should-be-backed-up"><a href="#Backgrounds-What-files-should-be-backed-up" class="headerlink" title="Backgrounds - What files should be backed up"></a>Backgrounds - What files should be backed up</h3><p>Let’s say Hexo lives in a dir called <code>Hexo</code>, and your theme is NexT, the file layout looks like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">- Hexo</div><div class="line">    - _config.yml (Hexo config file)</div><div class="line">    - node_modules (Node.js code of Hexo and all plugins)</div><div class="line">    - public (generated static website data)</div><div class="line">    - scaffolds (template files)</div><div class="line">    - source</div><div class="line">        - _posts (all your blogs)</div><div class="line">        -      (other top-level sites you want to add, like `about`, `categories`, and `tags`)</div><div class="line">    - themes (Hexo themes)</div><div class="line">        - landscape (Hexo default theme)</div><div class="line">        - next (NexT theme)</div><div class="line">            - _config.yml (NexT&apos;s config file)</div></pre></td></tr></table></figure>
<h3 id="1-Init-Hexo-in-a-temp-folder"><a href="#1-Init-Hexo-in-a-temp-folder" class="headerlink" title="1. Init Hexo in a temp folder"></a>1. Init Hexo in a temp folder</h3><p>Let’s init Hexo in a temp folder called <code>hexocopy</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">mkdir hexocopy</div><div class="line">cd hexocopy</div><div class="line">hexo init</div></pre></td></tr></table></figure>
<h3 id="2-Init-your-official-Hexo-repository"><a href="#2-Init-your-official-Hexo-repository" class="headerlink" title="2. Init your official Hexo repository"></a>2. Init your official Hexo repository</h3><ul>
<li>Create a repository called <code>Hexo</code></li>
<li>Clone that repository to your local machine by either calling <code>git clone</code> or using Github Desktop App<br>  e.g.   <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/phoenixjiangnan/Hexo</div></pre></td></tr></table></figure>
</li>
</ul>
<a id="more"></a>
<h3 id="3-Copy-files"><a href="#3-Copy-files" class="headerlink" title="3. Copy files"></a>3. Copy files</h3><p>Copy <code>_config.yml</code>, <code>node_modules</code>, <code>scaffolds</code>, and <code>themes</code> folders from <code>hexocopy</code> to <code>Hexo</code>.</p>
<p>The reason for not initing Hexo itself from dir <code>Hexo</code> is that, command <code>init Hexo</code> will clone bits from a git repository and it will overwrite <code>.git</code> of the <code>Hexo</code> repository.</p>
<h3 id="4-Continue"><a href="#4-Continue" class="headerlink" title="4. Continue"></a>4. Continue</h3><p>Then, refer to <a href="https://phoenixjiangnan.github.io/2016/01/23/hexo/Hexo-How-to-install-hexo-on-Mac-with-github-pages/">this post</a> to continue setting up plugins, themes, and configs.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Github Pages is great - it allows you to set up static website on it and use it as a blog.&lt;/p&gt;
&lt;p&gt;I’ve been using &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt; as the blogging platform and static website generator, and &lt;a href=&quot;http://theme-next.iissnan.com/&quot;&gt;NexT&lt;/a&gt; as its theme. One thing I really need when using Hexo is to back up all my blogs which are in markdown format, as well as Hexo configuration file, a bunch website configuration files, and the theme configuration file. The reason being that 1) self-managed blog generator is very easy to be messed up, and 2) I need to re-setup everything when switching computers, both cases requiring reconfigure Hexo and restore all contents and configurations.&lt;/p&gt;
&lt;p&gt;So wouldn’t it be nice that we can use your &lt;code&gt;&amp;lt;username&amp;gt;.github.io&lt;/code&gt; repository for Github Pages, and another repository, say &lt;code&gt;Hexo&lt;/code&gt;, for all your blogs and configurations?&lt;/p&gt;
&lt;p&gt;It’s a bit tricky, and I’m gonna show you how to do it.&lt;/p&gt;
&lt;h3 id=&quot;Backgrounds-What-files-should-be-backed-up&quot;&gt;&lt;a href=&quot;#Backgrounds-What-files-should-be-backed-up&quot; class=&quot;headerlink&quot; title=&quot;Backgrounds - What files should be backed up&quot;&gt;&lt;/a&gt;Backgrounds - What files should be backed up&lt;/h3&gt;&lt;p&gt;Let’s say Hexo lives in a dir called &lt;code&gt;Hexo&lt;/code&gt;, and your theme is NexT, the file layout looks like this:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;- Hexo&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    - _config.yml (Hexo config file)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    - node_modules (Node.js code of Hexo and all plugins)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    - public (generated static website data)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    - scaffolds (template files)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    - source&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - _posts (all your blogs)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        -      (other top-level sites you want to add, like `about`, `categories`, and `tags`)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    - themes (Hexo themes)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - landscape (Hexo default theme)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        - next (NexT theme)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            - _config.yml (NexT&amp;apos;s config file)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;1-Init-Hexo-in-a-temp-folder&quot;&gt;&lt;a href=&quot;#1-Init-Hexo-in-a-temp-folder&quot; class=&quot;headerlink&quot; title=&quot;1. Init Hexo in a temp folder&quot;&gt;&lt;/a&gt;1. Init Hexo in a temp folder&lt;/h3&gt;&lt;p&gt;Let’s init Hexo in a temp folder called &lt;code&gt;hexocopy&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;mkdir hexocopy&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;cd hexocopy&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;hexo init&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;2-Init-your-official-Hexo-repository&quot;&gt;&lt;a href=&quot;#2-Init-your-official-Hexo-repository&quot; class=&quot;headerlink&quot; title=&quot;2. Init your official Hexo repository&quot;&gt;&lt;/a&gt;2. Init your official Hexo repository&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Create a repository called &lt;code&gt;Hexo&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Clone that repository to your local machine by either calling &lt;code&gt;git clone&lt;/code&gt; or using Github Desktop App&lt;br&gt;  e.g.   &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;git clone https://github.com/phoenixjiangnan/Hexo&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="hexo" scheme="https://phoenixjiangnan.github.io/categories/hexo/"/>
    
    
  </entry>
  
  <entry>
    <title>Repost - &quot;Farewell, App Academy. Hello, Airbnb. (Part II)&quot;</title>
    <link href="https://phoenixjiangnan.github.io/2016/09/21/career%20and%20leadership/Repost-Farewell-App-Academy-Hello-Airbnb-Part-II/"/>
    <id>https://phoenixjiangnan.github.io/2016/09/21/career and leadership/Repost-Farewell-App-Academy-Hello-Airbnb-Part-II/</id>
    <published>2016-09-22T06:10:23.000Z</published>
    <updated>2016-09-29T09:36:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>I think this is a great great blog talking about, as a SDE, how to negotiate your salary package with tech companies.</p>
<hr>
<p>The original post is <a href="http://haseebq.com/farewell-app-academy-hello-airbnb-part-ii/" target="_blank" rel="external">here</a></p>
<hr>
<h2 id="Farewell-App-Academy-Hello-Airbnb-Part-II"><a href="#Farewell-App-Academy-Hello-Airbnb-Part-II" class="headerlink" title="Farewell, App Academy. Hello, Airbnb. (Part II)"></a>Farewell, App Academy. Hello, Airbnb. (Part II)</h2><p>April 14, 2016<br>airbnb<br>(Note: this is the second part of this story. You can read the first part here.)</p>
<p>Despite the rejection from 23AndMe, I kept pushing.</p>
<p>I applied to the all the big hiring websites. Hired rejected me from their platform. I got no bites anywhere on AngelList or LinkedIn—not even cold e-mails from recruiters. Nothing from WhiteTruffle or SmartHires.</p>
<p>Not a breath of interest anywhere.</p>
<p>I kept on. I asked friends, students, anyone I knew for referrals. I started reaching out to non-engineers. I asked anyone at all who worked at all at a tech company I found compelling.</p>
<p>I was nervous. Was I a fraud? At App Academy I might have seemed like a paragon, but to the rest of the world… was I simply not good enough?</p>
<p>A Swing of the Pendulum</p>
<p>In the middle of this flurry, I created an account on TripleByte. TripleByte is a young YC startup that’s trying to change tech hiring. From their manifesto:</p>
<p>“Credentials should not be used as a proxy for talent. Education and work history are meaningful but relying solely on them results in missing good programmers. Good programmers come from all types of background. It’s what you can do that matters, not where you went to school.”</p>
<p>Their interview process is completely blind to credentials. They decide to work with you purely on the basis of a technical assessment, then they learn about your credentials and then introduce you to startups they determine are a good match. Though they mostly work with experienced software engineers, I decided to give them a try.</p>
<p>To my surprise, I was able to pass their anonymous programming quiz and I was automatically invited to an onsite assessment. I showed up sheepishly to their office on King Street, and over an intense three hours I was grilled by my interviewer on coding, data structures, algorithms, and system design. At the end of the interview, he asked me what my computer science background was. None, I told him. I’d learned this stuff on my own and from teaching at App Academy. He was taken aback, and told me I was incredibly strong for only having studied this material less than a year.</p>
<p>The next day, TripleByte called me. They wanted to work with me! They’d introduce me directly to startups in the YC network. This was exciting—there were a ton of companies in YC network I wanted to talk to: Stripe, Twitch, Airbnb, to name a few.</p>
<p>Well, as it turned out, those companies weren’t really looking for someone like me. Since TripleByte is a two-sided process, they matched me up with companies for which there was a good mutual fit—Stripe, Twitch, or Airbnb were nowhere on that list. However, one of the companies they matched me with was Gusto, formerly Zenpayroll (and now a timely Zenefits competitor). I’d previously had my eye on them, so I was excited to see them matched up with me. They also lined up an onsite at a small YC startup called Flexport.</p>
<p>TripleByte had gotten things moving. But even more importantly, they had given me back some confidence. TripleByte wanting to work with me was a small affirmation, to be sure—it wasn’t even a job offer yet. But it felt like I’d finally pierced the membrane. If these guys thought I was good… I must have it. I just gotta prove that to everyone else now.</p>
<a id="more"></a>
<p>Onsites and Offers</p>
<p>I set off to the onsites. The first of the two, Flexport, was a freight company. In many ways, it seemed more like a logistics company than it was about technology company. But I felt like I performed well in the onsite.</p>
<p>A couple days later: no offer.</p>
<p>I was supposed to be upset, but by now this was an old game. I brushed it off. Each interview I could feel was making me better at interviewing. It was getting easier and easier to relax, to ask for what I needed, to make jokes, to grill my interviewers. I was getting into my rhythm now.</p>
<p>Rejected? It is what it is. Just keep putting in the practice, I told myself.</p>
<p>Next was Gusto. They were a groovy company (no shoes in the office!), and the interview was a delight. They told me they’d get back to me within a week.</p>
<p>Shortly afterward, an onsite from Yelp pulled through (my referral was through a non-engineer!). I went onsite and completely rocked it. Every interviewer was clearly impressed, and they immediately asked me for my references afterward. Perhaps things were looking up! It seemed like one of Gusto or Yelp might actually convert into an offer.</p>
<p>All the while, I kept putting more irons in the grill (gotta keep that kitchen running), and kept studying up on algorithms and system design. Around then, I got an e-mail in my inbox from TripleByte:</p>
<p>Saw you’re starting to get some interviews booked with companies!</p>
<p>I actually wanted to throw another company option into the mix, us :) We’re hiring to build out our engineering team, we really enjoyed meeting you and would love to talk more about that if its of interest. Just let me know and I can give you a call tomorrow!</p>
<p>Shit. Really?</p>
<p>I hadn’t considered joining a super small company. I’d already spent the last year at a 25 person startup; TripleByte was only 5 people. But I knew entrepreneurship was somewhere in my future, and eventually I’d be joining a super early stage startup (if not founding it myself). TripleByte was a fantastic company, already with huge traction in the tech hiring space. Their founder, Harj Taggar, was a former YC partner, and one of the most charismatic and capable people I’ve met.</p>
<p>They wanted me. A company actually wanted me! A good one! I laugh now, but it was a revelation to me. They invited me in for another onsite so the rest of the team could vet me.</p>
<p>Before I could savor it too long, I received a phone call soon after. Yelp recruiter. Yelp wanted to make an offer.</p>
<p>And with that, the first domino fell.</p>
<p>Then came the Gusto recruiter. Gusto wanted to make an offer.</p>
<p>A TripleByte offer came soon in tow. First I had nothing, now I had three offers in front of me.</p>
<p>And yet, it was a little bittersweet. Yelp’s offer was 105K salary with ~17K/yr in equity. Gusto hit around 115K with comparable equity in options, and TripleByte was in the same ballpark.</p>
<p>Now, if you’re outside tech (or the Bay Area), those might sound like knock-down amazing offers. But all of these offers were lower than I was currently making, and they were all for explicitly junior roles.</p>
<p>Still, 105K might sound like a lot of money to be miffed about. But in San Francisco, with the second highest cost of living in the US (behind only Manhattan), the purchasing power of $105K in SF is the same as making about $57K in Austin, Texas. I also still owed App Academy tuition (18% of my first year’s salary), which was deferred because of my employment by App Academy. That 18% would come due on whatever offer I accepted. And of course on top of that, I’d also be donating a third of my income.</p>
<p>But really, what stuck out most to me is that I’d be leaving App Academy to a downgrade. I would be no better off than if I’d just joined those companies a year before. My goal, somewhere in my head, was to make at least more than I was making now. An arbitrary benchmark in a lot of ways, but it was hard to give up.</p>
<p>Was what I’d learned over the last year working at App Academy not worth anything on the market?</p>
<p>Maybe not. Perhaps I just had to accept it.</p>
<p>Now that I had offers in hand, it was time to turn the crank. I reached out to every company I was talking to and told them I’d just received several offers, but was very much interested in moving forward. With that, suddenly recruiters started tripping over themselves to get me on site. I was no longer the ugly boy at the party.</p>
<p>I started mowing down onsites. My performance and experience were no different, yet I was treated completely differently. Phone screen from Google. Gusto raised their offer. Phone screen from Stripe. Yelp raised their offer. TripleByte raised their offer. Then the phone screen at Google converted to onsite.</p>
<p>Bam. Suddenly Google was in sight. I felt like Captain Ahab: for me, Google was the Big One.</p>
<p>My Google recruiter worked for the Youtube team (headquartered in San Bruno). That meant if I were to receive an offer, the Youtube team would have first dibs on me. The interview process is standardized Google-wide though, and Google would ultimately be my employer. I spent the weekend before practicing and took the day off from work so I could head down to San Bruno for the interview.</p>
<p>I arrived that morning at the San Bruno BART stop, hardly having gotten any sleep the night before, and walked 30 minutes to the Youtube campus in downtown San Bruno. It was still early, so I sat in the lobby and read over my notes from Cracking the Coding Interview for some thirty minutes, trying to keep my cool. Finally, the recruiter came out and met me and brought me to the interview room.</p>
<p>Unequivocally, Google was the toughest and most nerve-wracking interview I’ve ever done. The problems were complex and challenging, all abstract and extremely algorithms-heavy. They didn’t ask me a single thing about architecture, systems design, web development—all they cared was that I could solve hard abstract computer science problems. All the interviewers—clearly brilliant (and all older white men)—were stony and tight-lipped about my performance.</p>
<p>I walked back to the BART station that evening, exhausted and completely drained, without any idea whether I’d done well or bombed.</p>
<p>A week later, I got a phone call from the Google recruiter.</p>
<p>I was in. Google wanted me.</p>
<p>Their offer was reasonably strong—120K salary with a 15% guaranteed end-of-year bonus and 24K/yr in RSUs. The total package came out to 162K/yr annualized—much stronger than my other offers, but around the ballpark of what fresh graduates of App Academy who landed at Google would earn. In other words, it seemed they didn’t place me very high on their spectrum. But whatever, right? It was Google! I got Google!</p>
<p>With that, the floodgates opened. Just the whiff of the Google name got recruiters into a frenzy. Companies that wouldn’t even look at me now bent over backwards to expedite me through their funnels.</p>
<p>Stripe onsite. Uber phone screen. Twitch phone screen. Uber onsite. Stripe offer. Twitch onsite. Uber offer. Twitch offer. The offers came in, stronger and stronger. All the irons I’d put in the fire were now going off like Roman candles.</p>
<p>Which left me with the question of: well, shit, what do I do with them?</p>
<p>Deciding Factors</p>
<p>As the offers came in, I weighed them seriously. Every single company I talked to was compelling and had aspects that made me want to work there.</p>
<p>But in the end, it was hard to argue against Google being the best choice.</p>
<p>The first and most visceral reason was I’d never have to deal with this BS again. Google would be a golden mark on my resume—like the Harvard of software engineering. Never again would I have to worry about credentials, or people tossing my resume without reading it. Google is the one name I’d be able to brandish anywhere as undeniable proof of my legitimacy as an engineer.</p>
<p>Part of it had to do with my ego, no doubt. But this job search had shown me that part of it was very real and material.</p>
<p>Of course, then there was the compensation. Google would be hard to beat. Like Yelp, Google was a mature public company, so its equity compensation was as solid as cash. That liquidity is an important factor when it comes to earning-to-give—it means I can give sooner, which has compounding positive impact and flow-through effects.</p>
<p>On top of that, Google was the only company to do a donation matching program, up to $6,000. That’s an extra $6,000 a year that goes to charity that otherwise wouldn’t. When it came to earning-to-give and building my career, when I looked at the numbers, it seemed clear I couldn’t justify choosing any company other than Google.</p>
<p>At that point, it seemed like a done deal. Companies like Uber, Twitch, Stripe, TripleByte, were all awesome companies. But it had to be Google.</p>
<p>Google! I was on cloud nine. I was ready for everything to wrap up as I figured out which bus I would be taking to San Bruno, planned out my morning workout regimen, and all the rest.</p>
<p>But the irons in the fire were still shooting off sparks. Offers improved, sometimes without any prompting on my part. They became increasingly stronger and stronger. Soon, they started overtaking Google’s.</p>
<p>I hadn’t negotiated with Google at all, so I knew the Google offer would move upward. I expected Google to settle somewhere around 180K all-in, which would be a healthy jump upward. But I started to really think: what would it take for me to turn down Google?</p>
<p>It was a tough question. Because I was in a competitive situation, Google had allowed me to start chatting with Youtube hiring managers and essentially pick my team.</p>
<p>I was offered a spot on the Youtube Red team, doing back-end work in C++. A small and fast-moving team in one of Youtube’s shiniest new projects. It would be tough, certainly, to learn the technology and start becoming productive. But I’d be doing world-class engineering, and have a world-class company on my resume.</p>
<p>And yet, I couldn’t deny: there was something energetically captivating about each and every startup I talked to. There was something deeper about them that attracted me, something was missing from Google. Not just the growth and the responsibility, or the greater self-determination. Each of these companies were innovators, creating new value in an uncertain future, and—they needed my help.</p>
<p>They actually needed my help. That was captivating. And it was hard to ignore that I didn’t really feel that in my interactions with Google.</p>
<p>But Google continually assured me that they’d be able to beat any other startup’s offers. It seemed like a decision they were happy to make for me. As other companies kept bidding up, the Google offer moved up to 185K. And as I was moving into final negotiations, they assured me there was still more room for the offer to grow.</p>
<p>More room to grow? How bad did Google actually want me?</p>
<p>Enter Airbnb</p>
<p>It’s funny. I had initially been rejected by an Airbnb recruiter for a different position more than a month before. Airbnb wasn’t even on my radar. So when Ned, the CTO of App Academy told me he’d put me in touch with his friend David at Airbnb, I mostly brushed it off.</p>
<p>Upon Ned’s glowing recommendation, David met with me and agreed to refer me into Airbnb. With his referral, I was promptly un-rejected. David was a senior engineer there, and his recommendation seemed to carry a lot of clout (the Google offer helped of course), so I was quickly shuffled through to the phone screen.</p>
<p>I wasn’t expecting to do too well—I’d read in many places that Airbnb had an extremely challenging interview, and that they were more credentials-focused than other companies. So much so that I’d never heard of a bootcamp student getting placed at Airbnb. Apparently they used to post on their job listings: “no bootcampers, please.”</p>
<p>On the phone interview, I received a pretty challenging problem—I remember thinking when I got it that I was going to mess it up. But somehow I solved it with ease, and all of my outputs were correct. I’m not sure whether to attribute it to the accruing notches on my belt or to blind luck, but I aced the phone screen and they eagerly brought me on-site.</p>
<p>I’ve always had a lot of respect for Airbnb as a company—I was an early Airbnb user myself—but even as I was walking into their large and beautiful campus, my mind was already made up. I was joining Google.</p>
<p>The Airbnb interview was long and challenging. But, unlike the Google interview, it was invigorating. I met with payments engineers, infrastructure engineers, even a political analyst who worked on European Airbnb regulation. I was challenged on my understanding of algorithms, of caching layers and database indices, I was questioned on my beliefs about the human right of free movement across borders and on the future of travel.</p>
<p>I loved it. I loved everyone I met. Even more, I loved the spirit of Airbnb. Everything about it—the beautiful cosmopolitan office built as an homage to Airbnbs around the world, the people, the technology, the raw energy and drive toward connecting the world more tightly together.</p>
<p>Airbnb’s mission is simply, “Belong Anywhere.” I’ve been both an Airbnb guest and a host for years, and before that I was an avid Couchsurfer and Couchsurfing host. I believe strongly in the culture of sharing and interconnection.</p>
<p>But even so. Yeah, Airbnb was pretty cool. (And walking distance to my house!) But still, Google is Google—the tech equivalent of getting an invitation to the Justice League. How could I turn that down?</p>
<p>After the interview, Airbnb was slow to get back to me. Being a challenging interview, I was unsure whether an offer would come through—much less, if it could change anything. I’d already finished my last day working at App Academy, and was now officially between jobs. I decided to head home for a bit to wrap up my negotiations and rest up before starting my next gig. I had a short deadline until I wanted to start, and let each of the companies know. It was now time to go into final negotiations.</p>
<p>Turning Point</p>
<p>I get the phone call. Airbnb wants to make an offer. I have less than a week, but they’re scrambling to piece everything together to meet my deadline. They’ll get back to me.</p>
<p>They get back. Initial offer, 220K.</p>
<p>220K?</p>
<p>Jesus Christ.</p>
<p>What??</p>
<p>What on earth are they doing offering me 220K? This company must be out of its goddamn mind. No wonder tech is so overvalued. I lost my shit.</p>
<p>I had to take this really seriously now. Airbnb’s offer was highly illiquid, but they were offering me a very strong base salary: 130K, 25K signing bonus, and 65K worth of RSUs a year. Not as liquid as Google, no doubt. But Airbnb as a company was barreling toward an IPO, already worth more than 23B, and was of the most robust tech companies from a business and revenue perspective. Besides, if I’m risk-neutral—which I think I am—then it’s a clearly better deal.</p>
<p>I began really weighing things. Google… was Google. I already knew its advantages.</p>
<p>But at Airbnb, I’d clearly be able to take on more responsibility more quickly. The engineering organization was slated to double within a year. I was already very familiar with Airbnb’s tech stack, mostly Ruby on Rails, since it was what I’d been teaching at App Academy and developing on for the last year. It was also younger company. Unlike Google and its “lifers,” this was a place where I’d be more likely to meet people in an earlier stage in their lives—and meet potential co-founders.</p>
<p>Of course, it was also walking distance (the ol’ commute-less commute).</p>
<p>But, seemingly more affecting than all of those things—they needed me. At Google, I’d be a smart person sitting in a smart person chair. I’d be doing serious engineering work no doubt, but by the time I’d be done, I’d be walking away having changed little more than my own resume.</p>
<p>At Airbnb, they really needed my help. They believed I could make a difference. For the life of me I couldn’t figure out why, but they made it clear. They thought I was valuable, and they wanted someone with my experience.</p>
<p>God, I’m a sap.</p>
<p>I had less than a week to decide. I didn’t know how to make the decision. So I started trying to evaluate everything objectively. I tried estimating the value of different outcomes. I tried to ascribe weights and calculate the value of things like network, branding, growth, faster promotion, potential for meeting co-founders, likelihood of IPOing. But nothing was convincing. It felt like an impossible choice. Before long it was Thursday night and I had to make a decision before the end of day Friday. There was a lot of cursing.</p>
<p>Google was assuring me that they’d raise their number by Friday morning. That night I couldn’t sleep. I repeatedly got out of bed, pacing back and forth rehearsing conversations, frantically checking my e-mail and re-tweaking Excel models. What the fuck? Just a few weeks ago I could barely get anyone to look at my resume. My first offer barely cleared 120K. 220K? What on earth is Airbnb thinking??</p>
<p>That night and the following morning, I called up trusted friends and asked them what I should do. I was on the phone off and on for several hours through that morning. The consensus was overwhelming.</p>
<p>Everyone thought I should go with Airbnb.</p>
<p>Final Hours</p>
<p>That morning, worn thin with sleep deprivation, I got a call from Google. They were raising the offer to 211K. That’s 211K. Liquid. I’d receive all of that before a year’s end. I remember scratching my chin and just laughing. God fucking dammit. I thanked my recruiter, and let her know I’d get back to her before the end of the day.</p>
<p>I knew Google would have a strong final offer, maybe upwards of 200K, but 211K was stronger than I’d expected. Still, it shouldn’t sway my decision, right? Was this really a decision within a margin of 10K in EV? No one else seemed to think so. If Airbnb was the winner, it should still be the winner now.</p>
<p>Right?</p>
<p>I told myself: if I’m choosing Airbnb, just remember. 220K was their initial offer. That means there’s money on the table. If there’s one thing this job search had taught me, is that there’s always, always more money on the table.</p>
<p>As an instructor at App Academy I ceaselessly pushed my students to negotiate hard, without fear of being rejected, looking stupid, or being perceived as greedy. Employers negotiate even harder and with more power behind them, and so it’s up to candidates to tip the scales back in the direction of employees. As absurd as it seemed now, given an offer of 220K, I had to take my own advice and ask for more. I rehearsed it again and again in my head.</p>
<p>But if I’m going to ask for more, I shouldn’t just ask for 10K more. I should expect to be met somewhere in the middle, right? I’ve got to be aggressive. But I don’t have any stronger offers and they know that, so maybe it’s stupid.</p>
<p>Fuck.</p>
<p>I called my recruiter from Airbnb and, half expecting to get laughed at, announced: “If Airbnb can move up the RSUs by 30K to hit a total of 250K in all-in compensation, then I’ll sign.”</p>
<p>“250K? Do I have your word?” she said.</p>
<p>“My word.”</p>
<p>“Okay. Let me see what I can do. I’ll get back to you before evening.”</p>
<p>The rest of the afternoon was a blur of habitually checking my phone, email, anything for some word of what would happen. I was driving my car into Austin, bumper-to-bumper in traffic when I got the phone call. It was the Airbnb recruiter.</p>
<p>“Haseeb? This is Janice.”</p>
<p>“Janice! What’s the word??” I held my breath.</p>
<p>“I made the magic happen. 250K. 130K salary, 25K signing, 95K a year in RSUs. So you’re in?”</p>
<p>I almost swerved into the car next to me from punching the air so hard.</p>
<p>“I’m in.”</p>
<p>So that’s the story. I start on Monday.</p>
<p>Overview</p>
<p>On the whole, I’m really excited to be joining the Airbnb family. It’s definitely a different path than Google. Most significantly, there’ll be less that I can donate to charity in a year’s time. But not only is the total value of the assets greater, I’m also convinced that in the longer term, Airbnb will be the better move for my career.</p>
<p>Stock in a private company is of course still a valuable asset—I just won’t be able to donate a third of their value until a liquidation event (hopefully an IPO). But my pledge to donate 1/3rd of all of my income, including my salary, doesn’t change. Ultimately, this will be a better move in terms of EV, provided some discounting for the fact that I’ll have to donate later in the future.</p>
<p>That said, the brand of the company was obviously an explicit consideration for me. From that perspective, my choice to join Airbnb might seem a little strange to some. In many parts of the world, Google and Airbnb wouldn’t even be said in the same sentence. But within the SF ecosystem, Airbnb and Google are largely considered to be in the same tier of engineering organizations. I’d even argue that among certain crowds, Airbnb would be a stronger signal on one’s resume than Google would be.</p>
<p>So I think, on the whole, I really got lucky with a company that’s a perfect fit for what I’m looking for.</p>
<p>One other thing I want to add. I know some might think it’s weird to be sharing your compensation in a public venue like this. I take my inspiration from Jeff Kaufman (who ironically works for Google, albeit in Boston), who donates 50% of his income to EA charities and catalogues it publicly every year.</p>
<p>The norm of keeping compensation secret is a very American one. And I think on the whole, it stifles transparency and open dialogue about things like class and economic inequality. But more poignantly, it makes it harder to be transparent and galvanizing about earning-to-give, which is very much one of my goals.</p>
<p>I explicitly entered into tech a year ago because I believed it was a place where I’d be uniquely well-suited to create a lot of value. It’s an industry where smart, driven young people can quickly earn an outsized income. Seeing that opportunity, I decided to pivot into tech and make building a career here an active aim of my life. And I hope to, as long as I can, continue to speak openly about my compensation and give even more of it away.</p>
<p>So that’s enough of soapboxing. Here are some stats.</p>
<p>A Few Statistics</p>
<p>By the end of my process, I had received a total of 8 job offers (not including re-negotiation with App Academy). I had completed 12 onsites total, so my I had a total onsite-to-offer rate of 66%.</p>
<p>Considering only the onsites after receiving my first offer however, my onsite-to-offer rate was 87%.</p>
<p>Of my offers, 2 of them were through TripleByte (one from TripleByte itself, the other, Gusto, through their introduction). The remainder were through referrals. Two of the 6 referrals were non-engineering referrals.</p>
<p>There were 12 companies where I was referred but did not receive an offer. 25% of the companies I was referred to didn’t even talk to me. But that was a lot better than the base rate: overall, 53% of companies I applied to through any means rejected me without even talking to me.</p>
<p>In the end, I didn’t get a single offer through a raw application. Every single offer came through a referral of some kind. (This I did not expect, and strongly influences the advice I’d give to a job-seeker.)</p>
<p>The differential between my lowest (initial) offer and my highest (final) offer was, in total value, an increase of 104%. That is, my compensation literally more than doubled by not accepting my first job offer. (I expect this to be highly anomalous, since companies had a lot of discrepancies in where they leveled me on their engineering ladders.)</p>
<p>The largest percentage increase was also with my lowest initial offer, Yelp. They started at a total annualized value of 122.5K (105K salary + 17.5K stock), and their final offer was at a total value of 180K, an increase of $57.5K, or 47%. Again, I expect this to be anomalous, because I literally jumped in engineering levels (they initially offered me a junior role, then re-evaluated). The second largest increase was from Google, moving up $48.6K for an increase of 30%. This one is perhaps less anomalous. I’m not sure.</p>
<p>On all but two of my offers, I negotiated. The average delta for company offers at which I negotiated was +$30,671 for the final offer. So, negotiation pays, boys and girls. More on that later.</p>
<p>In total, I received offers from Airbnb, Youtube (Google subsidiary), Uber, Twitch (Amazon subsidiary), Yelp, Stripe, Gusto, and TripleByte. The average value of the final offers was approximately $193,600. The salaries were all approximately the same at $130K. (Only one company offered me a base salary of $125K.) I should also note that I negotiated more aggressively on RSUs and signing bonus. I’ll also talk more about that later.</p>
<p>There’s obviously a lot in this department, but this blog post is already long enough. I’m going to write one more post before I start at Airbnb this coming Monday: my key takeaways from this job search, and advice to current job seekers. I’ll also talk more about my thoughts on negotiation in that upcoming post.</p>
<p>Look forward to it!</p>
<p>Update (4/25/2016)</p>
<p>This post has gotten a lot of unexpected attention, so I want to clarify a couple things.</p>
<p>First (and this should go without saying), everything mentioned in my story are my own thoughts and words, and though I am employed by Airbnb, I in no way speak on their behalf. (It’s legally important to say that, I guess.)</p>
<p>That said, I realize that pay transparency is a controversial subject, and I seem to have stirred up a hornet’s nest. So I want to make clear a few things.</p>
<p>Many people seem to think that publicly disclosing your compensation is a fireable offense, and that Airbnb will fire me now that they see I’ve posted this. Neither of those statements are true.</p>
<p>It’s explicitly illegal under the National Labor Relations Act to fire workers for discussing their compensations, and it has been illegal in American law since 1935. This kind of policy is known as “Pay Secrecy,” and such policies have been repeatedly struck down by courts.</p>
<p>See: Pay Secrecy Policies at Work are Often Illegal and Misunderstood.</p>
<p>And yet, pay secrecy is considered the norm in American culture.</p>
<p>There was a time in American history when income tax records were in the public domain. Even now, there are many cultures in which talking about your compensation is not at all considered wrong or disrespectful. Rather, it’s seen as an important part of creating a transparent society.</p>
<p>Pay secrecy is an easy fallback, because talking about social inequality and pay differentials is uncomfortable. It’s especially uncomfortable in American culture, where your salary often serves as a signal of your social status.</p>
<p>But pay secrecy carries with it a lot of costs. It can hide gender and racial discrimination, systemic exploitation, and nepotism. It’s easy to sweep these problems under the rug and not want to look at them. And for what it’s worth, I have a lot of respect to Airbnb for openly working toward solving these problems in the culture they’re building.</p>
<p>These are complicated problems. I don’t have all the answers. But because I am committed to effective altruism and publicly earning-to-give, I will continue to be open every year about how much I donate (and by association, how much I earn), insofar as it’s possible.</p>
<p>(For further reading, check out Your Coworkers Should Know Your Salary, published in the Harvard Business Review. Also, my response to a commenter on how disclosing my salary might be unwise or affect colleagues at Airbnb.)</p>
<p>The second thing I want to say is that for some reason, many people are drawing the conclusion that I primarily got a job at Airbnb by being really good at negotiating, or gaming the system, or something like that.</p>
<p>If you have ever done a software engineering interview, you would know that this is completely absurd. There is no way to negotiate or charm someone into passing a software engineering interview. Much less 8 of them.</p>
<p>If you don’t know the ins and outs of object-oriented programming, database design, asymptotic analysis, binary search trees, or how to improve the cache efficiency of an algorithm, then you’re not going to pass a computer science-heavy interview at a top company. You don’t even have a shot at it.</p>
<p>The moral of my story is not “get really good at negotiating and you’ll get a great job.” Negotiating is important, and I certainly encourage everyone to negotiate!</p>
<p>But first, get good at the thing you’re doing. Then worry about negotiating.</p>
<p>I moved to San Francisco from Austin, Texas about a year ago because I decided I was going to earn-to-give. Since then I’ve been relentlessly trying to build a new career for myself, so I could earn more and donate more to charity.</p>
<p>Entering into App Academy, barely knowing the basics of Ruby, I came into the office and grinded every day, spending 80+ hour weeks just coding and studying. I’d come in at 9AM in the morning and leave around midnight, 7 days a week, sleeping in a bunk bed in SOMA in a 200 square feet shared room.</p>
<p>It’s certainly true that I probably have a mind that’s well-suited for coding. But it’s also true that I outworked almost everyone who was in my cohort. And when I was hired by App Academy to help teach the course, I continued working as hard as I could to get good at this.</p>
<p>I still stayed late in the evenings, I still came into the office on weekends alongside the students just to continue coding and learning more. I started an algorithms study group in the evenings where I taught students new algorithms that I had read about, and wrote specs and instructions to guide them through the implementation. I took over our entire algorithms curriculum and taught well over 100 students the basics of data structures and algorithms.</p>
<p>And of course, I was scared that none of this would matter. That having been an English major, having a non-traditional background, being 26 and too old to transition into tech, competing against 20 year olds who’d been coding since they were 10, I thought I must have no chance.</p>
<p>Thankfully, I was wrong. And I’m very, very lucky that I was wrong, because I was almost right.</p>
<p>So if there’s one takeaway I want people to have, it’s not “here’s why it pays to be a master negotiator.” I’m going to talk a lot more about negotiation advice in my next blog post, but that’s almost a detail compared to the larger point.</p>
<p>The real takeaway should be: get so good that they can’t ignore you. Because once you are, they won’t.</p>
<p>-Haseeb</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;I think this is a great great blog talking about, as a SDE, how to negotiate your salary package with tech companies.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The original post is &lt;a href=&quot;http://haseebq.com/farewell-app-academy-hello-airbnb-part-ii/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Farewell-App-Academy-Hello-Airbnb-Part-II&quot;&gt;&lt;a href=&quot;#Farewell-App-Academy-Hello-Airbnb-Part-II&quot; class=&quot;headerlink&quot; title=&quot;Farewell, App Academy. Hello, Airbnb. (Part II)&quot;&gt;&lt;/a&gt;Farewell, App Academy. Hello, Airbnb. (Part II)&lt;/h2&gt;&lt;p&gt;April 14, 2016&lt;br&gt;airbnb&lt;br&gt;(Note: this is the second part of this story. You can read the first part here.)&lt;/p&gt;
&lt;p&gt;Despite the rejection from 23AndMe, I kept pushing.&lt;/p&gt;
&lt;p&gt;I applied to the all the big hiring websites. Hired rejected me from their platform. I got no bites anywhere on AngelList or LinkedIn—not even cold e-mails from recruiters. Nothing from WhiteTruffle or SmartHires.&lt;/p&gt;
&lt;p&gt;Not a breath of interest anywhere.&lt;/p&gt;
&lt;p&gt;I kept on. I asked friends, students, anyone I knew for referrals. I started reaching out to non-engineers. I asked anyone at all who worked at all at a tech company I found compelling.&lt;/p&gt;
&lt;p&gt;I was nervous. Was I a fraud? At App Academy I might have seemed like a paragon, but to the rest of the world… was I simply not good enough?&lt;/p&gt;
&lt;p&gt;A Swing of the Pendulum&lt;/p&gt;
&lt;p&gt;In the middle of this flurry, I created an account on TripleByte. TripleByte is a young YC startup that’s trying to change tech hiring. From their manifesto:&lt;/p&gt;
&lt;p&gt;“Credentials should not be used as a proxy for talent. Education and work history are meaningful but relying solely on them results in missing good programmers. Good programmers come from all types of background. It’s what you can do that matters, not where you went to school.”&lt;/p&gt;
&lt;p&gt;Their interview process is completely blind to credentials. They decide to work with you purely on the basis of a technical assessment, then they learn about your credentials and then introduce you to startups they determine are a good match. Though they mostly work with experienced software engineers, I decided to give them a try.&lt;/p&gt;
&lt;p&gt;To my surprise, I was able to pass their anonymous programming quiz and I was automatically invited to an onsite assessment. I showed up sheepishly to their office on King Street, and over an intense three hours I was grilled by my interviewer on coding, data structures, algorithms, and system design. At the end of the interview, he asked me what my computer science background was. None, I told him. I’d learned this stuff on my own and from teaching at App Academy. He was taken aback, and told me I was incredibly strong for only having studied this material less than a year.&lt;/p&gt;
&lt;p&gt;The next day, TripleByte called me. They wanted to work with me! They’d introduce me directly to startups in the YC network. This was exciting—there were a ton of companies in YC network I wanted to talk to: Stripe, Twitch, Airbnb, to name a few.&lt;/p&gt;
&lt;p&gt;Well, as it turned out, those companies weren’t really looking for someone like me. Since TripleByte is a two-sided process, they matched me up with companies for which there was a good mutual fit—Stripe, Twitch, or Airbnb were nowhere on that list. However, one of the companies they matched me with was Gusto, formerly Zenpayroll (and now a timely Zenefits competitor). I’d previously had my eye on them, so I was excited to see them matched up with me. They also lined up an onsite at a small YC startup called Flexport.&lt;/p&gt;
&lt;p&gt;TripleByte had gotten things moving. But even more importantly, they had given me back some confidence. TripleByte wanting to work with me was a small affirmation, to be sure—it wasn’t even a job offer yet. But it felt like I’d finally pierced the membrane. If these guys thought I was good… I must have it. I just gotta prove that to everyone else now.&lt;/p&gt;
    
    </summary>
    
      <category term="career and leadership" scheme="https://phoenixjiangnan.github.io/categories/career-and-leadership/"/>
    
    
      <category term="career" scheme="https://phoenixjiangnan.github.io/tags/career/"/>
    
      <category term="negotiation" scheme="https://phoenixjiangnan.github.io/tags/negotiation/"/>
    
      <category term="salary negotiation" scheme="https://phoenixjiangnan.github.io/tags/salary-negotiation/"/>
    
  </entry>
  
  <entry>
    <title>Java Network - TCP and Socket</title>
    <link href="https://phoenixjiangnan.github.io/2016/09/19/java/network/Java-Network-TCP-and-Socket/"/>
    <id>https://phoenixjiangnan.github.io/2016/09/19/java/network/Java-Network-TCP-and-Socket/</id>
    <published>2016-09-19T07:34:53.000Z</published>
    <updated>2016-09-21T08:10:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>The <code>Transmission Control Protocol (TCP)</code> is a core protocol of the Internet protocol suite.</p>
<p>It originated in the initial network implementation in which it complemented the <code>Internet Protocol (IP)</code>. Therefore, the entire suite is commonly referred to as <code>TCP/IP</code>. </p>
<p>TCP provides <code>reliable</code>, <code>ordered</code>, and <code>error-checked</code> delivery of a stream of octets between applications running on hosts communicating over an IP network.</p>
<p>Major Internet applications such as the World Wide Web, email, remote administration and file transfer rely on TCP. Applications that do not require reliable data stream service may use the <code>User Datagram Protocol (UDP)</code>, which provides a connectionless datagram service that emphasizes reduced latency over reliability.</p>
<h2 id="Socket-and-ServerSocket"><a href="#Socket-and-ServerSocket" class="headerlink" title="Socket and ServerSocket"></a>Socket and ServerSocket</h2><p><a href="https://docs.oracle.com/javase/8/docs/api/java/net/Socket.html" target="_blank" rel="external">Socket</a> class:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Socket</span></span></div><div class="line"><span class="keyword">extends</span> <span class="title">Object</span></div><div class="line"><span class="keyword">implements</span> <span class="title">Closeable</span></div></pre></td></tr></table></figure>
<blockquote>
<p>This class implements <code>client sockets</code> (also called just <code>sockets</code>).</p>
<p>A socket is an endpoint for communication between two machines.</p>
<p>The actual work of the socket is performed by an instance of the <code>SocketImpl</code> class. An application, by changing the socket factory that creates the socket implementation, can configure itself to create sockets appropriate to the local firewall.</p>
</blockquote>
<hr>
<p><a href="https://docs.oracle.com/javase/8/docs/api/java/net/ServerSocket.html" target="_blank" rel="external">ServerSocket</a> class:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ServerSocket</span></span></div><div class="line"><span class="keyword">extends</span> <span class="title">Object</span></div><div class="line"><span class="keyword">implements</span> <span class="title">Closeable</span></div></pre></td></tr></table></figure>
<blockquote>
<p>This class implements <code>server sockets</code>. </p>
<p>A server socket waits for requests to come in over the network. It performs some operation based on that request, and then possibly returns a result to the requester.</p>
<p>The actual work of the server socket is performed by an instance of the <code>SocketImpl</code> class. An application can change the socket factory that creates the socket implementation to configure itself to create sockets appropriate to the local firewall.</p>
</blockquote>
<h2 id="Communication-between-Client-and-Server"><a href="#Communication-between-Client-and-Server" class="headerlink" title="Communication between Client and Server"></a>Communication between Client and Server</h2><h3 id="Client-Side-Procedures"><a href="#Client-Side-Procedures" class="headerlink" title="Client Side Procedures"></a>Client Side Procedures</h3><ol>
<li>Create TCP client sockets with <code>Socket</code> object. Specify the destination of targeted server when initializing clients</li>
<li><p>If clients are created successfully, it means the data transportation channel is established. The channel is socket stream, which is constructed underlayer. Since it’s a stream, it indicates that there are both input stream and output stream.</p>
<p> You can get those two streams from <code>Socket</code> objects by calling <code>getOutputStream()</code> and <code>getInputStream()</code>.</p>
</li>
<li>Use the stream to transport data</li>
<li>Close resources</li>
</ol>
<p>Example:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Client End</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClientDemo</span></span>&#123;</div><div class="line">    <span class="comment">/*</span></div><div class="line">        Client sends data to Server</div><div class="line">    */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> UnknownHostException, IOException</span>&#123;</div><div class="line">        <span class="comment">// 1. Create client socket</span></div><div class="line">        Socket socket = <span class="keyword">new</span> Socket(<span class="string">"10.128.51.138"</span>, <span class="number">10002</span>);</div><div class="line">         </div><div class="line">        <span class="comment">// 2. Get the output stream from socket</span></div><div class="line">        OutputStream out = socket.getOutputStream();</div><div class="line">         </div><div class="line">        <span class="comment">// 3. Use output stream to write data to Server </span></div><div class="line">        out.write(<span class="string">"TCP demo!"</span>.getBytes());</div><div class="line">         </div><div class="line">        <span class="comment">// 4. Close resources</span></div><div class="line">        socket.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Server-Side-Procedures"><a href="#Server-Side-Procedures" class="headerlink" title="Server Side Procedures"></a>Server Side Procedures</h3><ol>
<li>Create server side socket service by creating <code>ServerSocket</code> object</li>
<li><code>ServerSocket</code> must provide a <code>port</code> for client to connect to</li>
<li>Get the socket object that tries to connect</li>
<li>Read data from client via the socket object’s stream</li>
<li>Close resources</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Server End</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ServerDemo</span> </span>&#123;</div><div class="line">    <span class="comment">/**</span></div><div class="line">        Get the data from client, and print on console</div><div class="line">    */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> UnknownHostException, IOException</span>&#123;</div><div class="line">        <span class="comment">// 1. Create server side socket service by creating `ServerSocket` object</span></div><div class="line">        <span class="comment">// 2. `ServerSocket` must provide a `port` for client to connect to</span></div><div class="line">        ServerSocket ss = <span class="keyword">new</span> ServerSocket(<span class="number">10002</span>);</div><div class="line">         </div><div class="line">        <span class="comment">// 3. Get the socket object that tries to connect</span></div><div class="line">        Socket s = ss.accept();</div><div class="line">        String ip = s.getInetAddress().getHostAddress();</div><div class="line">         </div><div class="line">        <span class="comment">// 4. Read data from client via the socket object's stream</span></div><div class="line">        InputStream in = s.getInputStream();</div><div class="line">        <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</div><div class="line">        <span class="keyword">int</span> len = in.read(buf);</div><div class="line">        </div><div class="line">        String text = <span class="keyword">new</span> String(buf, <span class="number">0</span>, len);</div><div class="line">        System.out.println(ip + <span class="string">" Server: "</span> + text);</div><div class="line">        </div><div class="line">        <span class="number">5</span>. Close resources</div><div class="line">        s.close();</div><div class="line">        ss.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="Interaction-between-Client-and-Server-Sending-Data-back-and-forth"><a href="#Interaction-between-Client-and-Server-Sending-Data-back-and-forth" class="headerlink" title="Interaction between Client and Server - Sending Data back and forth"></a>Interaction between Client and Server - Sending Data back and forth</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Server Side</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ServerDemo2</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> UnknownHostException, IOException</span>&#123;</div><div class="line">        ServerSocket serverSocket = <span class="keyword">new</span> ServerSocket(<span class="number">10002</span>);</div><div class="line">        Socket socket = serverSocket.accept(); <span class="comment">// This is a blocking method</span></div><div class="line">         </div><div class="line">        String ip = socket.getInetAddress().getHostAddress();</div><div class="line">        InputStream in = socket.getInputStream();</div><div class="line">         </div><div class="line">        <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</div><div class="line">        <span class="keyword">int</span> len = in.read(buf);</div><div class="line">        </div><div class="line">        String text = <span class="keyword">new</span> String(buf, <span class="number">0</span>, len);</div><div class="line">        System.out.println(ip + <span class="string">"  "</span> + text);</div><div class="line">         </div><div class="line">        <span class="comment">// Write through ServerSocket's OutputStream to client socket's InputStream </span></div><div class="line">        OutputStream out = socket.getOutputStream();</div><div class="line">        out.write(<span class="string">"Received"</span>.getBytes());</div><div class="line">         </div><div class="line">        socket.close();</div><div class="line">        serverSocket.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Client Side</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ClientDemo2</span></span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> UnknownHostException, IOException</span>&#123;</div><div class="line">        Socket socket = <span class="keyword">new</span> Socket(<span class="string">"10.128.51.138"</span>, <span class="number">10002</span>);</div><div class="line">         </div><div class="line">        OutputStream out = socket.getOutputStream();</div><div class="line">        out.write(<span class="string">"TCP demo!"</span>.getBytes());</div><div class="line">         </div><div class="line">        <span class="comment">// Use socket's InputStream to read data sent from server</span></div><div class="line">        InputStream in = socket.getInputStream();</div><div class="line">        <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</div><div class="line">         </div><div class="line">        <span class="keyword">int</span> len = in.read(buf);</div><div class="line">         </div><div class="line">        String text = <span class="keyword">new</span> String(buf, <span class="number">0</span>, len);</div><div class="line">        System.out.println(text);</div><div class="line">         </div><div class="line">        socket.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Text-Conversion"><a href="#Text-Conversion" class="headerlink" title="Text Conversion"></a>Text Conversion</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Server End</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TransServer</span></span>&#123;</div><div class="line">    <span class="comment">/*</span></div><div class="line">        Analysis:</div><div class="line">        1. Create ServerSocket</div><div class="line">        2. Get Socket obejct</div><div class="line">        3. Get socket's InputStream, and decorate it </div><div class="line">        4. Get socket's OutputStream, decorate it, and write data through it</div><div class="line">        5. Close resources</div><div class="line">    */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> UnknownHostException, IOException</span>&#123;</div><div class="line">        <span class="comment">// 1. Create ServerSocket</span></div><div class="line">        ServerSocket ss = <span class="keyword">new</span> ServerSocket(<span class="number">10004</span>);</div><div class="line">       </div><div class="line">        <span class="comment">// 2. Get Socket obejct</span></div><div class="line">        Socket s = ss.accept(); <span class="comment">// Blocking!</span></div><div class="line">         </div><div class="line">        <span class="comment">// Get IP</span></div><div class="line">        String ip = s.getInetAddress().getHostAddress();</div><div class="line">        System.out.println(ip+<span class="string">"....connected"</span>);</div><div class="line">         </div><div class="line">        <span class="comment">// 3. Get socket's InputStream, and decorate it </span></div><div class="line">        BufferedReader bIn = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(s.getInputStream()));</div><div class="line">         </div><div class="line">        <span class="comment">// 4. Get socket's OutputStream, decorate it, and write data through it</span></div><div class="line">        PrintWriter out = <span class="keyword">new</span> PrintWriter(s.getOutputStream(),<span class="keyword">true</span>);</div><div class="line">        </div><div class="line">        String line = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">while</span>((line = bIn.readLine()) != <span class="keyword">null</span>)&#123;</div><div class="line">            System.out.println(line + <span class="string">"...."</span> + line.toUpperCase());</div><div class="line">            out.println();</div><div class="line">        &#125;</div><div class="line">        </div><div class="line">        <span class="comment">// 5. Close resources</span></div><div class="line">        s.close();</div><div class="line">        ss.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Client End</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TransClient</span> </span>&#123;</div><div class="line">    <span class="comment">/*</span></div><div class="line">        Analysis:</div><div class="line">        </div><div class="line">        Client Side:</div><div class="line">        1. Need a socket endpoint</div><div class="line">        2. data source: keyboard input</div><div class="line">        3. data destination: socket's OutputStream</div><div class="line">        4. data source from server: socket's InputStream</div><div class="line">        4. print data on console</div><div class="line">        6. all data processed is text data</div><div class="line">    */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span><span class="keyword">throws</span> UnknownHostException, IOException</span>&#123;</div><div class="line">        Socket s = <span class="keyword">new</span> Socket(<span class="string">"10.128.51.138"</span>, <span class="number">10004</span>);</div><div class="line">         </div><div class="line">        <span class="comment">// Get data from keyboard</span></div><div class="line">        BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</div><div class="line">         </div><div class="line">        <span class="comment">// socket's OutputStream</span></div><div class="line">        PrintWriter out = <span class="keyword">new</span> PrintWriter(s.getOutputStream(), <span class="keyword">true</span>);  <span class="comment">// `true` means supporting continue to write</span></div><div class="line">         </div><div class="line">        <span class="comment">// Read data from server via socket's InputStream</span></div><div class="line">        BufferedReader bIn = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(s.getInputStream()));</div><div class="line">         </div><div class="line">        String line = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">while</span>((line=br.readLine())!=<span class="keyword">null</span>)&#123;</div><div class="line">            <span class="keyword">if</span>(<span class="string">"over"</span>.equals(line)) &#123;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            &#125;</div><div class="line">         		   </div><div class="line">        	<span class="comment">//out.print(line + "\r\n");</span></div><div class="line">            <span class="comment">//out.flush();</span></div><div class="line">            out.println(line);</div><div class="line">            <span class="comment">// Write data to server, println() is PrintWriter's method, is different from System.out.println()</span></div><div class="line">         </div><div class="line">            <span class="comment">// Read a line of text in upper cases from server</span></div><div class="line">            String upperStr = bIn.readLine();</div><div class="line">            System.out.println(upperStr);</div><div class="line">        &#125;</div><div class="line">        </div><div class="line">        s.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h2><h3 id="Uploading-Files"><a href="#Uploading-Files" class="headerlink" title="Uploading Files"></a>Uploading Files</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UploadServer</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> UnknownHostException, SocketException,IOException</span>&#123;</div><div class="line">        ServerSocket ss = <span class="keyword">new</span> ServerSocket(<span class="number">10005</span>);</div><div class="line">        Socket s = ss.accept();</div><div class="line">        System.out.println(s.getInetAddress().getHostAddress() + <span class="string">"...connected"</span>);</div><div class="line">         </div><div class="line">        BufferedReader bIn = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(s.getInputStream())); </div><div class="line">        BufferedWriter bw = <span class="keyword">new</span> BufferedWriter(<span class="keyword">new</span> FileWriter(<span class="string">"Server.txt"</span>));</div><div class="line">         </div><div class="line">        String line = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">while</span>((line = bIn.readLine())!=<span class="keyword">null</span>)&#123;</div><div class="line">            bw.write(line);</div><div class="line">            bw.newLine();</div><div class="line">            bw.flush();</div><div class="line">        &#125;</div><div class="line">         </div><div class="line">        PrintWriter out = <span class="keyword">new</span> PrintWriter(s.getOutputStream(),<span class="keyword">true</span>);</div><div class="line">        out.println(<span class="string">"Successful!"</span>);</div><div class="line">         </div><div class="line">        bw.close();</div><div class="line">        s.close();</div><div class="line">        ss.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UploadClient</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> UnknownHostException,SocketException, IOException</span>&#123;</div><div class="line">        Socket s = <span class="keyword">new</span> Socket(<span class="string">"183.174.66.227"</span>, <span class="number">10005</span>);</div><div class="line">        </div><div class="line">        BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> FileReader(<span class="string">"client.txt"</span>));</div><div class="line">        PrintWriter out = <span class="keyword">new</span> PrintWriter(s.getOutputStream(),<span class="keyword">true</span>);</div><div class="line">         </div><div class="line">        String line = <span class="keyword">null</span>;</div><div class="line">        <span class="keyword">while</span>((line = br.readLine()) != <span class="keyword">null</span>)&#123;</div><div class="line">            out.println(line);</div><div class="line">        &#125;</div><div class="line"> </div><div class="line">        <span class="comment">// Tell the server side that client has finished writing</span></div><div class="line">        s.shutdownOutput();</div><div class="line">         </div><div class="line">        BufferedReader bIn = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(s.getInputStream()));</div><div class="line">        String str = bIn.readLine();</div><div class="line">        System.out.println(str);</div><div class="line">         </div><div class="line">        br.close();</div><div class="line">        bIn.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Uploading-Pictures"><a href="#Uploading-Pictures" class="headerlink" title="Uploading Pictures"></a>Uploading Pictures</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UploadPicServer</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> UnknownHostException,SocketException, IOException</span>&#123;</div><div class="line">        ServerSocket ss = <span class="keyword">new</span> ServerSocket(<span class="number">10006</span>);</div><div class="line">        Socket s = ss.accept();</div><div class="line">         </div><div class="line">        InputStream in = s.getInputStream();</div><div class="line">        String ip = s.getInetAddress().getHostAddress();</div><div class="line">        System.out.println(ip + <span class="string">"....connected"</span>);</div><div class="line">         </div><div class="line">        <span class="comment">// Read data and put it in a file</span></div><div class="line">        File dir = <span class="keyword">new</span> File(<span class="string">"pic"</span>);</div><div class="line">        <span class="keyword">if</span>(!dir.exists()) &#123;</div><div class="line">            dir.mkdirs();</div><div class="line">        &#125;</div><div class="line">         </div><div class="line">        File file = <span class="keyword">new</span> File(dir, ip + <span class="string">".bmp"</span>);</div><div class="line">        FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(file);</div><div class="line">         </div><div class="line">        <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</div><div class="line">        <span class="keyword">int</span> len = <span class="number">0</span>;</div><div class="line">        <span class="keyword">while</span>((len=in.read(buf))!=-<span class="number">1</span>)&#123;</div><div class="line">            fos.write(buf,<span class="number">0</span>,len);</div><div class="line">        &#125;</div><div class="line">         </div><div class="line">        <span class="comment">// Get socket's OutputStream, and send text to client</span></div><div class="line">        OutputStream out = s.getOutputStream();</div><div class="line">        out.write(<span class="string">"Successful!"</span>.getBytes());</div><div class="line">         </div><div class="line">        fos.close();</div><div class="line">        s.close();</div><div class="line">        ss.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UploadPicClient</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> UnknownHostException,SocketException, IOException</span>&#123;</div><div class="line">        Socket s = <span class="keyword">new</span> Socket(<span class="string">"183.174.66.227"</span>, <span class="number">10006</span>);</div><div class="line">        FileInputStream fis = <span class="keyword">new</span> FileInputStream(<span class="string">"0.jpg"</span>);</div><div class="line">        OutputStream out = s.getOutputStream();</div><div class="line">         </div><div class="line">        <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</div><div class="line">        <span class="keyword">int</span> len = <span class="number">0</span>;</div><div class="line">        <span class="keyword">while</span>((len=fis.read(buf))!=-<span class="number">1</span>)&#123;</div><div class="line">            out.write(buf,<span class="number">0</span>,len);</div><div class="line">        &#125;</div><div class="line">         </div><div class="line">        <span class="comment">// Tell Server that it has finished data transportion, and let server stop reading</span></div><div class="line">        s.shutdownOutput();</div><div class="line">         </div><div class="line">        <span class="comment">// Read content from server</span></div><div class="line">        InputStream in = s.getInputStream();</div><div class="line">        <span class="keyword">byte</span>[] bufIn = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</div><div class="line">         </div><div class="line">        <span class="keyword">int</span> lenIn = in.read(buf);</div><div class="line">        String text = <span class="keyword">new</span> String(buf, <span class="number">0</span>, lenIn);</div><div class="line">        System.out.println(text);</div><div class="line">         </div><div class="line">        fis.close();</div><div class="line">        s.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Multi-Threaded-Server"><a href="#Multi-Threaded-Server" class="headerlink" title="Multi-Threaded Server"></a>Multi-Threaded Server</h2><p>Client is the same as the above one.<br>Only need to change Server side.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UploadPicServer</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> UnknownHostException,SocketException, IOException</span>&#123;</div><div class="line">        ServerSocket ss = <span class="keyword">new</span> ServerSocket(<span class="number">10006</span>);</div><div class="line">         </div><div class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</div><div class="line">            Socket s = ss.accept();</div><div class="line">            <span class="keyword">new</span> Thread(<span class="keyword">new</span> UploadTask(s)).start();</div><div class="line">        &#125;</div><div class="line">         </div><div class="line">        ss.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UploadTask</span> <span class="keyword">implements</span> <span class="title">Runnable</span>  </span>&#123;</div><div class="line">   <span class="keyword">private</span> Socket s;</div><div class="line"> </div><div class="line">   <span class="function"><span class="keyword">public</span> <span class="title">UploadTask</span><span class="params">(Socket s)</span></span>&#123;</div><div class="line">      <span class="keyword">this</span>.s = s;</div><div class="line">   &#125;</div><div class="line"> </div><div class="line">   <span class="meta">@Override</span></div><div class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</div><div class="line">          <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">          String ip = s.getInetAddress().getHostAddress();</div><div class="line">          System.out.println(ip + <span class="string">"....connected"</span>);</div><div class="line">     </div><div class="line">          <span class="keyword">try</span>&#123;</div><div class="line">                 <span class="comment">// Read data from client</span></div><div class="line">                 InputStream in = s.getInputStream();</div><div class="line">         </div><div class="line">                 <span class="comment">// Put data into a file</span></div><div class="line">                 File dir = <span class="keyword">new</span> File(<span class="string">"pic"</span>);</div><div class="line">                 <span class="keyword">if</span>(!dir.exists()) &#123;</div><div class="line">                     dir.mkdirs();</div><div class="line">                 &#125;</div><div class="line">                 </div><div class="line">                 File file = <span class="keyword">new</span> File(dir, ip + <span class="string">".jpg"</span>);</div><div class="line">                 </div><div class="line">                 <span class="keyword">if</span>(file.exists())&#123;</div><div class="line">                     file = <span class="keyword">new</span> File(dir, ip + <span class="string">" "</span> + (count++) + <span class="string">".jpg"</span>);</div><div class="line">                 &#125;</div><div class="line">         </div><div class="line">                 FileOutputStream fos = <span class="keyword">new</span> FileOutputStream(file);</div><div class="line">                 <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</div><div class="line">                 <span class="keyword">int</span> len = <span class="number">0</span>;</div><div class="line">                 <span class="keyword">while</span>((len=in.read(buf))!=-<span class="number">1</span>)&#123;</div><div class="line">                     fos.write(buf, <span class="number">0</span>, len);</div><div class="line">                 &#125;</div><div class="line">         </div><div class="line">                 <span class="comment">// Get the socket's OutputStream, send response to client</span></div><div class="line">                 OutputStream out = s.getOutputStream();</div><div class="line">                 out.write(<span class="string">"Successful"</span>.getBytes());</div><div class="line">         </div><div class="line">                 fos.close();</div><div class="line">                 s.close();</div><div class="line">         &#125; <span class="keyword">catch</span>(IOException e) &#123;&#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Common-Clients-and-Server-and-Principles-behind-them"><a href="#Common-Clients-and-Server-and-Principles-behind-them" class="headerlink" title="Common Clients and Server, and Principles behind them"></a>Common Clients and Server, and Principles behind them</h2><p>The most common clients - web browser, or custom client<br>The most common server - web servers, like Apache Tomcat, Ngnix, JBoss, WebSphere</p>
<p>In order to learn the principles of server, let’s implement a mocked server and use existing client - web browser - to see what kind of data client sends to server</p>
<h3 id="Mocked-Server"><a href="#Mocked-Server" class="headerlink" title="Mocked Server"></a>Mocked Server</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyTomcatDemo</span>  </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, UnknownHostException, SocketException</span>&#123;</div><div class="line">        ServerSocket ss = <span class="keyword">new</span> ServerSocket(<span class="number">9090</span>);</div><div class="line">        Socket s = ss.accept();</div><div class="line">        System.out.println(s.getInetAddress().getHostAddress() + <span class="string">"....connected"</span>);</div><div class="line">         </div><div class="line">        InputStream in = s.getInputStream();</div><div class="line">        <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</div><div class="line">        <span class="keyword">int</span> len = in.read(buf);</div><div class="line">        String text = <span class="keyword">new</span> String(buf,<span class="number">0</span>,len);</div><div class="line">        System.out.println(text);</div><div class="line">         </div><div class="line">        <span class="comment">// Send a message to client</span></div><div class="line">        PrintWriter out = <span class="keyword">new</span> PrintWriter(s.getOutputStream(), <span class="keyword">true</span>);</div><div class="line">         </div><div class="line">        out.println(<span class="string">"&lt;font size=7&gt;Welcome&lt;/font&gt;"</span>);</div><div class="line">         </div><div class="line">        s.close();</div><div class="line">        ss.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="Mocked-Client"><a href="#Mocked-Client" class="headerlink" title="Mocked Client"></a>Mocked Client</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBrowserDemo</span>  </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, UnknownHostException, SocketException</span>&#123;</div><div class="line">        Socket s = <span class="keyword">new</span> Socket(<span class="string">"183.174.66.227"</span>, <span class="number">8080</span>);</div><div class="line">         </div><div class="line">        <span class="comment">// Mock web browser, send a HTTP request to Tomcat</span></div><div class="line">        PrintWriter out = <span class="keyword">new</span> PrintWriter(s.getOutputStream(), <span class="keyword">true</span>);</div><div class="line">        out.println(<span class="string">"GET /1.html HTTP/1.1"</span>);</div><div class="line">        out.println(<span class="string">"Accept: */*"</span>);</div><div class="line">        out.println(<span class="string">"Host: 183.174.66.227:8080"</span>);</div><div class="line">        out.println(<span class="string">"Connection: close"</span>);</div><div class="line">        out.println();</div><div class="line">        out.println();</div><div class="line">         </div><div class="line">        InputStream in = s.getInputStream();</div><div class="line">        <span class="keyword">byte</span>[] buf = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</div><div class="line">        <span class="keyword">int</span> len = in.read(buf);</div><div class="line">         </div><div class="line">        String str = <span class="keyword">new</span> String(buf, <span class="number">0</span>, len);</div><div class="line">        System.out.println(str);</div><div class="line">         </div><div class="line">        s.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">// The message received is:</div><div class="line">HTTP/1.1 200 OK     // response line, with HTTP version number, status code, and status message</div><div class="line">// Response&apos;s attribtues-  &lt;name&gt;: &lt;value&gt;</div><div class="line">Server: Apache-Coyote/1.1</div><div class="line">Accept-Ranges: bytes</div><div class="line">ETag: W/&quot;43-1357224984849&quot;</div><div class="line">Last-Modified: Thu, 03 Jan 2013 14:56:24 GMT</div><div class="line">Content-Type: text/html</div><div class="line">Content-Length: 43</div><div class="line">Date: Thu, 03 Jan 2013 16:12:54 GMT</div><div class="line">Connection: close</div><div class="line">// Empty line</div><div class="line">// Response body</div><div class="line">&lt;font color = &quot;blue&quot; size=7&gt;welcome&lt;/font&gt;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The &lt;code&gt;Transmission Control Protocol (TCP)&lt;/code&gt; is a core protocol of the Internet protocol suite.&lt;/p&gt;
&lt;p&gt;It originated in the initial network implementation in which it complemented the &lt;code&gt;Internet Protocol (IP)&lt;/code&gt;. Therefore, the entire suite is commonly referred to as &lt;code&gt;TCP/IP&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;TCP provides &lt;code&gt;reliable&lt;/code&gt;, &lt;code&gt;ordered&lt;/code&gt;, and &lt;code&gt;error-checked&lt;/code&gt; delivery of a stream of octets between applications running on hosts communicating over an IP network.&lt;/p&gt;
&lt;p&gt;Major Internet applications such as the World Wide Web, email, remote administration and file transfer rely on TCP. Applications that do not require reliable data stream service may use the &lt;code&gt;User Datagram Protocol (UDP)&lt;/code&gt;, which provides a connectionless datagram service that emphasizes reduced latency over reliability.&lt;/p&gt;
&lt;h2 id=&quot;Socket-and-ServerSocket&quot;&gt;&lt;a href=&quot;#Socket-and-ServerSocket&quot; class=&quot;headerlink&quot; title=&quot;Socket and ServerSocket&quot;&gt;&lt;/a&gt;Socket and ServerSocket&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/net/Socket.html&quot;&gt;Socket&lt;/a&gt; class:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Socket&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Object&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Closeable&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;This class implements &lt;code&gt;client sockets&lt;/code&gt; (also called just &lt;code&gt;sockets&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;A socket is an endpoint for communication between two machines.&lt;/p&gt;
&lt;p&gt;The actual work of the socket is performed by an instance of the &lt;code&gt;SocketImpl&lt;/code&gt; class. An application, by changing the socket factory that creates the socket implementation, can configure itself to create sockets appropriate to the local firewall.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/net/ServerSocket.html&quot;&gt;ServerSocket&lt;/a&gt; class:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ServerSocket&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Object&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Closeable&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;This class implements &lt;code&gt;server sockets&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;A server socket waits for requests to come in over the network. It performs some operation based on that request, and then possibly returns a result to the requester.&lt;/p&gt;
&lt;p&gt;The actual work of the server socket is performed by an instance of the &lt;code&gt;SocketImpl&lt;/code&gt; class. An application can change the socket factory that creates the socket implementation to configure itself to create sockets appropriate to the local firewall.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Communication-between-Client-and-Server&quot;&gt;&lt;a href=&quot;#Communication-between-Client-and-Server&quot; class=&quot;headerlink&quot; title=&quot;Communication between Client and Server&quot;&gt;&lt;/a&gt;Communication between Client and Server&lt;/h2&gt;&lt;h3 id=&quot;Client-Side-Procedures&quot;&gt;&lt;a href=&quot;#Client-Side-Procedures&quot; class=&quot;headerlink&quot; title=&quot;Client Side Procedures&quot;&gt;&lt;/a&gt;Client Side Procedures&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Create TCP client sockets with &lt;code&gt;Socket&lt;/code&gt; object. Specify the destination of targeted server when initializing clients&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If clients are created successfully, it means the data transportation channel is established. The channel is socket stream, which is constructed underlayer. Since it’s a stream, it indicates that there are both input stream and output stream.&lt;/p&gt;
&lt;p&gt; You can get those two streams from &lt;code&gt;Socket&lt;/code&gt; objects by calling &lt;code&gt;getOutputStream()&lt;/code&gt; and &lt;code&gt;getInputStream()&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Use the stream to transport data&lt;/li&gt;
&lt;li&gt;Close resources&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// Client End&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ClientDemo&lt;/span&gt;&lt;/span&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;/*&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        Client sends data to Server&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    */&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String args[])&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;throws&lt;/span&gt; UnknownHostException, IOException&lt;/span&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;// 1. Create client socket&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        Socket socket = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; Socket(&lt;span class=&quot;string&quot;&gt;&quot;10.128.51.138&quot;&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10002&lt;/span&gt;);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;         &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;// 2. Get the output stream from socket&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        OutputStream out = socket.getOutputStream();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;         &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;// 3. Use output stream to write data to Server &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        out.write(&lt;span class=&quot;string&quot;&gt;&quot;TCP demo!&quot;&lt;/span&gt;.getBytes());&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;         &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;// 4. Close resources&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        socket.close();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;Server-Side-Procedures&quot;&gt;&lt;a href=&quot;#Server-Side-Procedures&quot; class=&quot;headerlink&quot; title=&quot;Server Side Procedures&quot;&gt;&lt;/a&gt;Server Side Procedures&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Create server side socket service by creating &lt;code&gt;ServerSocket&lt;/code&gt; object&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ServerSocket&lt;/code&gt; must provide a &lt;code&gt;port&lt;/code&gt; for client to connect to&lt;/li&gt;
&lt;li&gt;Get the socket object that tries to connect&lt;/li&gt;
&lt;li&gt;Read data from client via the socket object’s stream&lt;/li&gt;
&lt;li&gt;Close resources&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// Server End&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ServerDemo&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;/**&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        Get the data from client, and print on console&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    */&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(String[] args)&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;throws&lt;/span&gt; UnknownHostException, IOException&lt;/span&gt;&amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;// 1. Create server side socket service by creating `ServerSocket` object&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;// 2. `ServerSocket` must provide a `port` for client to connect to&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ServerSocket ss = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; ServerSocket(&lt;span class=&quot;number&quot;&gt;10002&lt;/span&gt;);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;         &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;// 3. Get the socket object that tries to connect&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        Socket s = ss.accept();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        String ip = s.getInetAddress().getHostAddress();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;         &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;comment&quot;&gt;// 4. Read data from client via the socket object&#39;s stream&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        InputStream in = s.getInputStream();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;byte&lt;/span&gt;[] buf = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;byte&lt;/span&gt;[&lt;span class=&quot;number&quot;&gt;1024&lt;/span&gt;];&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; len = in.read(buf);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        String text = &lt;span class=&quot;keyword&quot;&gt;new&lt;/span&gt; String(buf, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, len);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        System.out.println(ip + &lt;span class=&quot;string&quot;&gt;&quot; Server: &quot;&lt;/span&gt; + text);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;. Close resources&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        s.close();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        ss.close();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="java" scheme="https://phoenixjiangnan.github.io/categories/java/"/>
    
      <category term="network" scheme="https://phoenixjiangnan.github.io/categories/java/network/"/>
    
    
      <category term="tcp" scheme="https://phoenixjiangnan.github.io/tags/tcp/"/>
    
      <category term="socket" scheme="https://phoenixjiangnan.github.io/tags/socket/"/>
    
      <category term="serversocket" scheme="https://phoenixjiangnan.github.io/tags/serversocket/"/>
    
  </entry>
  
  <entry>
    <title>Java Concurrency - wait(), notify(), and notifyAll()</title>
    <link href="https://phoenixjiangnan.github.io/2016/09/17/java/concurrency/Java-Concurrency-wait-notify-and-notifyAll/"/>
    <id>https://phoenixjiangnan.github.io/2016/09/17/java/concurrency/Java-Concurrency-wait-notify-and-notifyAll/</id>
    <published>2016-09-17T17:57:30.000Z</published>
    <updated>2016-09-30T06:53:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="How-to-Work-With-wait-notify-and-notifyAll-in-Java"><a href="#How-to-Work-With-wait-notify-and-notifyAll-in-Java" class="headerlink" title="How to Work With wait(), notify() and notifyAll() in Java?"></a>How to Work With <code>wait()</code>, <code>notify()</code> and <code>notifyAll()</code> in Java?</h2><p>Multithreading in java is pretty complex topic and requires a lot of attention while writing application code dealing with multiple threads accessing one/more shared resources at any given time.</p>
<p>Java 5, introduced some classes like <code>BlockingQueue</code> and <code>Executors</code> which take away some of the complexity by providing easy to use APIs. Programmers using these classes will feel a lot more confident than programmers directly handling synchronization stuff using <code>wait()</code> and <code>notify()</code> method calls.</p>
<p>I will also recommend to use these newer APIs over synchronization yourself, BUT many times we are required to do so for various reasons e.g. maintaining legacy code. A good knowledge around these methods will help you in such situation when arrived.</p>
<p>I am discussing some concepts around methods <code>wait()</code>, <code>notify()</code> and <code>notifyAll()</code>.</p>
<h2 id="What-are-wait-notify-and-notifyAll-methods"><a href="#What-are-wait-notify-and-notifyAll-methods" class="headerlink" title="What are wait(), notify() and notifyAll() methods?"></a>What are <code>wait()</code>, <code>notify()</code> and <code>notifyAll()</code> methods?</h2><p>Before moving into concepts, lets note down a few very basic definitions involved for these methods.</p>
<p>The <code>Object</code> class in Java has three final methods that allow threads to communicate about the locked status of a resource. These are :</p>
<hr>
<ul>
<li><p><code>wait()</code> : </p>
<blockquote>
<p>It tells the calling thread to give up the lock and go to sleep until some other thread enters the same monitor and calls notify().</p>
<p>The <code>wait()</code> method <code>releases the lock prior to waiting</code> and <code>re-acquires the lock prior to returning from the wait() method</code>.</p>
<p>The <code>wait()</code> method is actually tightly integrated with the synchronization lock, using a feature not available directly from the synchronization mechanism. In other words, it is not possible for us to implement the <code>wait()</code> method purely in Java: it is a <code>native method</code>.</p>
</blockquote>
<p>  General syntax for calling <code>wait()</code> method is like this:</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">synchronized</span>( lockObject ) &#123; </div><div class="line">    <span class="keyword">while</span>( ! condition )</div><div class="line">    &#123; </div><div class="line">        lockObject.wait();</div><div class="line">    &#125;</div><div class="line">     </div><div class="line">    <span class="comment">//take the action here;</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<ul>
<li><p><code>notify()</code> : </p>
<blockquote>
<p>It wakes up one single thread that called <code>wait()</code> on the same object. </p>
<p>It should be noted that calling <code>notify()</code> does not actually give up a lock on a resource. <code>It tells a waiting thread that that thread can wake up.</code> <code>However, the lock is not actually given up until the notifier’s synchronized block has completed.</code></p>
</blockquote>
<p>  So, if a notifier calls <code>notify()</code> on a resource but the notifier still needs to perform 10 seconds of actions on the resource within its synchronized block, the thread that had been waiting will need to wait at least another additional 10 seconds for the notifier to release the lock on the object, even though <code>notify()</code> had been called.</p>
<p>  <code>General syntax for calling</code>notify()` method is like this:</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">synchronized</span>(lockObject) &#123;</div><div class="line">    <span class="comment">//establish_the_condition;</span></div><div class="line"> </div><div class="line">    lockObject.notify();</div><div class="line">     </div><div class="line">    <span class="comment">//any additional code if needed</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<ul>
<li><p><code>notifyAll()</code> : </p>
<blockquote>
<p>It wakes up all the threads that called <code>wait()</code> on the same object. The <code>highest priority thread</code> will run first in most of the situation, <code>though not guaranteed</code>. Other things are same as <code>notify()</code> method above.</p>
</blockquote>
<p>  General syntax for calling notify() method is like this:</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">synchronized</span>(lockObject) &#123;</div><div class="line">    establish_the_condition;</div><div class="line"> </div><div class="line">    lockObject.notifyAll();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<a id="more"></a>
<h2 id="Practice-Counting-Semaphore"><a href="#Practice-Counting-Semaphore" class="headerlink" title="Practice: Counting Semaphore"></a>Practice: Counting Semaphore</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBoundedSemaphore</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> signal;</div><div class="line">    <span class="keyword">int</span> capacity;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MyBoundedSemaphore</span><span class="params">(<span class="keyword">int</span> cap)</span> </span>&#123;</div><div class="line">        signal = <span class="number">0</span>;</div><div class="line">        capacity = cap;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/*</span></div><div class="line">    * Notice how the acquire() method now blocks if the number of signals is equal to the upper bound.</div><div class="line">    *</div><div class="line">    * Not until a thread has called release() will the thread calling take() be allowed to deliver its signal,</div><div class="line">    * if the BoundedSemaphore has reached its upper signal limit.</div><div class="line">    * </div><div class="line">    * If the queue size is not equal to either bound when enqueue() or dequeue() is called,</div><div class="line">    * there can be no threads waiting to either enqueue or dequeue items.</div><div class="line">    * */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">acquire</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</div><div class="line">        <span class="keyword">while</span>(signal == capacity) &#123;</div><div class="line">            wait();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span>(signal == <span class="number">0</span>) &#123;</div><div class="line">            notifyAll();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        signal ++;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">release</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</div><div class="line">        <span class="keyword">while</span>(signal == <span class="number">0</span>) &#123;</div><div class="line">            wait();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span>(signal == capacity) &#123;</div><div class="line">            notifyAll();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        signal --;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>For more details about <code>Semaphore</code>, please read <a href="https://phoenixjiangnan.github.io/2016/04/03/java/concurrency/Java-Concurrency-Semaphore/">https://phoenixjiangnan.github.io/2016/04/03/java/concurrency/Java-Concurrency-Semaphore/</a></p>
<h2 id="Practice-Blocking-Queue"><a href="#Practice-Blocking-Queue" class="headerlink" title="Practice: Blocking Queue"></a>Practice: Blocking Queue</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBlockingQueue</span>&lt;<span class="title">T</span>&gt; </span>&#123;</div><div class="line">    <span class="keyword">private</span> List&lt;T&gt; queue;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">int</span> limit = <span class="number">10</span>;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MyBlockingQueue</span><span class="params">(<span class="keyword">int</span> limit)</span> </span>&#123;</div><div class="line">        queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</div><div class="line">        <span class="keyword">this</span>.limit = limit;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/*</span></div><div class="line">    * Notice how notifyAll() is only called from enqueue() and dequeue() if the queue size is equal</div><div class="line">    * to the size bounds (0 or limit).</div><div class="line">    *</div><div class="line">    * If the queue size is not equal to either bound when enqueue() or dequeue() is called,</div><div class="line">    * there can be no threads waiting to either enqueue or dequeue items.</div><div class="line">    * */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">enqueue</span><span class="params">(T t)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</div><div class="line">        <span class="keyword">while</span>(queue.size() == limit) &#123;</div><div class="line">            wait();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span>(queue.size() == <span class="number">0</span>) &#123;</div><div class="line">            notifyAll();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        queue.add(t);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> T <span class="title">dequeue</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</div><div class="line">        <span class="keyword">while</span>(queue.size() == <span class="number">0</span>) &#123;</div><div class="line">            wait();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">if</span>(queue.size() == limit) &#123;</div><div class="line">            notifyAll();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">return</span> queue.remove(<span class="number">0</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Practice-Thread-Pool"><a href="#Practice-Thread-Pool" class="headerlink" title="Practice: Thread Pool"></a>Practice: Thread Pool</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyThreadPool</span> </span>&#123;</div><div class="line">    <span class="keyword">private</span> MyBlockingQueue&lt;Runnable&gt; myqueue;</div><div class="line">    <span class="keyword">private</span> List&lt;MyThread&gt; threads;</div><div class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> isStopped;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MyThreadPool</span><span class="params">(<span class="keyword">int</span> threadCount, <span class="keyword">int</span> maxNoOfTasks)</span> </span>&#123;</div><div class="line">        myqueue = <span class="keyword">new</span> MyBlockingQueue&lt;&gt;(maxNoOfTasks);</div><div class="line"></div><div class="line">        threads = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; threadCount; i ++) &#123;</div><div class="line">            threads.add(<span class="keyword">new</span> MyThread(myqueue));</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="keyword">for</span>(MyThread thread : threads) &#123;</div><div class="line">            thread.start();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Runnable task)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</div><div class="line">        <span class="keyword">if</span>(isStopped) &#123;</div><div class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"ThreadPool is stopped"</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        myqueue.enqueue(task);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">stop</span><span class="params">()</span> </span>&#123;</div><div class="line">        isStopped = <span class="keyword">true</span>;</div><div class="line">        <span class="keyword">for</span>(MyThread pt : threads) &#123;</div><div class="line">            pt.doStop();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</div><div class="line">        <span class="keyword">private</span> MyBlockingQueue&lt;Runnable&gt; taskQueue;</div><div class="line">        <span class="keyword">private</span> <span class="keyword">boolean</span> isStopped;</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="title">MyThread</span><span class="params">(MyBlockingQueue&lt;Runnable&gt; blockingQueue)</span> </span>&#123;</div><div class="line">            taskQueue = blockingQueue;</div><div class="line">            isStopped = <span class="keyword">false</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">            <span class="keyword">while</span>(!isStopped) &#123;</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                    Runnable r = taskQueue.dequeue();</div><div class="line">                    r.run();</div><div class="line">                &#125; <span class="keyword">catch</span>(Exception e) &#123;</div><div class="line">                    <span class="comment">// log</span></div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">doStop</span><span class="params">()</span> </span>&#123;</div><div class="line">            isStopped = <span class="keyword">true</span>;</div><div class="line">            <span class="keyword">this</span>.interrupt();</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">isStopped</span><span class="params">()</span> </span>&#123;</div><div class="line">            <span class="keyword">return</span> isStopped;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<hr>
<p>References:</p>
<ul>
<li><a href="http://www.programcreek.com/2009/02/notify-and-wait-example/" target="_blank" rel="external">http://www.programcreek.com/2009/02/notify-and-wait-example/</a></li>
<li><a href="http://howtodoinjava.com/core-java/multi-threading/how-to-work-with-wait-notify-and-notifyall-in-java/" target="_blank" rel="external">http://howtodoinjava.com/core-java/multi-threading/how-to-work-with-wait-notify-and-notifyall-in-java/</a></li>
<li><a href="http://tutorials.jenkov.com/java-concurrency/blocking-queues.html" target="_blank" rel="external">http://tutorials.jenkov.com/java-concurrency/blocking-queues.html</a></li>
<li><a href="http://tutorials.jenkov.com/java-concurrency/thread-pools.html" target="_blank" rel="external">http://tutorials.jenkov.com/java-concurrency/thread-pools.html</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;How-to-Work-With-wait-notify-and-notifyAll-in-Java&quot;&gt;&lt;a href=&quot;#How-to-Work-With-wait-notify-and-notifyAll-in-Java&quot; class=&quot;headerlink&quot; title=&quot;How to Work With wait(), notify() and notifyAll() in Java?&quot;&gt;&lt;/a&gt;How to Work With &lt;code&gt;wait()&lt;/code&gt;, &lt;code&gt;notify()&lt;/code&gt; and &lt;code&gt;notifyAll()&lt;/code&gt; in Java?&lt;/h2&gt;&lt;p&gt;Multithreading in java is pretty complex topic and requires a lot of attention while writing application code dealing with multiple threads accessing one/more shared resources at any given time.&lt;/p&gt;
&lt;p&gt;Java 5, introduced some classes like &lt;code&gt;BlockingQueue&lt;/code&gt; and &lt;code&gt;Executors&lt;/code&gt; which take away some of the complexity by providing easy to use APIs. Programmers using these classes will feel a lot more confident than programmers directly handling synchronization stuff using &lt;code&gt;wait()&lt;/code&gt; and &lt;code&gt;notify()&lt;/code&gt; method calls.&lt;/p&gt;
&lt;p&gt;I will also recommend to use these newer APIs over synchronization yourself, BUT many times we are required to do so for various reasons e.g. maintaining legacy code. A good knowledge around these methods will help you in such situation when arrived.&lt;/p&gt;
&lt;p&gt;I am discussing some concepts around methods &lt;code&gt;wait()&lt;/code&gt;, &lt;code&gt;notify()&lt;/code&gt; and &lt;code&gt;notifyAll()&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&quot;What-are-wait-notify-and-notifyAll-methods&quot;&gt;&lt;a href=&quot;#What-are-wait-notify-and-notifyAll-methods&quot; class=&quot;headerlink&quot; title=&quot;What are wait(), notify() and notifyAll() methods?&quot;&gt;&lt;/a&gt;What are &lt;code&gt;wait()&lt;/code&gt;, &lt;code&gt;notify()&lt;/code&gt; and &lt;code&gt;notifyAll()&lt;/code&gt; methods?&lt;/h2&gt;&lt;p&gt;Before moving into concepts, lets note down a few very basic definitions involved for these methods.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Object&lt;/code&gt; class in Java has three final methods that allow threads to communicate about the locked status of a resource. These are :&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;wait()&lt;/code&gt; : &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It tells the calling thread to give up the lock and go to sleep until some other thread enters the same monitor and calls notify().&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;wait()&lt;/code&gt; method &lt;code&gt;releases the lock prior to waiting&lt;/code&gt; and &lt;code&gt;re-acquires the lock prior to returning from the wait() method&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;wait()&lt;/code&gt; method is actually tightly integrated with the synchronization lock, using a feature not available directly from the synchronization mechanism. In other words, it is not possible for us to implement the &lt;code&gt;wait()&lt;/code&gt; method purely in Java: it is a &lt;code&gt;native method&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;  General syntax for calling &lt;code&gt;wait()&lt;/code&gt; method is like this:&lt;/p&gt;
  &lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;synchronized&lt;/span&gt;( lockObject ) &amp;#123; &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;while&lt;/span&gt;( ! condition )&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;#123; &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        lockObject.wait();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;     &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;//take the action here;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;notify()&lt;/code&gt; : &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It wakes up one single thread that called &lt;code&gt;wait()&lt;/code&gt; on the same object. &lt;/p&gt;
&lt;p&gt;It should be noted that calling &lt;code&gt;notify()&lt;/code&gt; does not actually give up a lock on a resource. &lt;code&gt;It tells a waiting thread that that thread can wake up.&lt;/code&gt; &lt;code&gt;However, the lock is not actually given up until the notifier’s synchronized block has completed.&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;  So, if a notifier calls &lt;code&gt;notify()&lt;/code&gt; on a resource but the notifier still needs to perform 10 seconds of actions on the resource within its synchronized block, the thread that had been waiting will need to wait at least another additional 10 seconds for the notifier to release the lock on the object, even though &lt;code&gt;notify()&lt;/code&gt; had been called.&lt;/p&gt;
&lt;p&gt;  &lt;code&gt;General syntax for calling&lt;/code&gt;notify()` method is like this:&lt;/p&gt;
  &lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;synchronized&lt;/span&gt;(lockObject) &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;//establish_the_condition;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt; &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    lockObject.notify();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;     &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;//any additional code if needed&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;notifyAll()&lt;/code&gt; : &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It wakes up all the threads that called &lt;code&gt;wait()&lt;/code&gt; on the same object. The &lt;code&gt;highest priority thread&lt;/code&gt; will run first in most of the situation, &lt;code&gt;though not guaranteed&lt;/code&gt;. Other things are same as &lt;code&gt;notify()&lt;/code&gt; method above.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;  General syntax for calling notify() method is like this:&lt;/p&gt;
  &lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;synchronized&lt;/span&gt;(lockObject) &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    establish_the_condition;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt; &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    lockObject.notifyAll();&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Apache Thrift - Introduction</title>
    <link href="https://phoenixjiangnan.github.io/2016/09/16/distributed%20system/thrift/Apache-Thrift-Introduction/"/>
    <id>https://phoenixjiangnan.github.io/2016/09/16/distributed system/thrift/Apache-Thrift-Introduction/</id>
    <published>2016-09-17T00:17:06.000Z</published>
    <updated>2016-09-17T00:19:29.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="What-is-Thrift"><a href="#What-is-Thrift" class="headerlink" title="What is Thrift"></a>What is Thrift</h2><p>According to <a href="https://thrift.apache.org/" target="_blank" rel="external">Apache Thrift</a>, </p>
<blockquote>
<p>The Apache Thrift software framework, for <code>scalable cross-language services development</code>, combines a software stack with <code>a code generation engine</code> to build services that work efficiently and seamlessly between C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js, Smalltalk, OCaml and Delphi and other languages.</p>
</blockquote>
<p>It’s originated from Facebook, and there’s a <a href="https://thrift.apache.org/static/files/thrift-20070401.pdf" target="_blank" rel="external">paper</a> about it. According to the <em>Abstract</em> of the paper, </p>
<pre><code>------

&quot;Thrift is a software library and set of code-generation tools developed at Facebook to expedite development and implementation of efficient and scalable backend services.

Its primary goal is to ###enable efficient and reliable communication across programming languages### by abstracting the portions of each language that tend to require the most customization into a common library that is implemented in each language.

Specifically, Thrift allows developers to define `datatypes` and `service interfaces` in a single language-neutral file and generate all the necessary code to build RPC clients and servers.&quot;

------
</code></pre><p>As far as I understand, Thrift is a high-performant RPC framework with the following characteristics:</p>
<blockquote>
<ol>
<li>A high-performant encode and decode framework based on bytes</li>
<li>Bottom communication based on NIO</li>
<li>Relative easy service call model</li>
<li>Use Interface Description Language (IDL) to support cross-platform calls</li>
<li>A high performant binary encoding/decoding framework</li>
</ol>
</blockquote>
<h2 id="Thrift-Network-Stack"><a href="#Thrift-Network-Stack" class="headerlink" title="Thrift Network Stack"></a>Thrift Network Stack</h2><p>According to the model, Thrift is composed of the following core components:</p>
<ul>
<li>TServer and Client</li>
<li>TProcessor - a generic object which operates upon an input stream and writes to some output stream.</li>
<li>TProtocol - protocol interface that encode/decode data</li>
<li>TTransport - encapsulates the I/O layer. Basically a thin wrapper around the combined functionality of Java input/output streams.</li>
<li>The Thrift Interface Definition Language (IDL) - describes the service, and generates cross-platform clients</li>
</ul>
<p>Here’s a simple representation of the Apache Thrift networking stack from <a href="https://thrift.apache.org/docs/concepts" target="_blank" rel="external">https://thrift.apache.org/docs/concepts</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">+-------------------------------------------+</div><div class="line">| Transport                                 |</div><div class="line">| (raw TCP, HTTP etc)                       |</div><div class="line">+-------------------------------------------+</div><div class="line">| Protocol                                  |</div><div class="line">| (JSON, compact etc)                       |</div><div class="line">+-------------------------------------------+</div><div class="line">| Processor                                 |</div><div class="line">| (compiler generated)                      |</div><div class="line">+-------------------------------------------+</div><div class="line">| Server                                    |</div><div class="line">| (single-threaded, event-driven etc)       |</div><div class="line">+-------------------------------------------+</div></pre></td></tr></table></figure>
<hr>
<p>Reference: RPC framework basic components</p>
<p><img src="http://img.blog.csdn.net/20150108170231000?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbWluZGZsb2F0aW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="RPC framework basic components"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;What-is-Thrift&quot;&gt;&lt;a href=&quot;#What-is-Thrift&quot; class=&quot;headerlink&quot; title=&quot;What is Thrift&quot;&gt;&lt;/a&gt;What is Thrift&lt;/h2&gt;&lt;p&gt;According to &lt;a href=&quot;
    
    </summary>
    
      <category term="distributed system" scheme="https://phoenixjiangnan.github.io/categories/distributed-system/"/>
    
      <category term="thrift" scheme="https://phoenixjiangnan.github.io/categories/distributed-system/thrift/"/>
    
    
      <category term="thrift" scheme="https://phoenixjiangnan.github.io/tags/thrift/"/>
    
  </entry>
  
  <entry>
    <title>ZooKeeper - Sessions and Session Management</title>
    <link href="https://phoenixjiangnan.github.io/2016/09/15/distributed%20system/zookeeper/ZooKeeper-Sessions-and-Session-Management/"/>
    <id>https://phoenixjiangnan.github.io/2016/09/15/distributed system/zookeeper/ZooKeeper-Sessions-and-Session-Management/</id>
    <published>2016-09-15T22:14:21.000Z</published>
    <updated>2016-09-15T22:16:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>A ZooKeeper client establishes a <code>session</code> with the ZooKeeper service by creating a handle to the service using a language binding.</p>
<h2 id="Session-States"><a href="#Session-States" class="headerlink" title="Session States"></a>Session States</h2><p>Once created, the handle starts of in the <code>CONNECTING</code> state and the client library tries to connect to one of the servers that make up the ZooKeeper service at which point it switches to the <code>CONNECTED</code> state.</p>
<p>During normal operation will be in one of these two states. If an unrecoverable error occurs, such as session expiration or authentication failure, or if the application explicitly closes the handle, the handle will move to the <code>CLOSED</code> state.</p>
<p>The following figure shows the possible state transitions of a ZooKeeper client:</p>
<p><img src="https://zookeeper.apache.org/doc/trunk/images/state_dia.jpg" alt="zookeeper session"></p>
<h2 id="How-to-Create-A-Client-Session"><a href="#How-to-Create-A-Client-Session" class="headerlink" title="How to Create A Client Session"></a>How to Create A Client Session</h2><p>To create a client session, the application code must provide a string <code>(connection string)</code> containing a comma separated list of <code>host:port</code> pairs, each corresponding to a ZooKeeper server (e.g. <code>&quot;127.0.0.1:4545&quot;</code> or <code>&quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;</code>).</p>
<hr>
<pre><code>Added in 3.2.0: 
</code></pre><p>An optional <code>chroot</code> suffix may also be appended to the connection string. This will run the client commands while interpreting all paths relative to this root (similar to the unix chroot command). If used the example would look like: <code>127.0.0.1:4545/app/a</code> or <code>127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002/app/a</code> where the client would be rooted at “/app/a” and all paths would be relative to this root - ie getting/setting/etc… <code>/foo/bar</code> would result in operations being run on <code>/app/a/foo/bar</code> (from the server perspective). </p>
<p>This feature is particularly useful <code>in multi-tenant environments</code> where each user of a particular ZooKeeper service could be rooted differently. This makes re-use much simpler as each user can code his/her application as if it were rooted at <code>/</code>, while actual location (say <code>/app/a</code>) could be determined at deployment time.</p>
<hr>
<h2 id="What-Happens-in-Session-Creation"><a href="#What-Happens-in-Session-Creation" class="headerlink" title="What Happens in Session Creation"></a>What Happens in Session Creation</h2><p>The ZooKeeper client library will pick an arbitrary server <code>(Note! Not necessarily the local server if there&#39;s one running along the client!)</code> and try to connect to it. If this connection fails, or if the client becomes disconnected from the server for any reason, the client will automatically try the next server in the list, until a connection is (re-)established.</p>
<h3 id="Server-creates-Session-Id"><a href="#Server-creates-Session-Id" class="headerlink" title="Server creates Session Id"></a>Server creates <code>Session Id</code></h3><p>When a client gets a handle to the ZooKeeper service, ZooKeeper creates a ZooKeeper <code>session</code>, <code>represented as a 64-bit number</code>, that it assigns to the client.</p>
<h3 id="Server-creates-password-and-send-it-to-client"><a href="#Server-creates-password-and-send-it-to-client" class="headerlink" title="Server creates password and send it to client"></a>Server creates <code>password</code> and send it to client</h3><p>If the client connects to a different ZooKeeper server, it will send the <code>session id</code> as a part of the connection handshake. As a security measure, the server creates a <code>password</code> for the <code>session id</code> that any ZooKeeper server can validate.The password is sent to the client with the session id when the client establishes the session. The client sends this password with the session id whenever it reestablishes the session with a new server.</p>
<h3 id="Client-specifies-timeout"><a href="#Client-specifies-timeout" class="headerlink" title="Client specifies timeout"></a>Client specifies timeout</h3><p>One of the parameters to the ZooKeeper client library call to create a ZooKeeper session is <code>the session timeout in milliseconds</code>. The client sends a requested timeout, the server responds with the timeout that it can give the client. The current implementation requires that the timeout be <code>a minimum of 2 times the tickTime</code> (as set in the server configuration) and <code>a maximum of 20 times the tickTime</code>.</p>
<a id="more"></a>
<h3 id="Session-re-establishment"><a href="#Session-re-establishment" class="headerlink" title="Session re-establishment"></a>Session re-establishment</h3><p>When a client (session) becomes partitioned from the ZK serving cluster it will begin searching the list of servers that were specified during session creation. Eventually, when connectivity between the client and at least one of the servers is re-established, the session will either again transition to the <code>connected</code> state (if reconnected within the session timeout value) or it will transition to the <code>expired</code> state (if reconnected after the session timeout).</p>
<p>It is not advisable to create a new session object (a new <code>ZooKeeper.class</code> or zookeeper handle in the c binding) for disconnection. The ZK client library will handle reconnect for you. In particular we have heuristics built into the client library to handle things like <code>herd effect</code>, etc… Only create a new session when you are notified of session expiration (mandatory).</p>
<h3 id="Session-Expiration"><a href="#Session-Expiration" class="headerlink" title="Session Expiration"></a>Session Expiration</h3><pre><code>Session expiration is managed by the ZooKeeper cluster itself, not by the client.
</code></pre><p>When the ZK client establishes a session with the cluster it provides a <code>timeout</code> value detailed above. This value is used by the cluster to determine when the client’s session expires.</p>
<pre><code>Expirations happens when the cluster does not hear from the client within the specified session timeout period (i.e. no heartbeat).
</code></pre><p>At session expiration the cluster will delete any/all ephemeral nodes owned by that session and immediately notify any/all connected clients of the change (anyone watching those znodes).</p>
<blockquote>
<p>At this point the client of the expired session is still disconnected from the cluster, it will not be notified of the session expiration until/unless it is able to re-establish a connection to the cluster. The client will stay in disconnected state until the TCP connection is re-established with the cluster, at which point the watcher of the expired session will receive the <code>session expired</code> notification.</p>
</blockquote>
<p>Example state transitions for an expired session as seen by the expired session’s watcher:</p>
<ol>
<li><code>connected</code> : session is established and client is communicating with cluster (client/server communication is operating properly)</li>
<li>…. client is partitioned from the cluster</li>
<li><code>disconnected</code> : client has lost connectivity with the cluster</li>
<li>…. time elapses, after ‘timeout’ period the cluster expires the session, nothing is seen by client as it is disconnected from cluster</li>
<li>…. time elapses, the client regains network level connectivity with the cluster</li>
<li><code>expired</code> : eventually the client reconnects to the cluster, it is then notified of the expiration</li>
</ol>
<h2 id="Default-Watcher"><a href="#Default-Watcher" class="headerlink" title="Default Watcher"></a>Default Watcher</h2><p>Another parameter to the ZooKeeper session establishment call is <code>the default watcher</code>. Watchers are notified when any state change occurs in the client. For example if the client loses connectivity to the server the client will be notified, or if the client’s session expires, etc…</p>
<p>This watcher should consider the initial state to be <code>disconnected</code> (i.e. before any state changes events are sent to the watcher by the client lib). In the case of a new connection, the first event sent to the watcher is typically the session connection event.</p>
<h3 id="Client-Heartbeat"><a href="#Client-Heartbeat" class="headerlink" title="Client Heartbeat"></a>Client Heartbeat</h3><p>The session is kept alive by requests sent by the client. If the session is idle for a period of time that would timeout the session, the client will send <code>a PING request</code> to keep the session alive.</p>
<p>This PING request not only allows the ZooKeeper server to know that the client is still active, but it also allows the client to verify that its connection to the ZooKeeper server is still active. The timing of the PING is conservative enough to ensure reasonable time to detect a dead connection and reconnect to a new server.</p>
<p>Once a connection to the server is successfully established <code>connected</code> there are basically two cases where the client lib generates <code>connectionloss</code> (the result code in c binding, exception in Java – see the API documentation for binding specific details) when either a synchronous or asynchronous operation is performed and one of the following holds:</p>
<ul>
<li>The application calls an operation on a session that is no longer alive/valid</li>
<li>The ZooKeeper client disconnects from a server when there are pending operations to that server, i.e., there is a pending asynchronous call.</li>
</ul>
<hr>
<pre><code>Added in 3.2.0 -- SessionMovedException.
</code></pre><p>There is an internal exception that is generally not seen by clients called the <code>SessionMovedException</code>. This exception occurs because a request was received on a connection for a session which has been reestablished on a different server. The normal cause of this error is a client that sends a request to a server, but the network packet gets delayed, so the client times out and connects to a new server. When the delayed packet arrives at the first server, the old server detects that the session has moved, and closes the client connection.</p>
<p>Clients normally do not see this error since they do not read from those old connections. (Old connections are usually closed.) One situation in which this condition can be seen is when two clients try to reestablish the same connection using a saved session id and password. One of the clients will reestablish the connection and the second client will be disconnected (causing the pair to attempt to re-establish its connection/session indefinitely).</p>
<hr>
<h2 id="Updating-the-List-of-Servers"><a href="#Updating-the-List-of-Servers" class="headerlink" title="Updating the List of Servers"></a>Updating the List of Servers</h2><p>Updating the list of servers. We allow a client to update the connection string by providing a new comma separated list of <code>host:port</code> pairs, each corresponding to a ZooKeeper server.</p>
<p>The function invokes a probabilistic load-balancing algorithm which may cause the client to disconnect from its current host with the goal to achieve expected uniform number of connections per server in the new list. In case the current host to which the client is connected is not in the new list this call will always cause the connection to be dropped. Otherwise, the decision is based on whether the number of servers has increased or decreased and by how much.</p>
<p>For example, if the previous connection string contained 3 hosts and now the list contains these 3 hosts and 2 more hosts, 40% of clients connected to each of the 3 hosts will move to one of the new hosts in order to balance the load. The algorithm will cause the client to drop its connection to the current host to which it is connected with probability 0.4 and in this case cause the client to connect to one of the 2 new hosts, chosen at random.</p>
<p>Another example – suppose we have 5 hosts and now update the list to remove 2 of the hosts, the clients connected to the 3 remaining hosts will stay connected, whereas all clients connected to the 2 removed hosts will need to move to one of the 3 hosts, chosen at random. If the connection is dropped, the client moves to a special mode where he chooses a new server to connect to using the probabilistic algorithm, and not just round robin.</p>
<p>In the first example, each client decides to disconnect with probability 0.4 but once the decision is made, it will try to connect to a random new server and only if it cannot connect to any of the new servers will it try to connect to the old ones. After finding a server, or trying all servers in the new list and failing to connect, the client moves back to the normal mode of operation where it picks an arbitrary server from the <code>connectString</code> and attempt to connect to it. If that fails, is will continue trying different random servers in round robin. (see above the algorithm used to initially choose a server)</p>
<hr>
<p>Reference: <a href="https://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#ch_zkSessions" target="_blank" rel="external">https://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#ch_zkSessions</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A ZooKeeper client establishes a &lt;code&gt;session&lt;/code&gt; with the ZooKeeper service by creating a handle to the service using a language binding.&lt;/p&gt;
&lt;h2 id=&quot;Session-States&quot;&gt;&lt;a href=&quot;#Session-States&quot; class=&quot;headerlink&quot; title=&quot;Session States&quot;&gt;&lt;/a&gt;Session States&lt;/h2&gt;&lt;p&gt;Once created, the handle starts of in the &lt;code&gt;CONNECTING&lt;/code&gt; state and the client library tries to connect to one of the servers that make up the ZooKeeper service at which point it switches to the &lt;code&gt;CONNECTED&lt;/code&gt; state.&lt;/p&gt;
&lt;p&gt;During normal operation will be in one of these two states. If an unrecoverable error occurs, such as session expiration or authentication failure, or if the application explicitly closes the handle, the handle will move to the &lt;code&gt;CLOSED&lt;/code&gt; state.&lt;/p&gt;
&lt;p&gt;The following figure shows the possible state transitions of a ZooKeeper client:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://zookeeper.apache.org/doc/trunk/images/state_dia.jpg&quot; alt=&quot;zookeeper session&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;How-to-Create-A-Client-Session&quot;&gt;&lt;a href=&quot;#How-to-Create-A-Client-Session&quot; class=&quot;headerlink&quot; title=&quot;How to Create A Client Session&quot;&gt;&lt;/a&gt;How to Create A Client Session&lt;/h2&gt;&lt;p&gt;To create a client session, the application code must provide a string &lt;code&gt;(connection string)&lt;/code&gt; containing a comma separated list of &lt;code&gt;host:port&lt;/code&gt; pairs, each corresponding to a ZooKeeper server (e.g. &lt;code&gt;&amp;quot;127.0.0.1:4545&amp;quot;&lt;/code&gt; or &lt;code&gt;&amp;quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&amp;quot;&lt;/code&gt;).&lt;/p&gt;
&lt;hr&gt;
&lt;pre&gt;&lt;code&gt;Added in 3.2.0: 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;An optional &lt;code&gt;chroot&lt;/code&gt; suffix may also be appended to the connection string. This will run the client commands while interpreting all paths relative to this root (similar to the unix chroot command). If used the example would look like: &lt;code&gt;127.0.0.1:4545/app/a&lt;/code&gt; or &lt;code&gt;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002/app/a&lt;/code&gt; where the client would be rooted at “/app/a” and all paths would be relative to this root - ie getting/setting/etc… &lt;code&gt;/foo/bar&lt;/code&gt; would result in operations being run on &lt;code&gt;/app/a/foo/bar&lt;/code&gt; (from the server perspective). &lt;/p&gt;
&lt;p&gt;This feature is particularly useful &lt;code&gt;in multi-tenant environments&lt;/code&gt; where each user of a particular ZooKeeper service could be rooted differently. This makes re-use much simpler as each user can code his/her application as if it were rooted at &lt;code&gt;/&lt;/code&gt;, while actual location (say &lt;code&gt;/app/a&lt;/code&gt;) could be determined at deployment time.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;What-Happens-in-Session-Creation&quot;&gt;&lt;a href=&quot;#What-Happens-in-Session-Creation&quot; class=&quot;headerlink&quot; title=&quot;What Happens in Session Creation&quot;&gt;&lt;/a&gt;What Happens in Session Creation&lt;/h2&gt;&lt;p&gt;The ZooKeeper client library will pick an arbitrary server &lt;code&gt;(Note! Not necessarily the local server if there&amp;#39;s one running along the client!)&lt;/code&gt; and try to connect to it. If this connection fails, or if the client becomes disconnected from the server for any reason, the client will automatically try the next server in the list, until a connection is (re-)established.&lt;/p&gt;
&lt;h3 id=&quot;Server-creates-Session-Id&quot;&gt;&lt;a href=&quot;#Server-creates-Session-Id&quot; class=&quot;headerlink&quot; title=&quot;Server creates Session Id&quot;&gt;&lt;/a&gt;Server creates &lt;code&gt;Session Id&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;When a client gets a handle to the ZooKeeper service, ZooKeeper creates a ZooKeeper &lt;code&gt;session&lt;/code&gt;, &lt;code&gt;represented as a 64-bit number&lt;/code&gt;, that it assigns to the client.&lt;/p&gt;
&lt;h3 id=&quot;Server-creates-password-and-send-it-to-client&quot;&gt;&lt;a href=&quot;#Server-creates-password-and-send-it-to-client&quot; class=&quot;headerlink&quot; title=&quot;Server creates password and send it to client&quot;&gt;&lt;/a&gt;Server creates &lt;code&gt;password&lt;/code&gt; and send it to client&lt;/h3&gt;&lt;p&gt;If the client connects to a different ZooKeeper server, it will send the &lt;code&gt;session id&lt;/code&gt; as a part of the connection handshake. As a security measure, the server creates a &lt;code&gt;password&lt;/code&gt; for the &lt;code&gt;session id&lt;/code&gt; that any ZooKeeper server can validate.The password is sent to the client with the session id when the client establishes the session. The client sends this password with the session id whenever it reestablishes the session with a new server.&lt;/p&gt;
&lt;h3 id=&quot;Client-specifies-timeout&quot;&gt;&lt;a href=&quot;#Client-specifies-timeout&quot; class=&quot;headerlink&quot; title=&quot;Client specifies timeout&quot;&gt;&lt;/a&gt;Client specifies timeout&lt;/h3&gt;&lt;p&gt;One of the parameters to the ZooKeeper client library call to create a ZooKeeper session is &lt;code&gt;the session timeout in milliseconds&lt;/code&gt;. The client sends a requested timeout, the server responds with the timeout that it can give the client. The current implementation requires that the timeout be &lt;code&gt;a minimum of 2 times the tickTime&lt;/code&gt; (as set in the server configuration) and &lt;code&gt;a maximum of 20 times the tickTime&lt;/code&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="distributed system" scheme="https://phoenixjiangnan.github.io/categories/distributed-system/"/>
    
      <category term="zookeeper" scheme="https://phoenixjiangnan.github.io/categories/distributed-system/zookeeper/"/>
    
    
      <category term="zookeepr session" scheme="https://phoenixjiangnan.github.io/tags/zookeepr-session/"/>
    
      <category term="zookeeper client session" scheme="https://phoenixjiangnan.github.io/tags/zookeeper-client-session/"/>
    
  </entry>
  
  <entry>
    <title>How did I develop a crypto service for Tableau Server</title>
    <link href="https://phoenixjiangnan.github.io/2016/09/14/system%20design/project%20experience/How-did-I-develop-a-crypto-service-for-Tableau-Server/"/>
    <id>https://phoenixjiangnan.github.io/2016/09/14/system design/project experience/How-did-I-develop-a-crypto-service-for-Tableau-Server/</id>
    <published>2016-09-15T05:23:35.000Z</published>
    <updated>2016-09-30T06:52:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>For more information about Tableau Server, you can read my blogs <a href="https://phoenixjiangnan.github.io/categories/system-design/project-experience/">here</a></p>
<h2 id="What-is-this-project-about"><a href="#What-is-this-project-about" class="headerlink" title="What is this project about?"></a>What is this project about?</h2><p>Tableau Server is an on-premise data analytics and visualization platform. Customer can deploy bits in their own data center, or in AWS, Azure, and GCP.</p>
<p>There are two categories of passwords within a Tableau Server cluster:</p>
<ul>
<li><p>User input passwords, which are entered by users</p>
<ul>
<li>AWS key</li>
<li>smtp passwords</li>
<li>Windows runas user password</li>
<li>…</li>
</ul>
</li>
<li><p>Server managed passwords, which are randomly generated by server</p>
<ul>
<li>jdbc password</li>
<li>Postgres admin password</li>
<li>ZooKeeper password</li>
<li>…</li>
</ul>
</li>
</ul>
<p>So the passwords I’m talking about is not the password for someone’s server account. The latter ones live in a table of the cluster’s PostgreSQL database and are already encrypted.</p>
<p>The current version of Tableau Server (v10) stores all the passwords in <code>plain text</code> in a configuration file, which is distributed among machines.</p>
<p>The new version of Tableau Server also stores all passwords in <code>plain text</code>, but in a <code>Znode</code> in ZooKeeper, as I described in <a href="https://phoenixjiangnan.github.io/2016/08/16/system%20design/project%20experience/How-did-I-build-a-custom-dynamic-ZooKeeper-management-and-migration-framework-and-an-asynchronous-job-framework-for-Tableau-Server-11/">this blog</a>.</p>
<blockquote>
<p>The project is about developing a crypto service that encrypts passwords before persisting them somewhere, and decrypts passwords into memory when server uses them, so that no more clear text secrets live in Tableau Server. The new service should support both our old monolith JRuby app and our new service-oriented server architecture.</p>
</blockquote>
<a id="more"></a>
<h2 id="Why-are-we-developing-this-feature"><a href="#Why-are-we-developing-this-feature" class="headerlink" title="Why are we developing this feature?"></a>Why are we developing this feature?</h2><p>Tableau Server is for enterprise customers, and the top 3 things that enterprise IT cares about are SECURITY, SECURITY, SECURITY.</p>
<p>According to our sales team, keeping secrets in clear text has been one of the biggest sales blockers for Tableau Server. Our customers have also been bringing up many times that either their huge concerns w.r.t the potential password leaks, or the friction they’ve experienced to buy Tableau Server - they need to get an exception from IT since Tableau Server is the only app in house that don’t encrypt passwords.</p>
<p>In a nutshell, Tableau Server is not <code>enterprise-ready</code> as long as it doesn’t support password encryption.</p>
<h2 id="Trade-offs"><a href="#Trade-offs" class="headerlink" title="Trade-offs"></a>Trade-offs</h2><p>There’s no doubt that we are gonna build the crypto service for new Tableau Server, a service-oriented server architecture that will come out in 2017. The biggest trade-off we faced was whether to put efforts on enabling this feature in our old monolith JRuby server app.</p>
<p>The downsides of doing that are:</p>
<ul>
<li>Lots of efforts required. Our old server architecture is a monolith JRuby app with messy logic (yeah, everyone agrees on it! It’s totally messed up because people have been keeping on adding new stuff here and there for <code>9 years</code> - the ruby app started on 2007). The crypto service will be written in Java and incorporate it into old server can be really hard.</li>
<li>Uncertain schedule. Because of the large amount of work expected, no one is certain whether we can finish this feature, fully test it, and ship it on time. If it comes out too late, say only three months before new server gets shipped, it’s probably much less valuable.</li>
<li>We are not sure if it worths it. If our customers have been living with plain text passwords for 9 years, they probably can wait for the new server for another six months?</li>
<li>Developer frastruction. Not only because it’s a messy job, but also because you know your code will be deleted a year later.</li>
</ul>
<p>The upsides of doing that are:</p>
<ul>
<li>Removes the sales blocker as soon as possible</li>
<li>Relieves customers pain and complaints as soon as possible</li>
<li>Or at least demonstrate to our customers that we have heard of their requirements and we are executing on it (this is sounds very much self-comforting, but it’s been brought up serveral time. Yeah, it’s all about ATTITUDE)</li>
</ul>
<p>The team eventually decided to do it.</p>
<h2 id="Design"><a href="#Design" class="headerlink" title="Design"></a>Design</h2><p>Compared to the complicated integration of crypto service and our ruby app, the design of crypto service itself is actually much more straightforward.</p>
<p>The core idea is:</p>
<blockquote>
<p>Encrpt actual secrets with an encryption key, and encrypt the encryption key with a master key</p>
</blockquote>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">File System / Config Dir / Config File</div><div class="line">    ------yml file</div><div class="line">    | inputs</div><div class="line">    |    - master key algorithm</div><div class="line">    |    - encryption key algorithm</div><div class="line">    |    - keystore location</div><div class="line">    |</div><div class="line">    | generated values</div><div class="line">    |    - master key id</div><div class="line">    |    - encryption key</div><div class="line">    |    - encryption key id</div><div class="line">    ------</div><div class="line">    </div><div class="line">File System / Config Dir / Java KeyStore (JKS)</div><div class="line">    -----------</div><div class="line">    |  Master |</div><div class="line">    |   key   |</div><div class="line">    -----------</div></pre></td></tr></table></figure>
<h3 id="Bootstrap"><a href="#Bootstrap" class="headerlink" title="Bootstrap"></a>Bootstrap</h3><ol>
<li><p>Provide the following three args:</p>
<ul>
<li>master key algorithm</li>
<li>encryption key algorithm</li>
<li>keystore location</li>
</ul>
</li>
<li><p>Use <code>master key algorithm</code> to create a master key, and put it in <code>Java Keystore (JKS)</code> in <code>keystore location</code>. JKS will be in Tableau Server’s config directory, and will be guarded by the dir’s access permission.</p>
</li>
<li><p>Create encryption key using <code>encryption key algorithm</code>, encrypt it with master key, and put the encrypted value in config file.</p>
</li>
<li><p>Encrypt all existing passwords in config file</p>
</li>
</ol>
<h3 id="When-encrypting-passwords"><a href="#When-encrypting-passwords" class="headerlink" title="When encrypting passwords"></a>When encrypting passwords</h3><p>Whenever a new key-value pair is requested to be put into config file:</p>
<ol>
<li>Check if the key is in our encryption list. If true, proceed; otherwise, break out and just put them as clear text in config file</li>
<li>Get encrypted encryption key value, decrypt it with master key to get the actual encryption key. You need to have access to the JKS file to read master key.</li>
<li>Encrypt passwords with encryption key</li>
</ol>
<h3 id="When-decrypting-passwords"><a href="#When-decrypting-passwords" class="headerlink" title="When decrypting passwords"></a>When decrypting passwords</h3><ol>
<li>Get encrypted encryption key value, decrypt it with master key to get the actual encryption key. You need to have access to the JKS file to read master key.</li>
<li>Decrypt passwords with encryption key</li>
</ol>
<h3 id="Key-rolling"><a href="#Key-rolling" class="headerlink" title="Key-rolling"></a>Key-rolling</h3><p>I made key rolling an atmoic transaction, so that it can recover from any failure.</p>
<ol>
<li>Take a backup of the current config file and the JKS, put them in a temp dir which shares the same access permission as the config dir</li>
<li>Roll all <code>server-managed</code> passwords, encrypt them with existing crypto service, and persist new values in config file</li>
<li>Decrypt all passwords <code>(both server-managed and user-managed)</code> in config file, and hold them as clear text in memory</li>
<li>Force creating a new JKS with new master key</li>
<li>Create a new encyption key</li>
<li>Reinitialize crypto service with new crypto properties</li>
<li>Use the new crypto service to encrypt all password held in memory</li>
<li>Put all new crypto properties and all encrypted passwords into the config file</li>
<li>If any error happens, force restoring the config file and the JKS in your backup</li>
</ol>
<h3 id="When-to-roll-keys"><a href="#When-to-roll-keys" class="headerlink" title="When to roll keys"></a>When to roll keys</h3><p>Here are some best practices I recommend:</p>
<ul>
<li>When a worker machine is removed/decommissioned from your cluster</li>
<li>Every a few months</li>
<li>Whenever the admin think is necessary</li>
</ul>
<h2 id="My-Role-in-this-project"><a href="#My-Role-in-this-project" class="headerlink" title="My Role in this project"></a>My Role in this project</h2><p>I played two major roles in this project</p>
<h3 id="1-Fireman"><a href="#1-Fireman" class="headerlink" title="1. Fireman"></a>1. Fireman</h3><p>I was not an orginal team member of this project. This project was started in parallel with <a href="https://phoenixjiangnan.github.io/2016/08/16/system%20design/project%20experience/How-did-I-build-a-custom-dynamic-ZooKeeper-management-and-migration-framework-and-an-asynchronous-job-framework-for-Tableau-Server-11/">my ZooKeeper project</a>. When I finished mine, my manager asked me to join my colleague in this project.</p>
<p>At that point, probably some time in June, everyone was positive about the project progress and thought it was on schedule. Adding me is just to help the project move faster. After I joined, and gave an estimation of all backlogs, I realized that we were actually far behind the schedule, even two of us were not able to finish it on time.</p>
<p>I brought this up to my manager, and had all our devs, testers, and PM sit down and evaluate our backlogs together. It turned out we WERE at an extreme risky schedule. I didn’t know why my colleague (the only original developer for this project) didn’t warn everybody.</p>
<h3 id="2-One-of-the-Key-Contributor"><a href="#2-One-of-the-Key-Contributor" class="headerlink" title="2. One of the Key Contributor"></a>2. One of the Key Contributor</h3><p>I am one of the main contributors, and finish the following works:</p>
<ul>
<li>crypto service bootstapping in both old Jruby app and new service-oriented Java app</li>
<li>key rolling</li>
<li>most Java-Ruby integration</li>
<li>enabling unit and integration tests</li>
<li>all customer-facing commands</li>
</ul>
<h2 id="Most-Important-Stuff-I-learned-from-this-project"><a href="#Most-Important-Stuff-I-learned-from-this-project" class="headerlink" title="Most Important Stuff I learned from this project"></a>Most Important Stuff I learned from this project</h2><p>Project Management:</p>
<ul>
<li>Always keep an eye on the project schedule, and don’t be shy to alert everybody when you cannot finish your work on time</li>
</ul>
<p>Tech:</p>
<ul>
<li>Learned quite a lot crypto architecture and related knowledge</li>
<li>Learned a lot Ruby and JRuby</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;For more information about Tableau Server, you can read my blogs &lt;a href=&quot;https://phoenixjiangnan.github.io/categories/system-design/project-experience/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;What-is-this-project-about&quot;&gt;&lt;a href=&quot;#What-is-this-project-about&quot; class=&quot;headerlink&quot; title=&quot;What is this project about?&quot;&gt;&lt;/a&gt;What is this project about?&lt;/h2&gt;&lt;p&gt;Tableau Server is an on-premise data analytics and visualization platform. Customer can deploy bits in their own data center, or in AWS, Azure, and GCP.&lt;/p&gt;
&lt;p&gt;There are two categories of passwords within a Tableau Server cluster:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;User input passwords, which are entered by users&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AWS key&lt;/li&gt;
&lt;li&gt;smtp passwords&lt;/li&gt;
&lt;li&gt;Windows runas user password&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Server managed passwords, which are randomly generated by server&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;jdbc password&lt;/li&gt;
&lt;li&gt;Postgres admin password&lt;/li&gt;
&lt;li&gt;ZooKeeper password&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So the passwords I’m talking about is not the password for someone’s server account. The latter ones live in a table of the cluster’s PostgreSQL database and are already encrypted.&lt;/p&gt;
&lt;p&gt;The current version of Tableau Server (v10) stores all the passwords in &lt;code&gt;plain text&lt;/code&gt; in a configuration file, which is distributed among machines.&lt;/p&gt;
&lt;p&gt;The new version of Tableau Server also stores all passwords in &lt;code&gt;plain text&lt;/code&gt;, but in a &lt;code&gt;Znode&lt;/code&gt; in ZooKeeper, as I described in &lt;a href=&quot;https://phoenixjiangnan.github.io/2016/08/16/system%20design/project%20experience/How-did-I-build-a-custom-dynamic-ZooKeeper-management-and-migration-framework-and-an-asynchronous-job-framework-for-Tableau-Server-11/&quot;&gt;this blog&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The project is about developing a crypto service that encrypts passwords before persisting them somewhere, and decrypts passwords into memory when server uses them, so that no more clear text secrets live in Tableau Server. The new service should support both our old monolith JRuby app and our new service-oriented server architecture.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="system design" scheme="https://phoenixjiangnan.github.io/categories/system-design/"/>
    
      <category term="project experience" scheme="https://phoenixjiangnan.github.io/categories/system-design/project-experience/"/>
    
    
      <category term="tableau server" scheme="https://phoenixjiangnan.github.io/tags/tableau-server/"/>
    
      <category term="crypto service" scheme="https://phoenixjiangnan.github.io/tags/crypto-service/"/>
    
  </entry>
  
  <entry>
    <title>How to compare float numbers</title>
    <link href="https://phoenixjiangnan.github.io/2016/08/27/best%20practices/How-to-compare-float-numbers/"/>
    <id>https://phoenixjiangnan.github.io/2016/08/27/best practices/How-to-compare-float-numbers/</id>
    <published>2016-08-27T23:49:26.000Z</published>
    <updated>2016-09-10T04:10:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>The reason I want to have a special blog talking about this problem is that, many people I know tend to think that float number comparison is the same as integer comparison. </p>
<p>They usually write code like</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">somefunction() &#123;</div><div class="line">	<span class="keyword">double</span> threeTenths1 = <span class="number">0.3</span>;</div><div class="line">	<span class="keyword">double</span> threeTenths2 = <span class="number">0.1</span> + <span class="number">0.1</span> + <span class="number">0.1</span>;</div><div class="line">	    </div><div class="line">	<span class="keyword">if</span> (threeTenths1 == threeTenths2) &#123;</div><div class="line">		<span class="comment">// ...</span></div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>then their unit tests fail and they don’t know what’s wrong until I helped them to print out the boolean </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">somefunction() &#123;</div><div class="line">	<span class="keyword">double</span> threeTenths1 = <span class="number">0.3</span>;</div><div class="line">	<span class="keyword">double</span> threeTenths2 = <span class="number">0.1</span> + <span class="number">0.1</span> + <span class="number">0.1</span>;</div><div class="line">	</div><div class="line">	System.out.println(threeTenths1);</div><div class="line">	System.out.println(threeTenths2);</div><div class="line">	</div><div class="line">	<span class="keyword">if</span> (threeTenths1 == threeTenths2) &#123;</div><div class="line">	    System.out.println(<span class="string">"Math is a world of absolute truth."</span>);</div><div class="line">	&#125; <span class="keyword">else</span> &#123;</div><div class="line">	    System.out.println(<span class="string">"This is not logical!"</span>);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">0.3</div><div class="line">0.30000000000000004</div><div class="line">This is not logical!</div></pre></td></tr></table></figure>
<p>which turned out to be <code>false</code>.</p>
<h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><p>Due to rounding errors, most floating-point numbers end up being slightly imprecise. As long as this imprecision stays small, it can usually be ignored.</p>
<p>However, it also means that numbers expected to be equal (e.g. when calculating the same result through different correct methods) often differ slightly, and a simple equality test fails. For example:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">float</span> a = <span class="number">0.15</span> + <span class="number">0.15</span></div><div class="line"><span class="keyword">float</span> b = <span class="number">0.1</span> + <span class="number">0.2</span></div><div class="line"><span class="keyword">if</span>(a == b) <span class="comment">// can be false!</span></div><div class="line"><span class="keyword">if</span>(a &gt;= b) <span class="comment">// can also be false!</span></div></pre></td></tr></table></figure>
<h2 id="What-do-you-mean-‘imprecise’"><a href="#What-do-you-mean-‘imprecise’" class="headerlink" title="What do you mean ‘imprecise’ ?"></a>What do you mean ‘imprecise’ ?</h2><p>Before we can continue I need to make clear the difference between 0.1, float(0.1), and double(0.1). In C/C++ the numbers 0.1 and double(0.1) are the same thing, but when I say “0.1” in text I mean the exact base-10 number, whereas float(0.1) and double(0.1) are rounded versions of 0.1. And, to be clear, float(0.1) and double(0.1) don’t have the same value, because float(0.1) has fewer binary digits, and therefore has more error.</p>
<p>Here are the exact values for 0.1, float(0.1), and double(0.1):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Number	    Value</div><div class="line">0.1	        0.1 (of course)</div><div class="line">float(.1)	0.100000001490116119384765625</div><div class="line">double(.1)	0.1000000000000000055511151231257827021181583404541015625</div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="Don’t-use-absolute-error-margins"><a href="#Don’t-use-absolute-error-margins" class="headerlink" title="Don’t use absolute error margins"></a>Don’t use absolute error margins</h2><p>The solution is to check not whether the numbers are exactly the same, but whether their difference is very small. The error margin that the difference is compared to is often called epsilon. The most simple form:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span>( Math.abs(a-b) &lt; <span class="number">0.00001</span>) <span class="comment">// wrong - don't do this</span></div></pre></td></tr></table></figure>
<p>This is a bad way to do it because a fixed epsilon chosen because it “looks small” could actually be way too large when the numbers being compared are very small as well. The comparison would return “true” for numbers that are quite different. And when the numbers are very large, the epsilon could end up being smaller than the smallest rounding error, so that the comparison always returns “false”. </p>
<h2 id="Look-out-for-edge-cases"><a href="#Look-out-for-edge-cases" class="headerlink" title="Look out for edge cases"></a>Look out for edge cases</h2><p>Therefore, it is necessary to see whether the relative error is smaller than epsilon:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span>( Math.abs((a-b)/b) &lt; <span class="number">0.00001</span> ) <span class="comment">// still not right!</span></div></pre></td></tr></table></figure>
<p>There are some important special cases where this will fail:</p>
<ul>
<li><p>When both a and b are zero. 0.0/0.0 is “not a number”, which causes an exception on some platforms, or returns false for all comparisons.</p>
</li>
<li><p>When only b is zero, the division yields “infinity”, which may also cause an exception, or is greater than epsilon even when a is smaller.</p>
</li>
<li><p>It returns false when both a and b are very small but on opposite sides of zero, even when they’re the smallest possible non-zero numbers.</p>
</li>
</ul>
<h2 id="The-Recommended-Way-of-Float-Number-Comparison-from-Me"><a href="#The-Recommended-Way-of-Float-Number-Comparison-from-Me" class="headerlink" title="The Recommended Way of Float Number Comparison from Me"></a>The Recommended Way of Float Number Comparison from Me</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">float</span> maxRelDiff = FLOAT_EPSILON; <span class="comment">// Can be 1/100,000</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">almostEqualRelatively</span><span class="params">(<span class="keyword">float</span> a, <span class="keyword">float</span> b)</span> </span>&#123;</div><div class="line">    <span class="comment">// Calculate the difference.</span></div><div class="line">    <span class="keyword">float</span> diff = Math.abs(a - b);</div><div class="line">    a = Math.abs(a);</div><div class="line">    b = Math.abs(b);</div><div class="line">    </div><div class="line">    <span class="comment">// Find the largest</span></div><div class="line">    <span class="keyword">float</span> largest = (b &gt; a) ? b : a;</div><div class="line"> </div><div class="line">    <span class="keyword">if</span> (diff &lt;= largest * maxRelDiff) &#123;</div><div class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<hr>
<p>References:</p>
<ul>
<li><a href="http://floating-point-gui.de/errors/comparison/" target="_blank" rel="external">http://floating-point-gui.de/errors/comparison/</a></li>
<li><a href="https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/" target="_blank" rel="external">https://randomascii.wordpress.com/2012/02/25/comparing-floating-point-numbers-2012-edition/</a></li>
<li><a href="http://stackoverflow.com/questions/1088216/whats-wrong-with-using-to-compare-floats-in-java" target="_blank" rel="external">http://stackoverflow.com/questions/1088216/whats-wrong-with-using-to-compare-floats-in-java</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The reason I want to have a special blog talking about this problem is that, many people I know tend to think that float number comparison is the same as integer comparison. &lt;/p&gt;
&lt;p&gt;They usually write code like&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;somefunction() &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;double&lt;/span&gt; threeTenths1 = &lt;span class=&quot;number&quot;&gt;0.3&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;double&lt;/span&gt; threeTenths2 = &lt;span class=&quot;number&quot;&gt;0.1&lt;/span&gt; + &lt;span class=&quot;number&quot;&gt;0.1&lt;/span&gt; + &lt;span class=&quot;number&quot;&gt;0.1&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	    &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (threeTenths1 == threeTenths2) &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;		&lt;span class=&quot;comment&quot;&gt;// ...&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;then their unit tests fail and they don’t know what’s wrong until I helped them to print out the boolean &lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;somefunction() &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;double&lt;/span&gt; threeTenths1 = &lt;span class=&quot;number&quot;&gt;0.3&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;double&lt;/span&gt; threeTenths2 = &lt;span class=&quot;number&quot;&gt;0.1&lt;/span&gt; + &lt;span class=&quot;number&quot;&gt;0.1&lt;/span&gt; + &lt;span class=&quot;number&quot;&gt;0.1&lt;/span&gt;;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	System.out.println(threeTenths1);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	System.out.println(threeTenths2);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (threeTenths1 == threeTenths2) &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	    System.out.println(&lt;span class=&quot;string&quot;&gt;&quot;Math is a world of absolute truth.&quot;&lt;/span&gt;);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&amp;#125; &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &amp;#123;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	    System.out.println(&lt;span class=&quot;string&quot;&gt;&quot;This is not logical!&quot;&lt;/span&gt;);&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;	&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;0.3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;0.30000000000000004&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;This is not logical!&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;which turned out to be &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&quot;Comparison&quot;&gt;&lt;a href=&quot;#Comparison&quot; class=&quot;headerlink&quot; title=&quot;Comparison&quot;&gt;&lt;/a&gt;Comparison&lt;/h2&gt;&lt;p&gt;Due to rounding errors, most floating-point numbers end up being slightly imprecise. As long as this imprecision stays small, it can usually be ignored.&lt;/p&gt;
&lt;p&gt;However, it also means that numbers expected to be equal (e.g. when calculating the same result through different correct methods) often differ slightly, and a simple equality test fails. For example:&lt;/p&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;float&lt;/span&gt; a = &lt;span class=&quot;number&quot;&gt;0.15&lt;/span&gt; + &lt;span class=&quot;number&quot;&gt;0.15&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;float&lt;/span&gt; b = &lt;span class=&quot;number&quot;&gt;0.1&lt;/span&gt; + &lt;span class=&quot;number&quot;&gt;0.2&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(a == b) &lt;span class=&quot;comment&quot;&gt;// can be false!&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(a &amp;gt;= b) &lt;span class=&quot;comment&quot;&gt;// can also be false!&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;What-do-you-mean-‘imprecise’&quot;&gt;&lt;a href=&quot;#What-do-you-mean-‘imprecise’&quot; class=&quot;headerlink&quot; title=&quot;What do you mean ‘imprecise’ ?&quot;&gt;&lt;/a&gt;What do you mean ‘imprecise’ ?&lt;/h2&gt;&lt;p&gt;Before we can continue I need to make clear the difference between 0.1, float(0.1), and double(0.1). In C/C++ the numbers 0.1 and double(0.1) are the same thing, but when I say “0.1” in text I mean the exact base-10 number, whereas float(0.1) and double(0.1) are rounded versions of 0.1. And, to be clear, float(0.1) and double(0.1) don’t have the same value, because float(0.1) has fewer binary digits, and therefore has more error.&lt;/p&gt;
&lt;p&gt;Here are the exact values for 0.1, float(0.1), and double(0.1):&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;Number	    Value&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;0.1	        0.1 (of course)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;float(.1)	0.100000001490116119384765625&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;double(.1)	0.1000000000000000055511151231257827021181583404541015625&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="compare float numbers" scheme="https://phoenixjiangnan.github.io/tags/compare-float-numbers/"/>
    
  </entry>
  
  <entry>
    <title>How to test a program involving probability - take an algorithm that shuffles array as example</title>
    <link href="https://phoenixjiangnan.github.io/2016/08/26/data%20structures%20and%20algorithms/probability/How-to-test-a-program-involving-probability-take-an-algorithm-that-shuffles-array-as-example/"/>
    <id>https://phoenixjiangnan.github.io/2016/08/26/data structures and algorithms/probability/How-to-test-a-program-involving-probability-take-an-algorithm-that-shuffles-array-as-example/</id>
    <published>2016-08-27T06:00:49.000Z</published>
    <updated>2016-09-07T21:41:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>This article is about some inspirations I got from another blog pasted below.</p>
<blockquote>
<p>That post discusses which algorithm is truly correct to randomly shuffle an array of integers (or shuffle pokers), and how to test an algorithm to make sure it’s the right one.</p>
<p>The core idea is that: when it comes to probability problem, there’s only one way to actually test that - statistics.</p>
</blockquote>
<p>For example, if a function can randomly return a number from 1 - 10, it means the probability of occurance of each individual number is 1/10. Therefore, if we run this function a huge amount of times (let’s say 1 million times), the distribution of returned values should be uniform.</p>
<p>At least, the error rate should be reasonable, say 5% but not 20%. We can define ‘reasonable’ as:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Sample: 1 million times</div><div class="line">Max error rate: 10%</div><div class="line">The 95% Percentile: over 90% error rate is less than 5%</div></pre></td></tr></table></figure>
<blockquote>
<p>Don’t use average error rate here as the post mentioned! I personally really hate using average of something. What’s the problem with ‘average’? You might ask. My question is: the average personal assets between you and Mark Zuckerburgh is pretty huge, so you a billinaire.</p>
</blockquote>
<p>And the corresponding test plan is:</p>
<blockquote>
<ul>
<li>a test unit should run this function 100 thousand times (100 samples)</li>
<li>run this test unit 1 million times for each sample</li>
<li>if over 95% percentile of the 1m sample tests has an error rate less than 5%, we say this function is truly random</li>
</ul>
<p>(From my point of view, there’s actually another way if the function given to you is not in a black box - you are able to give out the math probability equation)</p>
</blockquote>
<a id="more"></a>
<hr>
<p>The following content is from <a href="http://coolshell.cn/articles/8593.html" target="_blank" rel="external">http://coolshell.cn/articles/8593.html</a></p>
<p>Use Google Translate if necessary</p>
<hr>
<p>我希望本文有助于你了解测试软件是一件很重要也是一件不简单的事。</p>
<p>我们有一个程序，叫<code>ShuffleArray()</code>，是用来洗牌的，我见过N多千变万化的<code>ShuffleArray()</code>，但是似乎从来没人去想过怎么去测试这个算法。所以，我在面试中我经常会问应聘者如何测试<code>ShuffleArray()</code>，没想到这个问题居然难倒了很多有多年编程经验的人。对于这类的问题，其实，测试程序可能比算法更难写，代码更多。而这个问题正好可以加强一下我在<code>《我们需要专职的QA吗？》</code>中我所推崇的——开发人员更适合做测试的观点。</p>
<h2 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h2><p>我们先来看几个算法（第一个用递归二分随机抽牌，第二个比较偷机取巧，第三个比较通俗易懂）</p>
<h3 id="1-递归二分随机抽牌"><a href="#1-递归二分随机抽牌" class="headerlink" title="1. 递归二分随机抽牌"></a>1. 递归二分随机抽牌</h3><p>有一次是有一个朋友做了一个网页版的扑克游戏，他用到的算法就是想模拟平时我们玩牌时用手洗牌的方式，是用递归+二分法，我说这个程序恐怕不对吧。他觉得挺对的，说测试了没有问题。他的程序大致如下（原来的是用Javascript写的，我在这里凭记忆用C复现一下）：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//递归二分方法</span></div><div class="line"><span class="keyword">const</span> <span class="keyword">size_t</span> MAXLEN = <span class="number">10</span>;</div><div class="line"><span class="keyword">const</span> <span class="keyword">char</span> TestArr[MAXLEN] = &#123;<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>,<span class="string">'E'</span>,<span class="string">'F'</span>,<span class="string">'G'</span>,<span class="string">'H'</span>,<span class="string">'I'</span>,<span class="string">'J'</span>&#125;;</div><div class="line"> </div><div class="line"><span class="keyword">static</span> <span class="keyword">char</span> RecurArr[MAXLEN]=&#123;<span class="number">0</span>&#125;;</div><div class="line"><span class="keyword">static</span> <span class="keyword">int</span> cnt = <span class="number">0</span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShuffleArray_Recursive_Tmp</span><span class="params">(<span class="keyword">char</span>* arr, <span class="keyword">int</span> len)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">if</span>(cnt &gt; MAXLEN || len &lt;=<span class="number">0</span>)&#123;</div><div class="line">        <span class="keyword">return</span>;</div><div class="line">    &#125;</div><div class="line"> </div><div class="line">    <span class="keyword">int</span> pos = rand() % len;</div><div class="line">    RecurArr[cnt++] = arr[pos];</div><div class="line">    <span class="keyword">if</span> (len==<span class="number">1</span>) <span class="keyword">return</span>;</div><div class="line">    ShuffleArray_Recursive_Tmp(arr, pos);</div><div class="line">    ShuffleArray_Recursive_Tmp(arr+pos+<span class="number">1</span>, len-pos<span class="number">-1</span>);</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShuffleArray_Recursive</span><span class="params">(<span class="keyword">char</span>* arr, <span class="keyword">int</span> len)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="built_in">memset</span>(RecurArr, <span class="number">0</span>, <span class="keyword">sizeof</span>(RecurArr));</div><div class="line">    cnt=<span class="number">0</span>;</div><div class="line">    ShuffleArray_Recursive_Tmp(arr, len);</div><div class="line">    <span class="built_in">memcpy</span>(arr, RecurArr, len);</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">char</span> temp[MAXLEN]=&#123;<span class="number">0</span>&#125;;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">5</span>; i++) &#123;</div><div class="line">        <span class="built_in">strncpy</span>(temp, TestArr, MAXLEN);</div><div class="line">        ShuffleArray_Recursive((<span class="keyword">char</span>*)temp, MAXLEN);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>随便测试几次，还真像那么回事：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">第一次：D C A B H E G F I J</div><div class="line">第二次：A G D B C E F J H I</div><div class="line">第三次：A B H F C E D G I J</div><div class="line">第四次：J I F B A D C E H G</div><div class="line">第五次：F B A D C E H G I J</div></pre></td></tr></table></figure>
<h3 id="2-快排Hack法"><a href="#2-快排Hack法" class="headerlink" title="2. 快排Hack法"></a>2. 快排Hack法</h3><p>让我们再看一个hack快排的洗牌程序（只看算法，省去别的代码）：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">compare</span><span class="params">( <span class="keyword">const</span> <span class="keyword">void</span> *a, <span class="keyword">const</span> <span class="keyword">void</span> *b )</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">return</span> rand() % <span class="number">3</span><span class="number">-1</span>;</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShuffleArray_Sort</span><span class="params">(<span class="keyword">char</span>* arr, <span class="keyword">int</span> len)</span></span></div><div class="line">&#123;</div><div class="line">    qsort( (<span class="keyword">void</span> *)arr, (<span class="keyword">size_t</span>)len, <span class="keyword">sizeof</span>(<span class="keyword">char</span>), compare );</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>运行个几次，感觉得还像那么回事：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">第一次：H C D J F E A G B I</div><div class="line">第二次：B F J D C E I H G A</div><div class="line">第三次：C G D E J F B I A H</div><div class="line">第四次：H C B J D F G E I A</div><div class="line">第五次：D B C F E A I H G J</div></pre></td></tr></table></figure>
<p>看不出有什么破绽。</p>
<h3 id="3-大多数人的实现"><a href="#3-大多数人的实现" class="headerlink" title="3. 大多数人的实现"></a>3. 大多数人的实现</h3><p>下面这个算法是大多数人的实现，就是<code>for</code>循环一次，然后随机交换两个数</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShuffleArray_General</span><span class="params">(<span class="keyword">char</span>* arr, <span class="keyword">int</span> len)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">const</span> <span class="keyword">int</span> suff_time = len;</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> idx=<span class="number">0</span>; idx&lt;suff_time; idx++) &#123;</div><div class="line">        <span class="keyword">int</span> i = rand() % len;</div><div class="line">        <span class="keyword">int</span> j = rand() % len;</div><div class="line">        <span class="keyword">char</span> temp = arr[i];</div><div class="line">        arr[i] = arr[j];</div><div class="line">        arr[j] = temp;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>跑起来也还不错，洗得挺好的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">第一次：G F C D A J B I H E</div><div class="line">第二次：D G J F E I A H C B</div><div class="line">第三次：C J E F A D G B H I</div><div class="line">第四次：H D C F A E B J I G</div><div class="line">第五次：E A J F B I H G D C</div></pre></td></tr></table></figure>
<p>但是上述三个算法哪个的效果更好？好像都是对的。一般的QA或是程序员很有可能就这样把这个功能Pass了。但是事情并没有那么简单……</p>
<h2 id="如何测试-How-to-test"><a href="#如何测试-How-to-test" class="headerlink" title="如何测试 How to test"></a>如何测试 How to test</h2><p>在做测试之前，我们还需要了解一下一个基本知识——PC机上是做不出真随机数的，只能做出伪随机数。真随机数需要硬件支持。但是不是这样我们就无法测试了呢，不是的。我们依然可以测试。</p>
<blockquote>
<p>我们知道，洗牌洗得好不好，主要是看是不是够随机。那么如何测试随机性呢？</p>
<p>一到概率问题，我们只有一个方法来做测试，那就是用统计的方式.</p>
</blockquote>
<p>试想，我们有个随机函数<code>rand()</code>返回 1 到 10 中的一个数，如果够随机的话，每个数返回的概率都应该是一样的，也就是说每个数都应该有10分之1的概率会被返回。</p>
<p>也就是说，你调用<code>rand()</code>函数100次，其中，每个数出现的次数大约都在10次左右。（注意：我用了左右，这说明概率并不是很准确的）不应该有一个数出现了15次以上，另一个在5次以下，要是这样的话，这个函数就是错的。</p>
<p>举一反三，测试洗牌程序也一样，需要通过概率的方式来做统计，是不是每张牌出现在第一个位置的次数都是差不多的。</p>
<p>于是，这样一来上面的程序就可以很容易做测试了。</p>
<h2 id="Test-Result"><a href="#Test-Result" class="headerlink" title="Test Result"></a>Test Result</h2><p>下面是测试结果（测试样本1000次——列是每个位置出现的次数，行是各个字符的统计，出现概率应该是1/10，也就是100次）：</p>
<h3 id="1-递归随机抽牌的方法"><a href="#1-递归随机抽牌的方法" class="headerlink" title="1. 递归随机抽牌的方法"></a>1. 递归随机抽牌的方法</h3><p>很明显，这个洗牌程序太有问题。算法是错的！</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">     1    2    3    4    5    6    7    8    9    10</div><div class="line">----------------------------------------------------</div><div class="line">A | 101  283  317  208   65   23    3    0    0    0</div><div class="line">B | 101  191  273  239  127   54   12    2    1    0</div><div class="line">C | 103  167  141  204  229  115   32    7    2    0</div><div class="line">D | 103  103   87  128  242  195  112   26    3    1</div><div class="line">E | 104   83   62   67  116  222  228   93   22    3</div><div class="line">F |  91   58   34   60   69  141  234  241   65    7</div><div class="line">G |  93   43   35   19   44  102  174  274  185   31</div><div class="line">H |  94   28   27   27   46   68   94  173  310  133</div><div class="line">I | 119   27   11   30   28   49   64   96  262  314</div><div class="line">J |  91   17   13   18   34   31   47   88  150  511</div></pre></td></tr></table></figure>
<h3 id="2-快排Hack法-1"><a href="#2-快排Hack法-1" class="headerlink" title="2. 快排Hack法"></a>2. 快排Hack法</h3><p>看看对角线（从左上到右下）上的数据，很离谱！所以，这个算法也是错的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">      1    2    3    4    5    6    7    8    9    10</div><div class="line">-----------------------------------------------------</div><div class="line">A |   74  108  123  102   93  198   40   37   52  173</div><div class="line">B |  261  170  114   70   49   28   37   76  116   79</div><div class="line">C |  112  164  168  117   71   37   62   96  116   57</div><div class="line">D |   93   91  119  221  103   66   91   98   78   40</div><div class="line">E |   62   60   82   90  290  112   95   98   71   40</div><div class="line">F |   46   60   63   76   81  318   56   42   70  188</div><div class="line">G |   72   57   68   77   83   39  400  105   55   44</div><div class="line">H |   99   79   70   73   87   34  124  317   78   39</div><div class="line">I |  127  112  102   90   81   24   57   83  248   76</div><div class="line">J |   54   99   91   84   62  144   38   48  116  264</div></pre></td></tr></table></figure>
<h3 id="3-大多数人的算法"><a href="#3-大多数人的算法" class="headerlink" title="3. 大多数人的算法"></a>3. 大多数人的算法</h3><p>我们再来看看大多数人的算法。还是对角线上的数据有问题，所以，还是错的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">      1    2    3    4    5    6    7    8    9    10</div><div class="line">-----------------------------------------------------</div><div class="line">A |  178   98   92   82  101   85   79  105   87   93</div><div class="line">B |   88  205   90   94   77   84   93   86  106   77</div><div class="line">C |   93   99  185   96   83   87   98   88   82   89</div><div class="line">D |  105   85   89  190   92   94  105   73   80   87</div><div class="line">E |   97   74   85   88  204   91   80   90  100   91</div><div class="line">F |   85   84   90   91   96  178   90   91  105   90</div><div class="line">G |   81   84   84  104  102  105  197   75   79   89</div><div class="line">H |   84   99  107   86   82   78   92  205   79   88</div><div class="line">I |  102   72   88   94   87  103   94   92  187   81</div><div class="line">J |   87  100   90   75   76   95   72   95   95  215</div></pre></td></tr></table></figure>
<h2 id="正确的算法-The-correct-algorithm"><a href="#正确的算法-The-correct-algorithm" class="headerlink" title="正确的算法 The correct algorithm"></a>正确的算法 The correct algorithm</h2><p>下面，我们来看看性能高且正确的算法—— <code>Fisher_Yates算法</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">void ShuffleArray_Fisher_Yates(char* arr, int len)</div><div class="line">&#123;</div><div class="line">    int i = len, j;</div><div class="line">    char temp;</div><div class="line"> </div><div class="line">    if ( i == 0 ) return;</div><div class="line">    while ( i-- ) &#123;</div><div class="line">        j = rand() % (i+1);</div><div class="line">        temp = arr[i];</div><div class="line">        arr[i] = arr[j];</div><div class="line">        arr[j] = temp;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这个算法不难理解，看看测试效果（效果明显比前面的要好）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">      1    2    3    4    5    6    7    8    9    10</div><div class="line">-----------------------------------------------------</div><div class="line">A |  107   98   83  115   89  103  105   99   94  107</div><div class="line">B |   91  106   90  102   88  100  102   97  112  112</div><div class="line">C |  100  107   99  108  101   99   86   99  101  100</div><div class="line">D |   96   85  108  101  117  103  102   96  108   84</div><div class="line">E |  106   89  102   86   88  107  114  109  100   99</div><div class="line">F |  109   96   87   94   98  102  109  101   92  102</div><div class="line">G |   94   95  119  110   97  112   89  101   89   94</div><div class="line">H |   93  102  102  103  100   89  107  105  101   98</div><div class="line">I |   99  110  111  101  102   79  103   89  104  102</div><div class="line">J |  105  112   99   99  108  106   95   95   99   82</div></pre></td></tr></table></figure>
<p>但是我们可以看到还是不完美。因为我们使用的<code>rand()</code>是伪随机数，不过已经很不错的。最大的误差在20%左右。</p>
<p>我们再来看看洗牌100万次的统计值，你会看到误差在6%以内了。这个对于伪随机数生成的程序已经很不错了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">      1       2     3       4      5      6      7      8     9      10</div><div class="line">-------------------------------------------------------------------------</div><div class="line">A | 100095  99939 100451  99647  99321 100189 100284  99565 100525  99984</div><div class="line">B |  99659 100394  99699 100436  99989 100401  99502 100125 100082  99713</div><div class="line">C |  99938  99978 100384 100413 100045  99866  99945 100025  99388 100018</div><div class="line">D |  99972  99954  99751 100112 100503  99461  99932  99881 100223 100211</div><div class="line">E | 100041 100086  99966  99441 100401  99958  99997 100159  99884 100067</div><div class="line">F | 100491 100294 100164 100321  99902  99819  99449 100130  99623  99807</div><div class="line">G |  99822  99636  99924 100172  99738 100567 100427  99871 100125  99718</div><div class="line">H |  99445 100328  99720  99922 100075  99804 100127  99851 100526 100202</div><div class="line">I | 100269 100001  99542  99835 100070  99894 100229 100181  99718 100261</div><div class="line">J | 100268  99390 100399  99701  99956 100041 100108 100212  99906 100019</div></pre></td></tr></table></figure>
<h2 id="如何写测试案例"><a href="#如何写测试案例" class="headerlink" title="如何写测试案例"></a>如何写测试案例</h2><p>测试程序其实很容易写了。就是，设置一个样本大小，做一下统计，然后计算一下误差值是否在可以容忍的范围内。比如：</p>
<blockquote>
<p>样本：100万次<br>最大误差：10%以内<br>平均误差：5%以内 （或者：90%以上的误差要小于5%)</p>
</blockquote>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>其实，以上的测试只是测试了牌在各个位置的概率。这个还不足够好。因为还可能会现在有Pattern的情况。如：每次洗牌出来的都是一个循环顺序数组。这完全可以满足我上面的测试条件。但是那明显是错的。所以，还需要统计每种排列的出现的次数，看看是不是均匀。但是，如果这些排列又是以某种规律出现的呢？看来，这没完没了了。</p>
<p>测试的确是一个很重要，并不简单的事情。谢谢所有参与讨论的人。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>之前忘贴了一个模拟我们玩牌洗牌的算法，现补充如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">ShuffleArray_Manual</span><span class="params">(<span class="keyword">char</span>* arr, <span class="keyword">int</span> len)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> mid = len / <span class="number">2</span>;</div><div class="line"> </div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> n=<span class="number">0</span>; n&lt;<span class="number">5</span>; n++)&#123;</div><div class="line"> </div><div class="line">        <span class="comment">//两手洗牌</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;mid; i+=<span class="number">2</span>)&#123;</div><div class="line">            <span class="keyword">char</span> tmp = arr[i];</div><div class="line">            arr[i] = arr[mid+i];</div><div class="line">            arr[mid+i] = tmp;</div><div class="line">        &#125;</div><div class="line"> </div><div class="line">        <span class="comment">//随机切牌</span></div><div class="line">        <span class="keyword">char</span> *buf = (<span class="keyword">char</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">char</span>)*len);</div><div class="line"> </div><div class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;<span class="number">5</span>; j++) &#123;</div><div class="line">            <span class="keyword">int</span> start= rand() % (len<span class="number">-1</span>) + <span class="number">1</span>;</div><div class="line">            <span class="keyword">int</span> numCards= rand()% (len/<span class="number">2</span>) + <span class="number">1</span>;</div><div class="line"> </div><div class="line">            <span class="keyword">if</span> (start + numCards &gt; len )&#123;</div><div class="line">                numCards = len - start;</div><div class="line">            &#125;</div><div class="line"> </div><div class="line">            <span class="built_in">memset</span>(buf, <span class="number">0</span>, len);</div><div class="line">            <span class="built_in">strncpy</span>(buf, arr, start);</div><div class="line">            <span class="built_in">strncpy</span>(arr, arr+start, numCards);</div><div class="line">            <span class="built_in">strncpy</span>(arr+numCards, buf, start);</div><div class="line">        &#125;</div><div class="line">        <span class="built_in">free</span>(buf);</div><div class="line"> </div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>我们来看看测试结果：（10万次）效果更好一些，误差在2%以内了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">      1       2     3       4      5      6      7      8     9      10</div><div class="line">-------------------------------------------------------------------------</div><div class="line">A |  10002   9998   9924  10006  10048  10200   9939   9812  10080   9991</div><div class="line">B |   9939   9962  10118  10007   9974  10037  10149  10052   9761  10001</div><div class="line">C |  10054  10100  10050   9961   9856   9996   9853  10016   9928  10186</div><div class="line">D |   9851   9939   9852  10076  10208  10003   9974  10052   9992  10053</div><div class="line">E |  10009   9915  10050  10037   9923  10094  10078  10059   9880   9955</div><div class="line">F |  10151  10115  10113   9919   9844   9896   9891   9904  10225   9942</div><div class="line">G |  10001  10116  10097  10030  10061   9993   9891   9922   9889  10000</div><div class="line">H |  10075  10033   9866   9857  10170   9854  10062  10078  10056   9949</div><div class="line">I |  10045   9864   9879  10066   9930   9919  10085  10104  10095  10013</div><div class="line">J |   9873   9958  10051  10041   9986  10008  10078  10001  10094   9910</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article is about some inspirations I got from another blog pasted below.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;That post discusses which algorithm is truly correct to randomly shuffle an array of integers (or shuffle pokers), and how to test an algorithm to make sure it’s the right one.&lt;/p&gt;
&lt;p&gt;The core idea is that: when it comes to probability problem, there’s only one way to actually test that - statistics.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For example, if a function can randomly return a number from 1 - 10, it means the probability of occurance of each individual number is 1/10. Therefore, if we run this function a huge amount of times (let’s say 1 million times), the distribution of returned values should be uniform.&lt;/p&gt;
&lt;p&gt;At least, the error rate should be reasonable, say 5% but not 20%. We can define ‘reasonable’ as:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;Sample: 1 million times&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Max error rate: 10%&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;The 95% Percentile: over 90% error rate is less than 5%&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;Don’t use average error rate here as the post mentioned! I personally really hate using average of something. What’s the problem with ‘average’? You might ask. My question is: the average personal assets between you and Mark Zuckerburgh is pretty huge, so you a billinaire.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And the corresponding test plan is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;a test unit should run this function 100 thousand times (100 samples)&lt;/li&gt;
&lt;li&gt;run this test unit 1 million times for each sample&lt;/li&gt;
&lt;li&gt;if over 95% percentile of the 1m sample tests has an error rate less than 5%, we say this function is truly random&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(From my point of view, there’s actually another way if the function given to you is not in a black box - you are able to give out the math probability equation)&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="data structures and algorithms" scheme="https://phoenixjiangnan.github.io/categories/data-structures-and-algorithms/"/>
    
      <category term="probability" scheme="https://phoenixjiangnan.github.io/categories/data-structures-and-algorithms/probability/"/>
    
    
      <category term="random" scheme="https://phoenixjiangnan.github.io/tags/random/"/>
    
      <category term="probability" scheme="https://phoenixjiangnan.github.io/tags/probability/"/>
    
  </entry>
  
  <entry>
    <title>Facebook User Group and Bipartite Graph</title>
    <link href="https://phoenixjiangnan.github.io/2016/08/26/data%20structures%20and%20algorithms/interview/Facebook-User-Group-and-Bipartite-Graph/"/>
    <id>https://phoenixjiangnan.github.io/2016/08/26/data structures and algorithms/interview/Facebook-User-Group-and-Bipartite-Graph/</id>
    <published>2016-08-26T22:31:30.000Z</published>
    <updated>2016-08-26T22:37:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="The-Problem"><a href="#The-Problem" class="headerlink" title="The Problem"></a>The Problem</h2><p>大家都知道Facebook用户都是双向的好友，a是b的好友，那么b一定是a的好友，现在给定一个用户列表，其中有些用户是好友，有些不是，请判断，这些用户是否可以划分为两组，并且每组内的用户，互相都不是好友。如果能，请给出这个划分。</p>
<p>例子1:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">用户：&#123;1, 2, 3&#125;</div><div class="line">好友关系：1-2， 2-3</div><div class="line">划分：&#123;1,3&#125; &#123;2&#125;</div></pre></td></tr></table></figure>
<p>例子2:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">用户&#123;1,2,3,4&#125;</div><div class="line">好友关系：1-2， 2-3， 3-4，4-1</div><div class="line">划分：&#123;1, 3&#125; &#123;2, 4&#125;</div></pre></td></tr></table></figure>
<h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2><p>有很多面试题比较直接，看到题目，面试者很自然的就想到用什么方法，这时候，往往面试官要的是bug-free的code；还有一些问题，不那么直接，那就是考察，应试者的分析问题、分解问题的能力。往往是一个相对复杂的问题，分解为我们见过的、相对简单的问题；还有一类问题，从实际当中来，这就需要有建模的能力，也可以说是把问题抽象出来的能力，然后才是分解成小的问题等。这样的题目，能够全面的考察应试者的能力。例如Facebook这个题目。</p>
<p>这个面试题来自Facebook的实际问题。像在我们国内，有很多同学在做新浪微博的数据挖掘，好友推荐、关系预测、圈子发现、转发分析、影响力分析等等，这些都是很实际、很有用的问题，每一个问题，都可以作为一个值得深入探讨的面试题。大家可以思考一下：这个问题可以归结为一个什么问题？排序？查找？树？图？确定这个，大概的解决方法，也基本可以确认。</p>
<p>相对而言，这个面试题目比较简单。因为建模的部分，已经在题目中给出了。简单思考，就发现是一个图分割的问题，既然是图，无非也就是深度优先，或者宽度优先遍历了，递归或者非递归的实现。Facebook的好友关系是双向的，意味着是无向图（新浪微博，twitter都是有向图）。然后，要把图划分为两组，这两组满足什么条件呢？组内没有边，组间有边。了解图结构的，立刻会想到，这就是一个二分图。</p>
<p>那问题很明显了，就是判断Facebook好友关系构成的图，是否是一个二分图？如果是，请找到这个划分。就是二分图判断+找到一个划分即可。</p>
<p>根据二分图的特性，一条边上的两个点，肯定是属于不同的组。如果它们出现在同一个组中，肯定就不是二分图了。怎么判断，一条边上的两个点，分属于不同的组呢？我们需要遍历图，如果找到一条边，两个节点，都在同一组，则不是二分图；如果图遍历完成之后，没有找到这样的边，则是二分图。我们在遍历的过程中，我们需要区分，一条边的两个节点分属于不同的组，这里我们用到了染色法。核心思想如下：</p>
<p>从某一个点开始，将这个节点染色为白色，并且开始广度优先遍历，找到与其相邻的节点，如果是二分图，相邻节点的颜色都应该不同。如果是黑色，则不变；如果是无色，则染成黑色；如果是白色，也就是同色，程序退出。当图遍历完毕时，没有相邻节点同色的，则是二分图，标记为白色和黑色的两组就是一个划分。</p>
<a id="more"></a>
<h2 id="Step-by-Step-Explanation"><a href="#Step-by-Step-Explanation" class="headerlink" title="Step by Step Explanation"></a>Step by Step Explanation</h2><p>E.g.1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">用户&#123;1,2,3,4&#125;</div><div class="line">好友关系：1-2， 2-3， 3-4，4-1</div><div class="line">划分：&#123;1, 3&#125; &#123;2, 4&#125;</div></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>steps</th>
<th>vertex visiting</th>
<th>neighbours</th>
<th>queue</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1: white</td>
<td>2: black, 4: black</td>
<td>2, 4</td>
</tr>
<tr>
<td>2</td>
<td>2: black</td>
<td>1: white, 3: white</td>
<td>4, 3</td>
</tr>
<tr>
<td>3</td>
<td>4: black</td>
<td>1: white, 3: white</td>
<td>3</td>
</tr>
<tr>
<td>4</td>
<td>3: white</td>
<td>2: black, 4: black</td>
<td>empty</td>
</tr>
</tbody>
</table>
<p>队列空，结束遍历。找到划分{1,3}白 {2,4}黑。</p>
<p>E.g.2<br>再看一个不是二分图的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">用户&#123;1，2，3，4&#125;</div><div class="line">关系：1-2，1-3，1-4，2-3，3-4</div></pre></td></tr></table></figure>
<p>判断步骤如下：</p>
<table>
<thead>
<tr>
<th>steps</th>
<th>vertex visiting</th>
<th>neighbours</th>
<th>queue</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1: white</td>
<td>2: black, 3: black, 4: black</td>
<td>2, 3, 4</td>
</tr>
<tr>
<td>2</td>
<td>2: black</td>
<td>1: white, 3: white (but is already black)</td>
<td>3, 4</td>
</tr>
</tbody>
</table>
<p>发现3矛盾了，则上面的关系，不够成二分图。</p>
<hr>
<p>借鉴于’待字闺中’的微信公众号</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;The-Problem&quot;&gt;&lt;a href=&quot;#The-Problem&quot; class=&quot;headerlink&quot; title=&quot;The Problem&quot;&gt;&lt;/a&gt;The Problem&lt;/h2&gt;&lt;p&gt;大家都知道Facebook用户都是双向的好友，a是b的好友，那么b一定是a的好友，现在给定一个用户列表，其中有些用户是好友，有些不是，请判断，这些用户是否可以划分为两组，并且每组内的用户，互相都不是好友。如果能，请给出这个划分。&lt;/p&gt;
&lt;p&gt;例子1:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;用户：&amp;#123;1, 2, 3&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;好友关系：1-2， 2-3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;划分：&amp;#123;1,3&amp;#125; &amp;#123;2&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;例子2:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;用户&amp;#123;1,2,3,4&amp;#125;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;好友关系：1-2， 2-3， 3-4，4-1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;划分：&amp;#123;1, 3&amp;#125; &amp;#123;2, 4&amp;#125;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;Analysis&quot;&gt;&lt;a href=&quot;#Analysis&quot; class=&quot;headerlink&quot; title=&quot;Analysis&quot;&gt;&lt;/a&gt;Analysis&lt;/h2&gt;&lt;p&gt;有很多面试题比较直接，看到题目，面试者很自然的就想到用什么方法，这时候，往往面试官要的是bug-free的code；还有一些问题，不那么直接，那就是考察，应试者的分析问题、分解问题的能力。往往是一个相对复杂的问题，分解为我们见过的、相对简单的问题；还有一类问题，从实际当中来，这就需要有建模的能力，也可以说是把问题抽象出来的能力，然后才是分解成小的问题等。这样的题目，能够全面的考察应试者的能力。例如Facebook这个题目。&lt;/p&gt;
&lt;p&gt;这个面试题来自Facebook的实际问题。像在我们国内，有很多同学在做新浪微博的数据挖掘，好友推荐、关系预测、圈子发现、转发分析、影响力分析等等，这些都是很实际、很有用的问题，每一个问题，都可以作为一个值得深入探讨的面试题。大家可以思考一下：这个问题可以归结为一个什么问题？排序？查找？树？图？确定这个，大概的解决方法，也基本可以确认。&lt;/p&gt;
&lt;p&gt;相对而言，这个面试题目比较简单。因为建模的部分，已经在题目中给出了。简单思考，就发现是一个图分割的问题，既然是图，无非也就是深度优先，或者宽度优先遍历了，递归或者非递归的实现。Facebook的好友关系是双向的，意味着是无向图（新浪微博，twitter都是有向图）。然后，要把图划分为两组，这两组满足什么条件呢？组内没有边，组间有边。了解图结构的，立刻会想到，这就是一个二分图。&lt;/p&gt;
&lt;p&gt;那问题很明显了，就是判断Facebook好友关系构成的图，是否是一个二分图？如果是，请找到这个划分。就是二分图判断+找到一个划分即可。&lt;/p&gt;
&lt;p&gt;根据二分图的特性，一条边上的两个点，肯定是属于不同的组。如果它们出现在同一个组中，肯定就不是二分图了。怎么判断，一条边上的两个点，分属于不同的组呢？我们需要遍历图，如果找到一条边，两个节点，都在同一组，则不是二分图；如果图遍历完成之后，没有找到这样的边，则是二分图。我们在遍历的过程中，我们需要区分，一条边的两个节点分属于不同的组，这里我们用到了染色法。核心思想如下：&lt;/p&gt;
&lt;p&gt;从某一个点开始，将这个节点染色为白色，并且开始广度优先遍历，找到与其相邻的节点，如果是二分图，相邻节点的颜色都应该不同。如果是黑色，则不变；如果是无色，则染成黑色；如果是白色，也就是同色，程序退出。当图遍历完毕时，没有相邻节点同色的，则是二分图，标记为白色和黑色的两组就是一个划分。&lt;/p&gt;
    
    </summary>
    
      <category term="data structures and algorithms" scheme="https://phoenixjiangnan.github.io/categories/data-structures-and-algorithms/"/>
    
      <category term="interview" scheme="https://phoenixjiangnan.github.io/categories/data-structures-and-algorithms/interview/"/>
    
    
      <category term="bipartite graph" scheme="https://phoenixjiangnan.github.io/tags/bipartite-graph/"/>
    
  </entry>
  
</feed>
